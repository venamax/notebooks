{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python MapReduce\n",
    "\n",
    "Yesterday we saw an example of using Hadoop Streaming's API in Python to implement a word count. Today we'll be taking a closer look at both Python MapReduce and how to run \"real\" jobs using Amazon Web Services. We'll also be taking a look at another implementation of word count in Python using a library called `mrjob`.\n",
    "\n",
    "## Mrjob: a wrapper around Hadoop Streaming\n",
    "`mrjob` has a few advantages over directly using Hadoop Streaming.\n",
    "1. It is an open-source Python framework that provides a *pythonic API* to Hadoop Streaming and is actively developed by Yelp.\n",
    "2. It more closely resembles the Java Mapreduce paradigm that is the most commonly used in industry.\n",
    "3. Since Yelp operates entirely inside Amazon Web Services, mrjob’s integration with EMR is very smooth and easy (using the `boto` package). We are partially using `mrjob` because it enables seamless resource-sharing on EMR, e.g. for the `mr.py` miniproject.\n",
    "\n",
    "Some features:\n",
    "- Run jobs on EMR, your own Hadoop cluster, or locally (for testing).\n",
    "- Write multi-step jobs (one map-reduce step feeds into the next)\n",
    "\n",
    "However, keep in mind that `mrjob` *will* be slower than using the straight-up Streaming API. \n",
    "\n",
    "### Example\n",
    "Look at the script [`projects/mrjob/src/wordcount.py`](projects/mrjob/src/wordcount.py) in the `datacourse` folder.  \n",
    "\n",
    "1. The code consists mostly of a class that inherits from `MRJob`\n",
    "```python\n",
    "class MRWordCount(MRJob):\n",
    "```\n",
    "1. A mapper which uses regular expressions to break up each line into words and yields examples of each\n",
    "```python\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "```\n",
    "1. A reducer that sums up all instances of the words\n",
    "```python\n",
    "    def reducer(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "```\n",
    "1. So far, we have only defined a mapreduce class `MRWordCount`.  Because we want this to be a script, we need to actually run it:\n",
    "```python\n",
    "    if __name__ == '__main__':\n",
    "        MRWordCount.run()\n",
    "```\n",
    "\n",
    "## Python iterators and generator functions\n",
    "Generators are functions that create iterators, for example with Python’s yield statement. They have the advantage that an element of a sequence is not produced until you actually need it. This can help a lot in terms of computational expensiveness or memory consumption depending on the task at hand.  This has two practical consequences in `MRJob`:\n",
    "1. The keyword `yield` is like `return` but you can `return` as any times as you want!  This allows you to output zero, one, or multiple records in the mapper (Arguably, \"Mapreduce\" is more accurately called \"Flatmapreduce\", but that seems less catchy).\n",
    "1. The values in a reduce (`counts` in our above example) are python iterators so you can only run through them once.  How would you compute the mean of `counts`?\n",
    "\n",
    "\n",
    "### Bottlenecks in mapreduce\n",
    "The two most expensive things in a mapreduce are\n",
    "1. **Network IO:** sending data back and forth between nodes\n",
    "1. **Disk IO:** persisting data to disk.\n",
    "\n",
    "### Combiner\n",
    "We can use a `combiner` to reduce the amount of Network IO at the expense of Disk IO.  You might decide that this mapreduce is inefficient.  If the word `bear` appears twice in a line or node, it writes `(bear,1)` twice instead of `(bear,2)` once.  This would reduce the network IO costs.  Fortunately, there is the notion of a `combiner`: for certain types of reducers, it [effectively runs reduce locally on the node](http://mrjob.readthedocs.org/en/latest/guides/concepts.html), potentially reducing network traffic.  [Here's some documentation on how to add a combiner to mrjob](https://pythonhosted.org/mrjob/job.html#mrjob.job.MRJob.combiner).  Add one to word count.\n",
    "**Question:** What is the downside of a `combiner`?\n",
    "\n",
    "### Warning:\n",
    "This is not a normal class.  Normally in classes, we can store values and use them again later.  \n",
    "```python\n",
    "class Foo(object):\n",
    "    def store(self, value):\n",
    "        self._value = value\n",
    "        \n",
    "    def retrieve(self):\n",
    "        return self._value\n",
    "        \n",
    "foo = Foo()\n",
    "foo.store(3)\n",
    "print foo.retrieve()  # prints 3\n",
    "```\n",
    "\n",
    "Don't count on this with `MRJob`.  This class is more like DNA: every mapper and reducer gets the entire mapreduce, but the different nodes only run the portion of the code that are relevant for them.  For example, a mapper class will run `mapper` but not `reducer` and vice versa.  The only exception is that:\n",
    "1. Each mapper node will run `mapper_init`, `mapper`, and `mapper_final`\n",
    "1. Each combiner node will run `combiner_init`, `combiner`, and `combiner_final`\n",
    "1. Each reducer node will run `reducer_init`, `reducer`, and `reducer_final`\n",
    "\n",
    "Note that this means you still cannot share data between mapper nodes during the map phase, etc ...\n",
    "\n",
    "### Running code:\n",
    "To invoke MR job, run\n",
    "```bash\n",
    "python wordcount.py input_file.txt | more\n",
    "```\n",
    "to count the words in `input_file.txt`.  To run the wordcount, on our project gutenberg text, run (from datacourse):\n",
    "```bash\n",
    "python projects/mrjob/src/wordcount.py small_data/gutenberg/pg20417.txt | more\n",
    "```\n",
    "\n",
    "**Exercises**:\n",
    "1. Create a script called `src/unixwordcount.py`.  Modify this to output the number of characters, words, and lines in a string of text, i.e. to mimic the <a href=\"http://en.wikipedia.org/wiki/Wc_(Unix)\">unix word count program</a>.\n",
    "1. Use MR to comptue the total amount of sales per user in `data/sales.tsv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing statistics\n",
    "\n",
    "Suppose we want to take the mean grouped by a key.  Remember that the `values` in `reducer` are an *iterator*, not a list.  This has a practical consequence:\n",
    "\n",
    "**Anti-example:** What is wrong with this line of code?\n",
    "```python\n",
    "    def reducer(self, key, values):\n",
    "       yield key, 1. * sum(values) / len(values)\n",
    "```\n",
    "\n",
    "**Questions:**\n",
    "1. Write a mapreduce to compute the mean and variance of `data/numbers.txt` without storing the entire sequence.  How would you add a combiner?\n",
    "1. How would you compute the (approximate) median, 25% quantile, or 75% quantile?\n",
    "1. How would you normalize features by (e.g.) converting counts to percentages?  That is, if you have a list of numbers $N_i$, you want a list of numbers,\n",
    "    $$ \\frac{N_i}{\\sum_j N_j} .$$\n",
    "    Can you do this in a single mapreduce?  *Hints:*   You need to compute the total $\\sum_j N_j$ in the `mapper` (or more appropriately `mapper_final`) and \n",
    "1. How would we normalize features before submitting them to a machine learning algorithm?  That is, given features $X_{ij}$ in the above format, we want a mapreduce which outputs $X'_{ij}$ where\n",
    "    $$ X'_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j} $$\n",
    "    where $\\mu_j$ and $\\sigma_j$ are the mean and standard deviation of column $j$ of $X_{ij}$.  Can you do this in a single mapreduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating from SQL\n",
    "\n",
    "1. How would you filter by row in mapreduce?\n",
    "```SQL\n",
    "SELECT * FROM table WHERE country = 'US';\n",
    "```\n",
    "\n",
    "1. How would you filter by column?  Why would you do this in Mapreduce?\n",
    "```SQL\n",
    "SELECT col1, col2 FROM table;\n",
    "```\n",
    "\n",
    "1. How would you get a list of (distinct) users from your table?\n",
    "```SQL\n",
    "SELECT DISTINCT user FROM table\n",
    "```\n",
    "\n",
    "1. Let's say you're trying to sum all sales by country in a mapreduce.  How would you write a \"group by\" mapreduce?  The equivalent SQL code would be:\n",
    "``` SQL\n",
    "SELECT sum(sales) FROM table GROUP BY country;\n",
    "```\n",
    "\n",
    "1. **A \"hot\" node in mapreduce:** Let's say there is a key is 'hot' and records with that key account for 90% of the data in that phase.  (For example, you are running total sales per country but the vast majority of your sales occur in the US.)  Since all the data with the same key gets sent to a single reducer, you have not taken advantage of the parralization in MR.  How would you solve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining data in MapReduce\n",
    "\n",
    "Joining data in MapReduce which involves using multiple mappers for a mapreduce.  For example, let's say we want to to compute total sales by country.  Roughly speaking, we are trying to do the equivalent of this in SQL:\n",
    "```SQL\n",
    "SELECT sum(sales.amount)\n",
    "    FROM sales JOIN users\n",
    "    ON sales.user = users.user\n",
    "    GROUP BY users.country;\n",
    "```\n",
    "\n",
    "We will be joining `small_data/sales.tsv` and `small_data/users.tsv` to compute total sales by country. Notice that `mrjob` can take any number of arguments and all their contents get fed into mapper.  In the mapper, you will need to use the values in the first column to determine which table you are reading.  The full file is in [`projects/mrjob/src/join.py`](projects/mrjob/src/join.py)\n",
    "\n",
    "1.  The initial mapper emit `userid, amount` key-value pairs for the sales records and `userid, country` key-value pairs for user records.  Since both sets of key-value pairs are placed on the same stream, we need a dummy string (`\"amount\"` and `\"country\"`) statement to denote which type of keypair this is\n",
    "    ```python\n",
    "    def mapper1(self, _, line):\n",
    "        row = line.split(\",\")\n",
    "        if row[0] == \"sales\":\n",
    "            yield row[3], (\"amount\", row[4])\n",
    "        elif row[0] == \"users\":\n",
    "            yield row[1], (\"country\", row[-1])\n",
    "    ```\n",
    "1.  The initial reducer aggregates the amounts by user.  Since we ultimately want the results by country, we emit that as the key.\n",
    "    ```python\n",
    "    def reducer1(self, userid, values):\n",
    "        total = 0\n",
    "        for type_, val in values:\n",
    "            if type_ == \"amount\":\n",
    "                total += int(val)\n",
    "            elif type_ == \"country\":\n",
    "                country = val\n",
    "        yield country, total\n",
    "    ```\n",
    "1.  We need to run a 2nd mapreduce which aggregates by country.  Since the results are already keyed by country, the 2nd mapper does not need to do anything.  The 2nd reducer is very simple:\n",
    "    ```python\n",
    "    def reducer2(self, country, totals):\n",
    "        yield country, sum(totals)\n",
    "    ```\n",
    "1.  MR jobs are written to be composable: that is, we can take the output of one MR job and output it to another.  In `mrjob`, this is done by overriding the `steps` method to return a list of the individual `mr` classes which call mappers and reducers defined above in the class.  Check out the [mrjob documentation](https://pythonhosted.org/mrjob/guides/writing-mrjobs.html#multi-step-jobs) for more details.\n",
    "    ```python\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "            MRStep(reducer=self.reducer2)  # identity mapper implied\n",
    "        ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting in MapReduce\n",
    "\n",
    "We often want to count things up (e.g., count how many records did we process).  It's tempting to do something like this:\n",
    "\n",
    "**Anti-example:** What is wrong with this?\n",
    "```python        \n",
    "    def mapper(self, k, v):\n",
    "        print >>sys.stderr, (k, v)\n",
    "        yield f(k, v)\n",
    "```\n",
    "\n",
    "**Anti-example:** What does this not really work well on a distributed system?\n",
    "```python\n",
    "    def mapper_init(self):\n",
    "        self.counter = 0\n",
    "        \n",
    "    def mapper(self, k, v):\n",
    "        self.counter += 1\n",
    "        yield f(k, v)\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        print >>sys.stderr, self.counter\n",
    "```\n",
    "\n",
    "**Example**: instead of the above, here's a mechanism provided to do counting:\n",
    "```python\n",
    "    def mapper(self, k, v):\n",
    "        self.increment_counter(\"group name\", \"counter name\", amount=1)\n",
    "        yield f(k, v)\n",
    "```\n",
    "\n",
    "These counters are incremented at the end of the computation.\n",
    "\n",
    "**Questions:**\n",
    "1. What if you wanted to add a float (and only needed an approximate answer)?\n",
    "1. What if you wanted to compute the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Multistep Mapreduce\n",
    "\n",
    "Another classic example of an MRJob that requires two steps is squaring a matrix $M$.  The two steps are:\n",
    "1. The first mapreduce emits $M_{ij}$ keyed by both $i$s and $j$'s (map) and then performing the multiplication $M_{ij} * M_{jk}$ (reduce) keyed by $(i,k)$.\n",
    "2. The second mapreduce sums the values of $M_{ij} * M_{jk}$ along $j$ (reduce: it has an identity mapper).\n",
    "\n",
    "The algorithm is given in `src/matrix_square.py`.\n",
    "\n",
    "**Exercises**: write a new `src/matrix_multiply.py` that multiplies the matrices stored in `small_data/A.csv` and `small_data/B.csv`.\n",
    "\n",
    "**Interview Question**: How much memory does normal matrix multiplication take?  How much memory does MR matrix multiplication take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**:\n",
    "\n",
    "1.  Write a mapreduce to multiply two matrices $A_{ij}$ and $B_{ij}$.  Assume that the elements of $A_{ij}$ are encoded sparsely in a `csv` format:\n",
    "\n",
    "        \"A\", i, j, v\n",
    "    \n",
    "    That is, `i` and `j` are the indices and `v` is the value at position `i` and `j`.  $B_{ij}$ is similar.  Note that the first column is there to distinguish between the two matrices becasue `mrjob` only takes one input file.  Hint, this requires two mapreduces chained together.\n",
    "1.  Compute the set of all common friends between two individuals via a mapreudce.  Assume that the friendship matrix is encoded sparsely in a `csv` format:\n",
    "\n",
    "        u1, u2\n",
    "        \n",
    "    The above row implies, `u1` is friends with `u2` (and this is a symmetric relationship).  The output should be of the form\n",
    "    \n",
    "        u1, u2, 24\n",
    "    \n",
    "    which would represent that there are 24 common friends shared by `u1` and `u2`.  Can you reuse your solution to the previous mapreduces to help?\n",
    "1.  Write a mapreduce implementation of Naive Bayes.  The solution is what is called an *online* as opposed to *offline* algorihtm.  Assume the input data are rows of a feature matrix $X_{ij}$ given in a `csv` format\n",
    "\n",
    "        f1, f2, f3, ..., fp\n",
    "\n",
    "    where `f1`, ... `fp` are floats that represent the $p$ features of a row in $X$.  The output should be one row\n",
    "    \n",
    "        b1, b2, b3, ..., bp\n",
    "    \n",
    "    where `b1`, ..., `bp` are the components of $\\beta$, the coefficients of the model.  When would this be useful?\n",
    "1.  Write linear or logistic regression as a mapreduce with the same input and output.\n",
    "1.  What other machine-learning algorithms can you write as a mapreduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets\n",
    "1. Explain to a layman why reducer functions must be associative. Must they be commutative as well? What about combiner functions?\n",
    "1. Write a generator function that accepts as input a stream of all positive integers and yields only those divisible by k. \n",
    "1. Write a function sum() that accepts as input a stream of integers and returns the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
