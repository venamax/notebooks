{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online learning\n",
    "This is the fancy word for machine learning when you feed your data in little-by-little.  This could be just because you cannot hold all of the data in memory/disk at once, or because you are getting new data over time and need to feed it into an existing model.\n",
    "\n",
    "## Out-of-core learning\n",
    "This is the fancy word for machine learning when your data doesn't all fit into memory / fit on one computer.  Online learning algorithms give, in particular, out-of-core learning algorithms.  \n",
    "\n",
    "(You could imagine differences in desired behavior -- e.g., an online learner might depend on the order in which you feed in the data, so if you do not want this you might shuffle the data first.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data streams\n",
    "\n",
    "In online learning, you often want a pipeline operating on instances in a __stream__.  The point of streaming (i.e., what does streaming actually mean) is to use constant memory by keeping around only a bounded amount of history / state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three steps are:\n",
    "1. **Ingest data (in a _stream_ or in _chunks_)**: This means do not store everything.  In Python this usually means something like using a generator / a library that does this automatically.\n",
    "2. **Extract features (in a _stateless_ way)**: Being stateless means that your feature extractor should not store (much, or a growing amount of) information.  \n",
    "\n",
    "   This is most relevant in textual tasks, or when you have categorical features whose set of possible values is not known ahead of time -- the standard solution in both cases is to use the [_hashing trick_](http://scikit-learn.org/dev/modules/feature_extraction.html#feature-hashing).  We already mentioned this in the context of the `HashingVectorizer` for bag-of-words, but the same can be done for other categorical variables (see the [FeatureHasher](http://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher))\n",
    "3. **Update the learner**: This means to use the features to update the learned coefficients of the model.  In scikit-learn this is the `partial_fit` method that some (but not all) learners have.  \n",
    "\n",
    "  In general there is some question of _what this means_ for any given learner: i.e., are more recent test cases given greater weight in the optimization problem being solved, and if so how much greater? \n",
    "\n",
    "  (The answer is usually yes -- they are given greater weight.  This behavior is often desired -- where it might not be is if you are using an online learner to do out-of-core learning on data that does not have a natural ordering by relevance / time, in which case a shuffling step will help.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online learning in Scikit-learn\n",
    "\n",
    "It turns out that scikit-learn already has some support for online learning strategies.  See http://scikit-learn.org/dev/modules/scaling_strategies.html#incremental-learning for a full list, but some notable examples are:\n",
    "\n",
    "1. Stochastic gradient descent methods: `SGDClassifier` and `SGDRegressor`.  These can be used to train the standard _linear models_:  _linear regressions_ for regression; and, _logistic regressions_ and _SVM_ for classification.\n",
    "\n",
    "  Some caveats: \n",
    "   - In contrast to some other linear learners, SGD methods care a lot that the data be _scaled_ \n",
    "   - There are extra (numerical) parameters that must be tuned to ensure good learning.\n",
    "\n",
    "2. Naive Bayes methods: `MultinomialNB`, `BernoulliNB`.  It's just counting, after all.\n",
    "\n",
    "3. (Unsupervised) K-Means clustering: `MiniBatchKMeans`.\n",
    "\n",
    "####For more details:\n",
    "http://scikit-learn.org/dev/modules/scaling_strategies.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-core methods in Scikit-learn and Spark MLlib\n",
    "\n",
    "There are out-of-core algorithms that do not depend on an online algorithm.  That is, you distribute the data and do some iterative training step across a cluster -- rather than feeding in the data bit by bit and (e.g.,) exponentially weighting the recent test cases more.\n",
    "\n",
    "  So for instance, while scikit-learn's SGD learners will weight recent test cases higher (and thus depends on the ordering and the  size of the batches that you feed into `partial_fit`), the logistic regression that we saw implemented in Spark last time does not. \n",
    "  \n",
    "  Similarly the algorithms in Spark's MLLib tend to be out-of-core but _not necessarily_ online.  See the documentation link below for more information, but notable entries are:\n",
    "   - SGD for linear models: (regularized) linear regressions; logistic regressions and SVM.\n",
    "   - Naive bayes\n",
    "   - Decision trees (you should look into the exact state of random forest training, though)\n",
    "   - Neural networks (you should look into exactly which variants, and how good the learning really is...)\n",
    "   - k-means clustering\n",
    "   - non-negative matrix factorizations\n",
    "   \n",
    "#### For more details:\n",
    "http://spark.apache.org/docs/latest/mllib-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other scalable machine learning libraries\n",
    "\n",
    "The above two sets of libraries are by no means exhaustive.  However the further you move from the \"very standard\" algorithms (note the large intersection of the above two lists) the more novelty and engineering work tends to be involved -- and it's probably most practical to hope that someone else has already done it for you.  \n",
    "\n",
    "In addition to Spark/MLLib above, there is at least the following additional place to look at for this:\n",
    "\n",
    "- [Mahout](https://mahout.apache.org/) is an Apache project that aims to provide a toolbox of \"scalable\" machine learning algorithms. It used to be based on Hadoop/MR, and now aims to move towards being based on Spark.  See\n",
    "https://mahout.apache.org/users/basics/algorithms.html for a list of implemented algorithms (some in MR, some only on a single machine, at present very little is based on Spark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylib.reuters_parser import iter_minibatches, get_minibatch, stream_reuters_documents\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import ClassifierMixin \n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MINIBATCH_SIZE = 1000\n",
    "\n",
    "# Iterator over parsed Reuters SGML files.\n",
    "data_stream = stream_reuters_documents()\n",
    "\n",
    "# Create the data_stream that parses Reuters SGML files and iterates on\n",
    "# documents as a stream.\n",
    "minibatch_iterators = iter_minibatches(data_stream, MINIBATCH_SIZE)\n",
    "\n",
    "class PartialFitPipelineClassifier(ClassifierMixin):\n",
    "    \"\"\"\n",
    "    A pipeline for partial fit.\n",
    "    Ensure that the transformers do not need to be fit\n",
    "    \"\"\"\n",
    "    def __init__(self, steps):\n",
    "        self._steps = steps\n",
    "        self._transformers = steps[:-1]\n",
    "        self._estimator = steps[-1]\n",
    "        \n",
    "    def _transform(self, X):\n",
    "        Xtrans = X\n",
    "        for step in self._transformers:\n",
    "            Xtrans = step.transform(Xtrans)\n",
    "        return Xtrans\n",
    "            \n",
    "    def partial_fit(self, X, y, **kwargs):\n",
    "        Xtrans = self._transform(X)\n",
    "        return self._estimator.partial_fit(Xtrans, y, **kwargs)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Xtrans = self._transform(X)\n",
    "        return self._estimator.predict(Xtrans)\n",
    "    \n",
    "clf = PartialFitPipelineClassifier([\n",
    "    HashingVectorizer(decode_error='ignore', n_features=2 ** 18, non_negative=True),\n",
    "    SGDClassifier()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, y_test = get_minibatch(data_stream, 1000)\n",
    "all_classes = np.array([0, 1])\n",
    "times = []\n",
    "samples = []\n",
    "scores = []\n",
    "for i, (X_train, y_train) in enumerate(minibatch_iterators):\n",
    "    if i > 10:\n",
    "        break\n",
    "    start_time = time.time()\n",
    "    clf.partial_fit(X_train, y_train, classes=all_classes)\n",
    "    times += [time.time() - start_time]\n",
    "    samples += [len(y_train)]\n",
    "    scores += [clf.score(X_test, y_test)]\n",
    "    print \"Time: %2.2f secs, Iterations: %6d, Score: %f\" % (sum(times), sum(samples), scores[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(samples), scores)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Score')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.cumsum(times), scores)\n",
    "plt.xlabel('Seconds')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MINIBATCH_SIZE = 1000\n",
    "\n",
    "# Iterator over parsed Reuters SGML files.\n",
    "data_stream = stream_reuters_documents()\n",
    "\n",
    "# Create the data_stream that parses Reuters SGML files and iterates on\n",
    "# documents as a stream.\n",
    "minibatch_iterators = iter_minibatches(data_stream, MINIBATCH_SIZE)\n",
    "\n",
    "def add_vectorizer(clf):\n",
    "    return PartialFitPipelineClassifier([\n",
    "        HashingVectorizer(decode_error='ignore', n_features=2 ** 18, non_negative=True),\n",
    "        clf\n",
    "    ])\n",
    "\n",
    "classifiers = {\n",
    "    'SGD': add_vectorizer(SGDClassifier()),\n",
    "    'Perceptron': add_vectorizer(Perceptron()),\n",
    "    'NB Multinomial': add_vectorizer(MultinomialNB(alpha=0.01)),\n",
    "    'Passive-Aggressive': add_vectorizer(PassiveAggressiveClassifier()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, y_test = get_minibatch(data_stream, 1000)\n",
    "all_classes = np.array([0, 1])\n",
    "times = defaultdict(lambda: [])\n",
    "samples = defaultdict(lambda: [])\n",
    "scores = defaultdict(lambda: [])\n",
    "for i, (X_train, y_train) in enumerate(minibatch_iterators):\n",
    "    if i > 10:\n",
    "        break\n",
    "    sys.stdout.write(\"\\nIteration %2d\" % i)\n",
    "    for clf_name, clf in classifiers.iteritems():\n",
    "        sys.stdout.write(\", %s\" % clf_name)\n",
    "        start_time = time.time()\n",
    "        clf.partial_fit(X_train, y_train, classes=all_classes)\n",
    "        times[clf_name] += [time.time() - start_time]\n",
    "        samples[clf_name] += [len(y_train)]\n",
    "        scores[clf_name] += [clf.score(X_test, y_test)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf_name in classifiers:\n",
    "    plt.plot(np.cumsum(samples[clf_name]), scores[clf_name], label=clf_name)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "for clf_name in classifiers:\n",
    "    plt.plot(np.cumsum(times[clf_name]), scores[clf_name], label=clf_name)\n",
    "plt.xlabel('Seconds')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets\n",
    "1. Of the models we spoke about, which is more suitable for mapreduce, SGD or Naive Bayes?  How would you implement either?\n",
    "1. How would you adapt learning for SGD mapreduce?\n",
    "1. Explain the benefits/drawbacks of batch versus minibatch versus online/streaming learning for predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
