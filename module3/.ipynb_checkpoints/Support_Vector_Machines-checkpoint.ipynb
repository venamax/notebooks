{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machines are powerful tools in the machine learning toolbox.  We start with a simple idea to classify linearly separable data into two classes.  This is extended to non-separable data, non-linear boundaries, and multiple classes in the full-fledged SVM.  The ideas behind the SVM can be extended to regression and outlier dectection.\n",
    "\n",
    "## Maximal Margin Classifier\n",
    "\n",
    "Suppose we have two classes of observations that are linearly separable.  That means we can draw a *hyperplane* though our feature space such that all instances of one class are on one side of the hyperplane, and all instances of the other are on the opposite side.  (A hyperplace in $p$ dimensions is a $p-1$ dimensional subspace.  In the two-dimensional example that follows, a hyperplane is simply a line.)  In general, the equation for a hyperplane is\n",
    "\n",
    "$$ x \\cdot \\tilde\\beta + \\tilde\\beta_0 = 0 \\, , $$\n",
    "\n",
    "where $\\tilde\\beta$ is a $p$-vector and $\\tilde\\beta_0$ is a real number.  For convenience, we will require that $\\|\\tilde\\beta\\| = 1$, which means that the quantity $x \\cdot \\tilde\\beta + \\tilde\\beta_0$ is the distance from point $x$ to the hyperplane.  Thus we can label our classes with $y = \\pm 1$, and the requirement that the hyperplane divide the classes becomes\n",
    "\n",
    "$$ y_j \\left( X_{j\\cdot} \\cdot \\tilde\\beta + \\tilde\\beta_0 \\right) \\ge 0 \\, .$$\n",
    "\n",
    "The plot below displays two linearly separable classes and several candidate hyperplanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X1 = np.hstack((np.random.normal(1, 0.5, (2,100)), np.random.normal(-1, 0.5, (2, 100))))\n",
    "y1 = np.hstack((np.ones((1,100)), np.zeros((1,100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(*X1, c=y1, cmap=plt.cm.bwr)\n",
    "x = np.linspace(-3, 3, 100)\n",
    "plt.plot(x, -0.7*x, 'k')\n",
    "plt.plot(x, -0.15*x + 0.05, 'k--', alpha=0.5)\n",
    "plt.plot(x, -1.4*x + 0.5, 'k--', alpha=0.5)\n",
    "plt.axis((-3, 3, -3, 3))\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How should we choose the best hyperplane?\n",
    "\n",
    "The approach of the *Maximal Margin Classifier* is to choose the plane that results in the largest margin $M$ between the two classes.  That is, we choose $\\tilde\\beta_0$ and $\\tilde\\beta$ to maximize $M$, given the constraints\n",
    "\n",
    "$$ y_j \\left( X_{j\\cdot} \\cdot \\tilde\\beta + \\tilde\\beta_0 \\right) \\ge M \\, .$$\n",
    "\n",
    "Defining $\\beta \\equiv \\tilde\\beta / M$ and $\\beta_0 \\equiv \\tilde\\beta_0 / M$, we can rewrite this as\n",
    "\n",
    "$$ y_j \\left( X_{j\\cdot} \\cdot \\beta + \\beta_0 \\right) \\ge 1 \\, .$$\n",
    "\n",
    "Since $\\|\\tilde\\beta\\| = 1$, $\\|\\beta\\| = \\frac 1 M$ and we will attempt to minimize $\\|\\beta\\|$.  The following plot shows the maximum margin classifier for the previous data set.  The area shaded grey indicates the margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_SVC(X, y, C=1):\n",
    "    clf = svm.SVC(kernel='linear', C=C).fit(X.T, y.ravel())\n",
    "    b1, b2 = clf.coef_[0]\n",
    "    b0 = clf.intercept_[0]\n",
    "\n",
    "    xm = np.ceil(abs(X).max())\n",
    "    x = np.linspace(-xm, xm, 100)\n",
    "    plt.fill_between(x, (1 - b1 * x - b0) / b2, (-1 - b1 * x - b0) / b2,\n",
    "                     color='k', alpha=0.3)\n",
    "    plt.plot(x, (-b1 * x - b0) / b2, 'k')\n",
    "    plt.scatter(*clf.support_vectors_.T, s=100, c='y')\n",
    "    plt.scatter(*X, c=y, cmap=plt.cm.bwr)\n",
    "    plt.axis((-xm, xm, -xm, xm))\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_SVC(X1, y1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that only three data points, highlighted in yellow, define the separating hyperplane.  These are called the *support vectors*.  Any other data point may be moved (slightly, at least) without changing the classification boundary, but any movement of the support vectors will move the boundary.\n",
    "\n",
    "**Question:** With their dependence on only a few support vectors, do maximual margin classifers tend to be high bias or high variance?\n",
    "\n",
    "## Linear SVM\n",
    "\n",
    "In general, classes are not linearly separable.  This may be because the class boundary is not linear, but often there is no clear boundary.  To deal with this case, the support vector machine adds a set of *slack variables* $\\xi_j$, which forgive excursions of a few points into, or even across, the margin.  We want to minimize the total amount of slack while maximizing the  width of the margin.  The objective becomes\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\beta_0} \\frac 1 2 \\|\\beta\\|^2 + C \\sum_j \\xi_j \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    " y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge (1-\\xi_j) & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\xi_j \\ge 0 & \\mbox{for } j = 1,\\ldots,N \n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "for some constant $C$.  The constant $C$ represents the \"cost\" of the slack.  When $C$ is small, it is efficient to allow more points into the margin in order to acheive a larger margin.  Larger $C$ will produce boundaries with fewer support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "X2 = np.hstack((np.random.normal(1, 1, (2,100)), np.random.normal(-1, 1, (2, 100))))\n",
    "y2 = np.hstack((np.ones((1,100)), np.zeros((1,100))))\n",
    "\n",
    "plot_SVC(X2, y2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows a support vector machine dividing two overlapping classes.  Again, support vectors are highlighted in yellow.  Every observation requiring some slack, whether because it is in the margin or on the wrong side of the hyperplane, is a support vector.\n",
    "\n",
    "By increasing the number of support vectors, a SVM reduces its variance, since it depends less on any individual observation.  This can be useful even when the classes are separable.  Obverse what happens in the previous case as $C$ is changed.  Note that the two classes were draw from Gaussians centered at (1,1) and (-1,-1), so a classifer working with infinite observations would draw the boundary $x_2 = -x_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.html.widgets import interact\n",
    "\n",
    "def plot_SVC_log10C(log10C=1):\n",
    "    C = 10**log10C\n",
    "    plot_SVC(X1, y1, C)\n",
    "    plt.title('C = %g' % C)\n",
    "\n",
    "interact(plot_SVC_log10C, log10C=(-3, 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How do you select the correct value of $C$?\n",
    "\n",
    "### Implementation details\n",
    "\n",
    "We can rewrite the constrained optimization problem as the primal Lagrangian function with Lagrange multipliers $\\alpha_j \\ge 0$ and $\\mu_j \\ge 0$ for our two constraints:\n",
    "\n",
    "$$ \\min_{\\beta, \\beta_0, \\xi} \\max_{\\alpha, \\mu} \\left[\n",
    "L_P = \\frac{1}{2} \\| \\beta \\|^2 + C \\sum_j \\xi_j - \\sum_j \\alpha_j \\left[y_j (X_{j \\cdot} \\cdot \\beta + \\beta_0 - (1-\\xi_j)\\right] - \\sum_j \\mu_j \\xi_j \\right] \\, . $$\n",
    "\n",
    "By minimizing w.r.t. $\\beta$, $\\beta_0$, and $\\xi_j$, we can obtain the dual Lagrangian formulation:\n",
    "\n",
    "$$ \\max_{\\alpha} \\left[\n",
    "L_D =  \\sum_j \\alpha_j - \\frac{1}{2} \\sum_{j, j'} \\alpha_j \\alpha_{j'} y_j y_{j'} X_{j \\cdot} \\cdot X_{j' \\cdot} \\right] \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    "0 = \\sum_j \\alpha_j y_j \\\\\n",
    "0 \\le \\alpha_j \\le C & \\mbox{for } j = 1,\\ldots,N\n",
    "\\end{array}\\right. \\, .\n",
    "$$\n",
    "\n",
    "This is now a reasonably straightforward quadratic programming problem, solved via [Sequential Minimization Optimization](https://en.wikipedia.org/wiki/Sequential_minimal_optimization).  Once we have solved this problem for $\\alpha$, we can easily work out the coefficients from\n",
    "\n",
    "$$ \\beta = \\sum_j \\alpha_j y_j X_{j \\cdot} \\, . $$\n",
    "\n",
    "**Key takeaways**:\n",
    "1. Critically, only points inside the margin or on the wrong side of the margin ($j$ for which $\\xi_j > 0$) affect the SVM (see the picture).  This is intuitively clear from the picture.  In the dual form, this is because $\\alpha_j$ is the Lagrangian constraint corresponding to $y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge (1-\\xi_j)$ and Complementary Slackness shows tells us that $\\alpha_j > 0$ is non-zero only when the constraint is binding ($y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) = (1-\\xi_j)$), i.e. we're in the boundary region.  Only these support vectors contribute to the solution.\n",
    "1. $C$ gives a trade-off between the amount of forgiveness and the size of the margin or boundary region.  Hence, it controls how many points affect the SVM (based on the distance from the boundary).\n",
    "\n",
    "## Non-linear SVM\n",
    "\n",
    "What if we don't believe that our data can be cleanly split by a linear hyperplane?  The common way to incorporate non-linear features is to have a non-linear function $h$ to map the features to another (possibly higher-dimensional) space, where the data are linearly separable.  The intuition here is that the measured feature space is a projection of this higher-dimensional space, and nonlinearities in the projection result in the nonlinear boundary between classes.\n",
    "\n",
    "### Kernel trick\n",
    "\n",
    "The *kernel* of the SVM is the dot product of the feature vectors in the dual Lagrangian.  If we use the transformed features $h(X_{j \\cdot})$, the dual Lagrangian becomes\n",
    "\n",
    "$$ L_D = \\sum_j \\alpha_j - \\frac{1}{2} \\sum_j \\sum_{j'} \\alpha_j \\alpha_{j'} y_j y_{j'} h(X_{j \\cdot}) \\cdot h(X_{j' \\cdot}) \\, . $$\n",
    "\n",
    "Rather than directly computing the (potentially very large) vectors $h(X_{j \\cdot})$, we can just calculate the kernel directly:\n",
    "\n",
    "$$h(X_{j \\cdot}) \\cdot h(X_{j' \\cdot})  = K(X_{j \\cdot}, X_{j' \\cdot}) \\, ,$$ \n",
    "\n",
    "for some non-linear kernel $K$.  Our problem then becomes\n",
    "\n",
    "$$ \\max_{\\alpha} \\left[ \\sum_j \\alpha_j - \\frac{1}{2} \\sum_j \\sum_{j'} \\alpha_j \\alpha_{j'} y_j y_{j'} K(X_{j \\cdot}, X_{j' \\cdot}) \\right] \\, .$$\n",
    "\n",
    "There's a one-to-one correspondence between kernel functions and functions $h$ (although $h$'s range may be infinite dimensional).  Some common kernels include\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Kernel</th>\n",
    "<th>$K(x,x')$</th>\n",
    "<th>Scikit `kernel` parameter</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Linear Kernel</td>\n",
    "<td>$x \\cdot x'$</td>\n",
    "<td>`kernel='linear'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>$d$-th Degree Polynomial</td>\n",
    "<td>$(r + \\gamma\\, x \\cdot x')^d$</td>\n",
    "<td>`kernel='poly'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Radial Kernel</td>\n",
    "<td>$ \\exp(- \\gamma\\, \\|x - x' \\|^2) $</td>\n",
    "<td>`kernel='rbf'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Neural Network Kernel</td>\n",
    "<td>$\\tanh(\\gamma\\, x \\cdot x' + r)$</td>\n",
    "<td>`kernel='sigmoid'`</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The benefit of using a kernel is that we don't have to compute a very high-dimensional (possibly infinite-dimensional) $h$.  All that complexity is just wrapped into the function $K$.\n",
    "\n",
    "For more information on which parameters to pass the kernel, checkout [the Scikit documentation](http://scikit-learn.org/stable/modules/svm.html#svm-kernels).\n",
    "\n",
    "**Exercise:** Use non-linear kernels to produce a better classfication boundary between the two classes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_min = -.5\n",
    "x_max = 1.\n",
    "\n",
    "# we only take the first two features for visualization\n",
    "X = np.random.uniform(x_min, x_max, size=[1000,2])\n",
    "y = np.linalg.norm(X, axis=1) < .8\n",
    "\n",
    "#set our error margin\n",
    "C=1\n",
    "# linear kernel classifier\n",
    "clf = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "\n",
    "# create a mesh to plot for clf values\n",
    "h = .005  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(x_min, x_max, h))\n",
    "\n",
    "# predicted values\n",
    "zz = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, zz, cmap=plt.cm.OrRd)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.OrRd)\n",
    "plt.xlim([x_min,x_max])\n",
    "plt.ylim([x_min,x_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample code to test SVM Classification\n",
    "\n",
    "from sklearn import linear_model, cross_validation, metrics, random_projection\n",
    "import pandas as pd\n",
    "\n",
    "cv = cross_validation.ShuffleSplit(len(y), n_iter=3, test_size=0.25, random_state=42)\n",
    "C = 1.0\n",
    "\n",
    "models = [\n",
    "    (\"Logistic Regression\", linear_model.LogisticRegression()),\n",
    "    (\"Linear Kernel\", svm.SVC(kernel='linear', C=C)),\n",
    "    (\"RBF\", svm.SVC(kernel='rbf', gamma=1e-1, C=C)),\n",
    "    (\"Polynomial\", svm.SVC(kernel='poly', degree=2))\n",
    "]\n",
    "\n",
    "pd.DataFrame([\n",
    "    (name, cross_validation.cross_val_score(clf, X, y, scoring='accuracy', cv=cv).mean()) for name, clf in models\n",
    "], columns=[\"Model\", \"MSE\"]).plot(x=\"Model\", y=\"MSE\", kind=\"bar\", title=\"Accuracy of various models\", ylim=[.6,1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "1. Given a trained SVM model, how would you make predictions?  Are you able to compute the values for $\\beta$?.\n",
    "1. How does the memory and time scale with $n$, the number of samples, and $p$, the number of features for each kernel $K$?\n",
    "1. How might we reduce the computation time if one class contains the majority of samples?\n",
    "1. What happens if each of the features is on a very different scale?  How could you correct for this?\n",
    "1. Which of these kernels would benefit from subtracting the mean of each feature fom the data?\n",
    "\n",
    "## Multi-class SVM\n",
    "\n",
    "To do multi-class SVM, recall that there are generally two strategies, One-versus-One, and One-versus-All.  [Scikit uses One-versus-One (sometimes called All-versus-All strategy)](http://scikit-learn.org/stable/modules/multiclass.html).  If $f_{ij}$ is the classifier value where $i$ is a positive label ($y=1$) and $j$ is a negative label ($y=-1$), then we choose\n",
    "$$ \\mbox{argmax}_i \\sum_{j \\neq i} f_{ij}$$\n",
    "\n",
    "**Question**:\n",
    "1. Why would you choose One-versus-One or One-versus-All?  (One-versus-All requires more memory and everyone claims SVM is super-linear in memory so we're willing to run more simulations).\n",
    "1. Can you improve the accuracy by taking some of the steps outlined above?\n",
    "1. Modify the above code to do the full multi-class SVM (`SVC` automatically does multiple classes, you just have to feed in all the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating kernels\n",
    "\n",
    "The problem with the Kernel trick is that the memory required for $K$ is $O(n^2)$ and this can be both slow to compute over and expensive memory-wise.  Instead, we might choose to transform our data via an approximation $\\tilde h$ of the transformation function $h$.  For example, Scikit's `kernel_approximation.Nystroem` provides transformations that approximate each of the non-linear kernels.  Once you have the transformed features, you can pass them to `svm.LinearSVC` class to compute a linear support vector machine.\n",
    "\n",
    "For more information [Kernel Approximation](http://scikit-learn.org/stable/modules/kernel_approximation.html)\n",
    "\n",
    "**Exercise**: Load the MNIST Digits dataset.  This is a dataset of handwritten digits 0 - 9 that is used as a canonical training example:\n",
    "\n",
    "```\n",
    "digits = datasets.fetch_mldata('mnist-original')\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "```\n",
    "\n",
    "1. Try to build build a classifier using `svm.SVC` with a non-linear Kernel.  It will just take a long time ...\n",
    "1. Use `kernel_approximation.Nystroem` and `svm.LinearSVC` chained together via `pipeline.Pipeline` to build a tractable classifier.  Use grid_search to find optimal parameters.  *Hint:* to build up the learner, first restrict the number of classes and the number of examples and then slowly lift those restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression\n",
    "\n",
    "Support vector machines are also sometimes used to solve regression problems.  In regression, $y$ takes on real values instead of just $\\pm 1$.  In the classification case, we penalized the term for being either inside or on the *wrong* side of the margin (**Hinge Loss**).  In regression, we want to penalize for being too far away from the predicted value, regardless of whether you are above or below the margin  (**Well Loss**).  Below, we give a plot of the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot of the two loss functions\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def hinge_loss(x):\n",
    "    return np.maximum(0, x+1.)\n",
    "\n",
    "def well_loss(x):\n",
    "    return np.maximum(0, np.abs(x)-1.)\n",
    "\n",
    "x = np.arange(-4, 4, .1)\n",
    "plt.plot(x, hinge_loss(x), label=\"Hinge Loss\")\n",
    "plt.plot(x, well_loss(x), label=\"Well Loss\")\n",
    "plt.ylim([-1, 5])\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematics of SVM Loss\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\beta_0, \\xi_j, \\xi_j^*} \\left [ \\frac 1 2 \\|\\beta\\|^2 + C \\sum_j (\\xi_j + \\xi_j^*) \\right]\\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    " y_j - X_{j\\cdot} \\cdot \\beta - \\beta_0 \\le \\epsilon + \\xi_j & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " X_{j\\cdot} \\cdot \\beta + \\beta_0 - y_j \\le \\epsilon + \\xi_j^* & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\xi_j \\ge 0 & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\xi_j^* \\ge 0 & \\mbox{for } j = 1,\\ldots,N\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "We can perform a similar calculations to incorporate the kernel for a new feature map $h$ and real a dual quadratic programming problem.  [Here's a simple article that gives some of the details](http://alex.smola.org/papers/2003/SmoSch03b.pdf).  \n",
    "\n",
    "$$ \\max_{\\alpha} \\left[\n",
    "L_D(\\gamma) = \\sum_j y_j (\\alpha_j - \\alpha_j^*) - \\epsilon \\sum_j (\\alpha_j - \\alpha_j^*) - \\frac{1}{2} \\sum_{j, j'} (\\alpha_j - \\alpha_j^*) (\\alpha_{j'} - \\alpha_{j'}^*) K(X_{j \\cdot}, X_{j' \\cdot}) \\right] \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    "0 = \\sum_j (\\alpha_j -\\alpha_j^*) \\\\\n",
    "0 \\le \\alpha_j \\le \\gamma & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    "0 \\le \\alpha_j^* \\le \\gamma & \\mbox{for } j = 1,\\ldots,N\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "where $\\alpha_j$ and $\\alpha_j^*$ are the dual for $\\xi_j$ and $\\xi_j^*$.  The weights are given by\n",
    "$$ \\beta = \\sum_j (\\alpha_j - \\alpha_j^*) h(X_{j\\cdot})\\,. $$\n",
    "\n",
    "Details are available in the [sklearn documentation](http://scikit-learn.org/stable/modules/svm.html#regression).\n",
    "\n",
    "**Question**:\n",
    "1. Can you write prediction function $f(x)$ in terms of the kernel $K$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection using SVM\n",
    "\n",
    "A classic application of SVM is Outlier Detection, which is actually an Unsupervised Learning technique.  The reason it is unsupervised is that one usually has data for \"normal\" times (**inliers**) but not data for \"abnormal\" times (**outliers**).  For example, you might be looking at server logs for either abnormally high activity that might indicate security breaches or a failure in your code.  One doesn't really understand what those failure modes are *a priori*.  What you do have is a lot of log data for when the server is behaving normally.\n",
    "\n",
    "There's a well-known modification to a two-class SVM with a Radial Kernel turning it into a *single-class* SVM by SchÃ¶lkopf.  Mathematically, it is expressed as\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\xi, \\rho} \\frac{1}{2} \\|\\beta\\|^2 + \\frac{1}{\\nu n}\\sum_{j=1}^n \\xi_j - \\rho \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    " h(X_{j\\cdot}) \\cdot \\beta \\ge \\rho -\\xi_j & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\xi_j \\ge 0 & \\mbox{for } j = 1,\\ldots,N\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Notice that we are maximizing the value of $\\rho$ and that $\\rho$ forms a lower bound for $h(X_{j\\cdot}) \\cdot \\beta$ in our constraint.  Hence, we are pushing the points away from the origin in the transformed feature space (of course, there is the usual forgiveness in terms of $\\xi$'s).  Hence, instead of penalizing for being on the \"wrong\" side of the margin for a two-sided problem, we try to push the transformed features as far from the origin as possible.  In the original feature space, this creates a region near the training points which are considered \"regular\".  You can read more about this in the [Scikit Documentation](http://scikit-learn.org/stable/modules/outlier_detection.html).\n",
    "\n",
    "Observe that there is a parameter $\\nu$ that needs to be set.  This number sets both\n",
    "1. An upper bound on the fraction of training errors - training examples eroneously labeled as outliers.\n",
    "1. A lower bound of the fraction of support vectors - number of non-zero $\\xi$'s or $\\alpha$'s.\n",
    "\n",
    "$\\nu$ has to be set by the modeller and controls the number of false positives (training examples eroneously) and false negatives (although it is hard to know when a false negative occurs without training data).\n",
    "\n",
    "**Question:** Why are these two notions equivalent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "from sklearn import svm\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "# Generate train data\n",
    "X = 0.3 * np.random.randn(100, 2)\n",
    "X_train = np.r_[X + 2, X - 2]\n",
    "# Generate some regular novel observations\n",
    "X = 0.3 * np.random.randn(20, 2)\n",
    "X_test = np.r_[X + 2, X - 2]\n",
    "# Generate some abnormal novel observations\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "\n",
    "# fit the model\n",
    "clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_outliers)\n",
    "n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"Novelty Detection\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='orange')\n",
    "\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([a.collections[0], b1, b2, c],\n",
    "           [\"learned frontier\", \"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\",\n",
    "           prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "plt.xlabel(\n",
    "    \"error train: %d/200 ; errors novel regular: %d/40 ; \"\n",
    "    \"errors novel abnormal: %d/40\"\n",
    "    % (n_error_train, n_error_test, n_error_outliers))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets\n",
    "1. What information does an SVM model retain in memory after training?\n",
    "1. How do you decide which kernel to use?\n",
    "1. Explain to a layman what the advantages and drawbacks are of including a large margin in your optimization criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets, cross_validation, kernel_approximation, pipeline, preprocessing, random_projection\n",
    "    \n",
    "digits = datasets.fetch_mldata('mnist-original')\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X = X[y < 4]\n",
    "y = y[y < 4]\n",
    "\n",
    "N_SAMPLE=4000\n",
    "\n",
    "# shuffle the data, it'\n",
    "np.random.seed(42)\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "X = np.array(X[indices[:N_SAMPLE], :], dtype=float)\n",
    "y = y[indices[:N_SAMPLE]]\n",
    "\n",
    "model = pipeline.Pipeline([\n",
    "    (\"Kernel Approximation\", kernel_approximation.Nystroem(kernel='poly', gamma=1e-1, degree=2)),\n",
    "    (\"Linear SVC\", svm.LinearSVC(C=1e1))\n",
    "])\n",
    "\n",
    "cv = cross_validation.KFold(len(y), n_folds=5)\n",
    "cross_validation.cross_val_score(model, X, y, scoring='accuracy', cv=cv).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (A trained model are simply the values $\\alpha$.  Here's a subtle trick: in a linear model, predicting the values of a linear SVM comes from $x \\cdot \\beta = \\sum_{\\alpha_j} y_j X_{j \\cdot} \\cdot x$.  For a nonlinear SVM, this can be generalized to $\\sum_{\\alpha_j} y_j K(X_{j \\cdot}, x)$\n",
    "1. (According to the Scikit documentation, this takes [between $O(pn^2)$ and $O(pn^3)$ time](http://scikit-learn.org/stable/modules/svm.html#complexity).  It's easy to see the latter (think about the cost of computing the objective function).\n",
    "1. Downsampling the more frequently occurring class and increase its `weight` accordingly\n",
    "1. For each of the kernels, $\\gamma$ determines a characteristic scale-length.  Look at the distribution of $x \\cdot x'$ or $\\|x - x'\\|^2$ in your data and choose $\\gamma$ roughly as the inverse standard deviation.\n",
    "1. All the ones that involve an inner product, not the one that involves the difference of features.  Note that $r$ can be aliased to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
