{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s3://thedataincubator-course/mrdata/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-04-05 14:16:51--  https://s3.amazonaws.com/thedataincubator-course/mrdata/simple/part-00026.xml.bz2\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.11.131\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.11.131|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2897309 (2.8M) [application/xml]\n",
      "Saving to: ‘part-00026.xml.bz2’\n",
      "\n",
      "100%[======================================>] 2,897,309   --.-K/s   in 0.1s    \n",
      "\n",
      "2016-04-05 14:16:51 (21.9 MB/s) - ‘part-00026.xml.bz2’ saved [2897309/2897309]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/thedataincubator-course/mrdata/simple/part-00026.xml.bz2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR-Wikipedia.ipynb  part-00026.xml.bz2\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overiding steps method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pythonhosted.org/mrjob/guides/writing-mrjobs.html#multi-step-jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running job hadoop (multiple cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python job_file.py -r hadoop s3n://thedataincubator-course/mrdata/simple/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running job local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python job_file.py -r local s3n://thedataincubator-course/mrdata/simple/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some helpful articles on how mrjob works and how to pass parameters to\n",
    "your script:\n",
    "  - [How mrjob is\n",
    "    run](https://pythonhosted.org/mrjob/guides/concepts.html#how-your-program-is-run)\n",
    "  - [Adding passthrough\n",
    "  options](https://pythonhosted.org/mrjob/job.html#mrjob.job.MRJob.add_passthrough_option)\n",
    "  - [An example of someone solving similar\n",
    "  problems](http://arunxjacob.blogspot.com/2013/11/hadoop-streaming-with-mrjob.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top100_words_simple_plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top100_words_simple_plain.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top100_words_simple_plain.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "import re\n",
    "WORD_RE = re.compile(r\"\\w+\")\n",
    "\n",
    "class MRSimplePlain(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            self.increment_counter(\"Word Counter\", \"Total words\", amount=1)\n",
    "            yield (word.lower(), 1)\n",
    "    def combiner(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "    # input is: key, value\n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter(\"Word Counter\", \"Unique words\", amount=1)\n",
    "        yield (word, sum(counts))\n",
    "    def mapper2_init(self):\n",
    "        self.heap = []\n",
    "        self.nlargest=[]\n",
    "    def mapper2(self,word,counts):\n",
    "        #for pair in pairs :\n",
    "        heapq.heappush(self.heap, (word,counts))\n",
    "#        self.n_largest = heapq.nlargest(100,self.heap,key=lambda e:e[1])\n",
    "    def mapper2_final(self):\n",
    "        yield (None,self.heap)\n",
    "#        yield (None,self.n_largest)\n",
    "    \n",
    "    def reducer2_init(self):\n",
    "        self.new_heap=[]\n",
    "        self.final=[]\n",
    "    \n",
    "    def reducer2(self,_,heap):\n",
    "        count=0\n",
    "        for h in heap:\n",
    "            self.new_heap=heapq.chain(self.new_heap,h)\n",
    "        self.final=heapq.nlargest(100,self.new_heap,key=lambda e:e[1])\n",
    "    def reducer2_final(self):\n",
    "        print [(i[0], i[1]) for i in self.final]\n",
    "        yield (None,self.final)\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(mapper_init=self.mapper2_init, mapper=self.mapper2\n",
    "                 ,mapper_final=self.mapper2_final\n",
    "            ,reducer_init=self.reducer2_init\n",
    "                 , reducer=self.reducer2,reducer_final=self.reducer2_final)\n",
    "            ]\n",
    "if __name__ == '__main__':\n",
    "    MRSimplePlain.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 23), ('id', 18), ('b', 13), ('category', 12), ('marathon', 12), ('of', 11), ('it', 10), ('new', 10), ('text', 9), ('title', 9), ('york', 9), ('in', 8), ('a', 7), ('page', 7), ('city', 6), ('contributor', 6), ('format', 6), ('gt', 6), ('lt', 6), ('model', 6), ('ns', 6), ('ref', 6), ('revision', 6), ('sha1', 6), ('timestamp', 6), ('username', 6), ('was', 6), ('2013', 5), ('bism', 5), ('by', 5), ('http', 5), ('ing', 5), ('is', 5), ('suffolk', 5), ('to', 5), ('11', 4), ('and', 4), ('comment', 4), ('november', 4), ('parentid', 4), ('quot', 4), ('sports', 4), ('www', 4), ('2012', 3), ('bridge', 3), ('cancelled', 3), ('com', 3), ('for', 3), ('from', 3), ('on', 3), ('people', 3), ('preserve', 3), ('religion', 3), ('space', 3), ('sportspeople', 3), ('verrazano', 3), ('wiki', 3), ('wikitext', 3), ('x', 3), ('xml', 3), ('0', 2), ('2010', 2), ('at', 2), ('because', 2), ('been', 2), ('cite', 2), ('english', 2), ('events', 2), ('has', 2), ('he', 2), ('hotcat', 2), ('htm', 2), ('interpretation', 2), ('islam', 2), ('location', 2), ('narrows', 2), ('nyc', 2), ('nycmarathon', 2), ('official', 2), ('org', 2), ('origin', 2), ('other', 2), ('persia', 2), ('publisher', 2), ('race', 2), ('religions', 2), ('runners', 2), ('stub', 2), ('thumb', 2), ('url', 2), ('used', 2), ('web', 2), ('with', 2), ('08z', 1), ('10', 1), ('109566', 1), ('10t00', 1), ('10t08', 1), ('11t02', 1), ('14', 1)]\n",
      "[{'Word Counter': {'Total words': 608, 'Unique words': 276}}, {}]\n",
      "--- 0.00179255008698 min ---\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 4\n",
    "from top100_words_simple_plain import MRSimplePlain\n",
    "filename = 'sample.xml'\n",
    "##filename = 'part-00026.xml'\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "mr_job = MRSimplePlain(args=[filename])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    print runner.counters()\n",
    "    ##for line in runner.stream_output():\n",
    "        ##print line\n",
    "\n",
    "print(\"--- %s min ---\" % (((time.time() - start_time))/60)  )  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /home/vagrant/.mrjob.conf\n",
      "creating tmp directory /tmp/top100_words_simple_plain.vagrant.20160407.221138.022974\n",
      "writing wrapper script to /tmp/top100_words_simple_plain.vagrant.20160407.221138.022974/setup-wrapper.sh\n",
      "Using Hadoop version 2.5.0\n",
      "Copying local files into hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_plain.vagrant.20160407.221138.022974/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar] /tmp/streamjob1267738755138315539.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Permissions on staging directory /tmp/hadoop-yarn/staging/vagrant/.staging are incorrect: rwxrwxrwt. Fixing permissions to correct value rwx------\n",
      "HADOOP: Total input paths to process : 27\n",
      "HADOOP: number of splits:27\n",
      "HADOOP: Submitting tokens for job: job_1457060327319_0003\n",
      "HADOOP: Submitted application application_1457060327319_0003\n",
      "HADOOP: The url to track the job: http://alejandro-j-rojas:8088/proxy/application_1457060327319_0003/\n",
      "HADOOP: Running job: job_1457060327319_0003\n",
      "HADOOP: Job job_1457060327319_0003 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 1% reduce 0%\n",
      "HADOOP:  map 2% reduce 0%\n",
      "HADOOP:  map 3% reduce 0%\n",
      "HADOOP:  map 4% reduce 0%\n",
      "HADOOP:  map 5% reduce 0%\n",
      "HADOOP:  map 6% reduce 0%\n",
      "HADOOP:  map 7% reduce 0%\n",
      "HADOOP:  map 8% reduce 0%\n",
      "HADOOP:  map 9% reduce 0%\n",
      "HADOOP:  map 10% reduce 0%\n",
      "HADOOP:  map 11% reduce 0%\n",
      "HADOOP:  map 12% reduce 0%\n",
      "HADOOP:  map 13% reduce 0%\n",
      "HADOOP:  map 14% reduce 0%\n",
      "HADOOP:  map 15% reduce 0%\n",
      "HADOOP:  map 16% reduce 0%\n",
      "HADOOP:  map 17% reduce 0%\n",
      "HADOOP:  map 20% reduce 0%\n",
      "HADOOP:  map 22% reduce 0%\n",
      "HADOOP:  map 23% reduce 0%\n",
      "HADOOP:  map 24% reduce 0%\n",
      "HADOOP:  map 25% reduce 0%\n",
      "HADOOP:  map 26% reduce 0%\n",
      "HADOOP:  map 27% reduce 0%\n",
      "HADOOP:  map 28% reduce 0%\n",
      "HADOOP:  map 29% reduce 0%\n",
      "HADOOP:  map 30% reduce 0%\n",
      "HADOOP:  map 31% reduce 0%\n",
      "HADOOP:  map 32% reduce 0%\n",
      "HADOOP:  map 33% reduce 0%\n",
      "HADOOP:  map 34% reduce 0%\n",
      "HADOOP:  map 35% reduce 0%\n",
      "HADOOP:  map 36% reduce 0%\n",
      "HADOOP:  map 37% reduce 0%\n",
      "HADOOP:  map 41% reduce 0%\n",
      "HADOOP:  map 42% reduce 0%\n",
      "HADOOP:  map 43% reduce 0%\n",
      "HADOOP:  map 44% reduce 0%\n",
      "HADOOP:  map 45% reduce 15%\n",
      "HADOOP:  map 46% reduce 15%\n",
      "HADOOP:  map 47% reduce 15%\n",
      "HADOOP:  map 48% reduce 15%\n",
      "HADOOP:  map 49% reduce 15%\n",
      "HADOOP:  map 50% reduce 15%\n",
      "HADOOP:  map 51% reduce 15%\n",
      "HADOOP:  map 52% reduce 15%\n",
      "HADOOP:  map 53% reduce 15%\n",
      "HADOOP:  map 54% reduce 15%\n",
      "HADOOP:  map 55% reduce 15%\n",
      "HADOOP:  map 56% reduce 15%\n",
      "HADOOP:  map 57% reduce 15%\n",
      "HADOOP:  map 58% reduce 15%\n",
      "HADOOP:  map 59% reduce 15%\n",
      "HADOOP:  map 59% reduce 17%\n",
      "HADOOP:  map 60% reduce 17%\n",
      "HADOOP:  map 63% reduce 17%\n",
      "HADOOP:  map 63% reduce 21%\n",
      "HADOOP:  map 64% reduce 21%\n",
      "HADOOP:  map 65% reduce 21%\n",
      "HADOOP:  map 66% reduce 21%\n",
      "HADOOP:  map 67% reduce 21%\n",
      "HADOOP:  map 68% reduce 21%\n",
      "HADOOP:  map 69% reduce 21%\n",
      "HADOOP:  map 70% reduce 21%\n",
      "HADOOP:  map 71% reduce 21%\n",
      "HADOOP:  map 72% reduce 21%\n",
      "HADOOP:  map 73% reduce 21%\n",
      "HADOOP:  map 74% reduce 21%\n",
      "HADOOP:  map 75% reduce 21%\n",
      "HADOOP:  map 77% reduce 21%\n",
      "HADOOP:  map 78% reduce 21%\n",
      "HADOOP:  map 78% reduce 23%\n",
      "HADOOP:  map 79% reduce 23%\n",
      "HADOOP:  map 80% reduce 23%\n",
      "HADOOP:  map 81% reduce 26%\n",
      "HADOOP:  map 82% reduce 26%\n",
      "HADOOP:  map 82% reduce 27%\n",
      "HADOOP:  map 83% reduce 27%\n",
      "HADOOP:  map 84% reduce 27%\n",
      "HADOOP:  map 85% reduce 27%\n",
      "HADOOP:  map 86% reduce 27%\n",
      "HADOOP:  map 87% reduce 27%\n",
      "HADOOP:  map 88% reduce 27%\n",
      "HADOOP:  map 89% reduce 27%\n",
      "HADOOP:  map 90% reduce 27%\n",
      "HADOOP:  map 91% reduce 27%\n",
      "HADOOP:  map 92% reduce 27%\n",
      "HADOOP:  map 93% reduce 27%\n",
      "HADOOP:  map 94% reduce 27%\n",
      "HADOOP:  map 95% reduce 27%\n",
      "HADOOP:  map 96% reduce 28%\n",
      "HADOOP:  map 98% reduce 28%\n",
      "HADOOP:  map 98% reduce 31%\n",
      "HADOOP:  map 99% reduce 31%\n",
      "HADOOP:  map 100% reduce 32%\n",
      "HADOOP:  map 100% reduce 67%\n",
      "HADOOP:  map 100% reduce 69%\n",
      "HADOOP:  map 100% reduce 70%\n",
      "HADOOP:  map 100% reduce 72%\n",
      "HADOOP:  map 100% reduce 73%\n",
      "HADOOP:  map 100% reduce 74%\n",
      "HADOOP:  map 100% reduce 76%\n",
      "HADOOP:  map 100% reduce 79%\n",
      "HADOOP:  map 100% reduce 82%\n",
      "HADOOP:  map 100% reduce 85%\n",
      "HADOOP:  map 100% reduce 88%\n",
      "HADOOP:  map 100% reduce 90%\n",
      "HADOOP:  map 100% reduce 93%\n",
      "HADOOP:  map 100% reduce 95%\n",
      "HADOOP:  map 100% reduce 98%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1457060327319_0003 completed successfully\n",
      "HADOOP: Counters: 55\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=50179560\n",
      "HADOOP: \t\tFILE: Number of bytes written=103448086\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=3078\n",
      "HADOOP: \t\tHDFS: Number of bytes written=24441664\n",
      "HADOOP: \t\tHDFS: Number of read operations=57\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \t\tS3N: Number of bytes read=97450287\n",
      "HADOOP: \t\tS3N: Number of bytes written=0\n",
      "HADOOP: \t\tS3N: Number of read operations=0\n",
      "HADOOP: \t\tS3N: Number of large read operations=0\n",
      "HADOOP: \t\tS3N: Number of write operations=0\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tKilled map tasks=1\n",
      "HADOOP: \t\tLaunched map tasks=28\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tRack-local map tasks=28\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=1338735\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=202989\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=1338735\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=202989\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=1338735\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=202989\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=1370864640\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=207860736\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=9103899\n",
      "HADOOP: \t\tMap output records=62199836\n",
      "HADOOP: \t\tMap output bytes=614521272\n",
      "HADOOP: \t\tMap output materialized bytes=50179716\n",
      "HADOOP: \t\tInput split bytes=3078\n",
      "HADOOP: \t\tCombine input records=62199836\n",
      "HADOOP: \t\tCombine output records=3176508\n",
      "HADOOP: \t\tReduce input groups=1584646\n",
      "HADOOP: \t\tReduce shuffle bytes=50179716\n",
      "HADOOP: \t\tReduce input records=3176508\n",
      "HADOOP: \t\tReduce output records=1584646\n",
      "HADOOP: \t\tSpilled Records=6353016\n",
      "HADOOP: \t\tShuffled Maps =27\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=27\n",
      "HADOOP: \t\tGC time elapsed (ms)=5373\n",
      "HADOOP: \t\tCPU time spent (ms)=1372150\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=8005894144\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=54410747904\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=5467275264\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=97450287\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=24441664\n",
      "HADOOP: Output directory: hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_plain.vagrant.20160407.221138.022974/step-output/1\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar] /tmp/streamjob2041152660775551340.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1457060327319_0004\n",
      "HADOOP: Submitted application application_1457060327319_0004\n",
      "HADOOP: The url to track the job: http://alejandro-j-rojas:8088/proxy/application_1457060327319_0004/\n",
      "HADOOP: Running job: job_1457060327319_0004\n",
      "HADOOP: Job job_1457060327319_0004 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 19% reduce 0%\n",
      "HADOOP:  map 45% reduce 0%\n",
      "HADOOP:  map 59% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1457060327319_0004 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=30780282\n",
      "HADOOP: \t\tFILE: Number of bytes written=61890436\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=24443660\n",
      "HADOOP: \t\tHDFS: Number of bytes written=1761\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=23384\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=5030\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=23384\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=5030\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=23384\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=5030\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=23945216\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=5150720\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=1584646\n",
      "HADOOP: \t\tMap output records=2\n",
      "HADOOP: \t\tMap output bytes=30780266\n",
      "HADOOP: \t\tMap output materialized bytes=30780288\n",
      "HADOOP: \t\tInput split bytes=364\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=1\n",
      "HADOOP: \t\tReduce shuffle bytes=30780288\n",
      "HADOOP: \t\tReduce input records=2\n",
      "HADOOP: \t\tReduce output records=1\n",
      "HADOOP: \t\tSpilled Records=4\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=258\n",
      "HADOOP: \t\tCPU time spent (ms)=19110\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=816525312\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=5797429248\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=593494016\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=24443296\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=1761\n",
      "HADOOP: Output directory: hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_plain.vagrant.20160407.221138.022974/output\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_plain.vagrant.20160407.221138.022974/output\n",
      "null\t[[\"the\", 1596419], [\"quot\", 1400092], [\"gt\", 1211888], [\"lt\", 1205656], [\"id\", 1142905], [\"of\", 972204], [\"in\", 659218], [\"and\", 634202], [\"text\", 604352], [\"a\", 581510], [\"title\", 539917], [\"to\", 489127], [\"page\", 439939], [\"is\", 407340], [\"format\", 386311], [\"model\", 381564], [\"category\", 380599], [\"revision\", 380467], [\"ns\", 378544], [\"timestamp\", 377863], [\"contributor\", 377226], [\"sha1\", 376839], [\"ref\", 370336], [\"username\", 361677], [\"0\", 358489], [\"comment\", 350146], [\"s\", 295792], [\"parentid\", 292767], [\"1\", 250429], [\"on\", 244426], [\"amp\", 243908], [\"it\", 218535], [\"by\", 218491], [\"http\", 217221], [\"for\", 216011], [\"was\", 213411], [\"2\", 203461], [\"x\", 202304], [\"wiki\", 199562], [\"space\", 197216], [\"xml\", 189061], [\"preserve\", 188766], [\"wikitext\", 188566], [\"name\", 180142], [\"d\", 169346], [\"br\", 166273], [\"that\", 160304], [\"as\", 159125], [\"www\", 156239], [\"user\", 154075], [\"2013\", 153752], [\"font\", 153538], [\"from\", 152852], [\"align\", 148689], [\"with\", 137882], [\"style\", 136602], [\"are\", 134268], [\"he\", 126278], [\"this\", 123821], [\"talk\", 123576], [\"com\", 122914], [\"3\", 122138], [\"color\", 121533], [\"i\", 119877], [\"minor\", 117167], [\"or\", 113222], [\"at\", 112928], [\"center\", 111562], [\"redirect\", 111336], [\"be\", 109563], [\"small\", 102848], [\"an\", 101364], [\"4\", 95648], [\"not\", 95261], [\"new\", 94893], [\"people\", 94283], [\"10\", 93360], [\"football\", 90184], [\"nbsp\", 89135], [\"united\", 86507], [\"b\", 85584], [\"first\", 83424], [\"cite\", 83289], [\"other\", 82511], [\"date\", 82360], [\"5\", 82212], [\"url\", 79642], [\"03\", 77703], [\"if\", 77047], [\"they\", 76751], [\"his\", 75778], [\"span\", 74840], [\"now\", 74596], [\"american\", 73474], [\"states\", 72699], [\"one\", 71327], [\"stub\", 70368], [\"2009\", 69337], [\"moved\", 69255], [\"have\", 69087]]\n",
      "removing tmp directory /tmp/top100_words_simple_plain.vagrant.20160407.221138.022974\n",
      "deleting hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_plain.vagrant.20160407.221138.022974 from HDFS\n"
     ]
    }
   ],
   "source": [
    "! python top100_words_simple_plain.py -r hadoop s3n://thedataincubator-course/mrdata/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## top100_words_simple_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top100_words_simple_text.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top100_words_simple_text.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "import re\n",
    "import lxml\n",
    "from lxml import etree\n",
    "WORD_RE = re.compile(r\"\\w+\")\n",
    "\n",
    "\n",
    "class MRSimpleText(MRJob):\n",
    "    def mapper_page_init(self):\n",
    "        self.page = \"\"\n",
    "        self.counter = 0\n",
    "    \n",
    "    def mapper_page (self,_,line):\n",
    "        if '<page>' in line : \n",
    "            self.page = line\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            self.page = self.page +\" \"+ line\n",
    "        \n",
    "        if '</page>' in line :           \n",
    "            yield (self.counter,self.page)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def reducer_page(self, _, pchunk):\n",
    "        for chunk in pchunk:\n",
    "            self.increment_counter(\"Word Counter\", \"Pages Reviewed\", amount=1)\n",
    "            texts = etree.fromstring(chunk)\n",
    "            ##if '<revision>'in texts:\n",
    "            texts = texts.findall(\".//revision\")[-1]\n",
    "            texts = texts.findall(\".//text\")\n",
    "            text_string = etree.tostring(texts[0])\n",
    "            yield (None,text_string)\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            self.increment_counter(\"Word Counter\", \"Total words\", amount=1)\n",
    "            yield (word.lower(), 1)\n",
    "    # input is: key, value\n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter(\"Word Counter\", \"Unique words\", amount=1)\n",
    "        yield (word, sum(counts))\n",
    " \n",
    "    def mapper2_init(self):\n",
    "        self.heap = []\n",
    "        self.nlargest=[]\n",
    "    def mapper2(self,word,counts):\n",
    "        #for pair in pairs :\n",
    "        heapq.heappush(self.heap, (word,counts))\n",
    "#        self.n_largest = heapq.nlargest(100,self.heap,key=lambda e:e[1])\n",
    "    def mapper2_final(self):\n",
    "        n_largest = heapq.nlargest(100,self.heap,key=lambda e:e[1])\n",
    "        yield (None,n_largest)\n",
    "#        yield (None,self.n_largest)\n",
    "    \n",
    "    def reducer2_init(self):\n",
    "        self.new_heap=[]\n",
    "        self.final=[]\n",
    "    \n",
    "    def reducer2(self,_,heap):\n",
    "        count=0\n",
    "        for h in heap:\n",
    "            self.new_heap=heapq.chain(self.new_heap,h)\n",
    "        self.final=heapq.nlargest(100,self.new_heap,key=lambda e:e[1])\n",
    "    def reducer2_final(self):\n",
    "        print [(i[0], i[1]) for i in self.final]\n",
    "        yield (None,self.final)\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_page_init,\n",
    "                   mapper=self.mapper_page,\n",
    "                   reducer=self.reducer_page\n",
    "                  ),\n",
    "            MRStep(mapper=self.mapper,\n",
    "            #       combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(mapper_init=self.mapper2_init, mapper=self.mapper2\n",
    "                 ,mapper_final=self.mapper2_final\n",
    "            ,reducer_init=self.reducer2_init\n",
    "                 , reducer=self.reducer2,reducer_final=self.reducer2_final)\n",
    "            ]\n",
    "if __name__ == '__main__':\n",
    "    MRSimpleText.run()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 45047), ('lt', 32247), ('gt', 32218), ('of', 28145), ('in', 20342), ('ref', 18862), ('and', 16703), ('category', 16079), ('text', 15050), ('a', 14211), ('to', 11198), ('is', 10629), ('was', 9724), ('http', 8973), ('amp', 8074), ('space', 7430), ('s', 7354), ('preserve', 7327), ('xml', 7298), ('name', 7192), ('on', 6731), ('it', 6656), ('www', 6284), ('for', 5605), ('he', 5439), ('title', 5266), ('from', 5225), ('com', 5211), ('2013', 4667), ('by', 4465), ('as', 4323), ('br', 4049), ('url', 4006), ('at', 3853), ('2014', 3826), ('people', 3735), ('align', 3683), ('that', 3593), ('1', 3585), ('8211', 3578), ('an', 3562), ('with', 3421), ('accessdate', 3385), ('publisher', 3379), ('cite', 3371), ('center', 3367), ('this', 3334), ('date', 3137), ('american', 3041), ('stub', 2961), ('united', 2872), ('be', 2860), ('2', 2811), ('new', 2772), ('other', 2750), ('talk', 2748), ('references', 2745), ('web', 2690), ('his', 2669), ('user', 2614), ('states', 2601), ('first', 2562), ('are', 2519), ('jpg', 2383), ('reflist', 2354), ('p', 2254), ('0', 2163), ('not', 2156), ('or', 2152), ('south', 2109), ('3', 2050), ('style', 1979), ('file', 1953), ('january', 1940), ('defaultsort', 1925), ('small', 1919), ('they', 1864), ('10', 1863), ('year', 1848), ('html', 1846), ('one', 1843), ('span', 1827), ('february', 1817), ('were', 1812), ('born', 1810), ('redirect', 1806), ('page', 1798), ('wwe', 1773), ('news', 1770), ('11', 1746), ('12', 1735), ('history', 1731), ('she', 1725), ('movie', 1722), ('websites', 1720), ('4', 1713), ('image', 1712), ('world', 1689), ('2010', 1614), ('also', 1609)]\n",
      "[{'Word Counter': {'Pages Reviewed': 7288}}, {'Word Counter': {'Total words': 1444175, 'Unique words': 73887}}, {}]\n",
      "--- 0.729804869493 min ---\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 4\n",
    "from top100_words_simple_text import MRSimpleText\n",
    "##filename = 'sample.xml'\n",
    "filename = 'part-00026.xml'\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "mr_job = MRSimpleText(args=[filename])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    print runner.counters()\n",
    "    ##for line in runner.stream_output():\n",
    "        ##print line\n",
    "\n",
    "print(\"--- %s min ---\" % (((time.time() - start_time))/60)  )  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /home/vagrant/.mrjob.conf\n",
      "creating tmp directory /tmp/top100_words_simple_text.vagrant.20160410.031026.176339\n",
      "writing wrapper script to /tmp/top100_words_simple_text.vagrant.20160410.031026.176339/setup-wrapper.sh\n",
      "Using Hadoop version 2.5.0\n",
      "Copying local files into hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_text.vagrant.20160410.031026.176339/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar] /tmp/streamjob2566697572191504525.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Permissions on staging directory /tmp/hadoop-yarn/staging/vagrant/.staging are incorrect: rwxrwxrwt. Fixing permissions to correct value rwx------\n",
      "HADOOP: Total input paths to process : 27\n",
      "HADOOP: number of splits:27\n",
      "HADOOP: Submitting tokens for job: job_1457060327319_0043\n",
      "HADOOP: Submitted application application_1457060327319_0043\n",
      "HADOOP: The url to track the job: http://alejandro-j-rojas:8088/proxy/application_1457060327319_0043/\n",
      "HADOOP: Running job: job_1457060327319_0043\n",
      "HADOOP: Job job_1457060327319_0043 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 4% reduce 0%\n",
      "HADOOP:  map 7% reduce 0%\n",
      "HADOOP:  map 11% reduce 0%\n",
      "HADOOP:  map 15% reduce 0%\n",
      "HADOOP:  map 19% reduce 0%\n",
      "HADOOP:  map 22% reduce 0%\n",
      "HADOOP:  map 26% reduce 0%\n",
      "HADOOP:  map 30% reduce 0%\n",
      "HADOOP:  map 33% reduce 0%\n",
      "HADOOP:  map 37% reduce 0%\n",
      "HADOOP:  map 41% reduce 0%\n",
      "HADOOP:  map 41% reduce 14%\n",
      "HADOOP:  map 44% reduce 14%\n",
      "HADOOP:  map 48% reduce 14%\n",
      "HADOOP:  map 52% reduce 15%\n",
      "HADOOP:  map 56% reduce 15%\n",
      "HADOOP:  map 59% reduce 19%\n",
      "HADOOP:  map 63% reduce 20%\n",
      "HADOOP:  map 67% reduce 20%\n",
      "HADOOP:  map 70% reduce 22%\n",
      "HADOOP:  map 74% reduce 22%\n",
      "HADOOP:  map 78% reduce 25%\n",
      "HADOOP:  map 81% reduce 25%\n",
      "HADOOP:  map 81% reduce 27%\n",
      "HADOOP:  map 85% reduce 27%\n",
      "HADOOP:  map 89% reduce 28%\n",
      "HADOOP:  map 93% reduce 28%\n",
      "HADOOP:  map 96% reduce 28%\n",
      "HADOOP:  map 96% reduce 31%\n",
      "HADOOP:  map 100% reduce 31%\n",
      "HADOOP:  map 100% reduce 67%\n",
      "HADOOP:  map 100% reduce 70%\n",
      "HADOOP:  map 100% reduce 72%\n",
      "HADOOP:  map 100% reduce 74%\n",
      "HADOOP:  map 100% reduce 77%\n",
      "HADOOP:  map 100% reduce 79%\n",
      "HADOOP:  map 100% reduce 81%\n",
      "HADOOP:  map 100% reduce 83%\n",
      "HADOOP:  map 100% reduce 85%\n",
      "HADOOP:  map 100% reduce 88%\n",
      "HADOOP:  map 100% reduce 90%\n",
      "HADOOP:  map 100% reduce 92%\n",
      "HADOOP:  map 100% reduce 94%\n",
      "HADOOP:  map 100% reduce 96%\n",
      "HADOOP:  map 100% reduce 98%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1457060327319_0043 completed successfully\n",
      "HADOOP: Counters: 55\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=456720141\n",
      "HADOOP: \t\tFILE: Number of bytes written=916517856\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=3078\n",
      "HADOOP: \t\tHDFS: Number of bytes written=348051549\n",
      "HADOOP: \t\tHDFS: Number of read operations=57\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \t\tS3N: Number of bytes read=97450287\n",
      "HADOOP: \t\tS3N: Number of bytes written=0\n",
      "HADOOP: \t\tS3N: Number of read operations=0\n",
      "HADOOP: \t\tS3N: Number of large read operations=0\n",
      "HADOOP: \t\tS3N: Number of write operations=0\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=27\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tRack-local map tasks=27\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=202143\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=81431\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=202143\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=81431\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=202143\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=81431\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=206994432\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=83385344\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=9103899\n",
      "HADOOP: \t\tMap output records=188418\n",
      "HADOOP: \t\tMap output bytes=455966116\n",
      "HADOOP: \t\tMap output materialized bytes=456720273\n",
      "HADOOP: \t\tInput split bytes=3078\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=10716\n",
      "HADOOP: \t\tReduce shuffle bytes=456720273\n",
      "HADOOP: \t\tReduce input records=188418\n",
      "HADOOP: \t\tReduce output records=188418\n",
      "HADOOP: \t\tSpilled Records=376836\n",
      "HADOOP: \t\tShuffled Maps =27\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=27\n",
      "HADOOP: \t\tGC time elapsed (ms)=5189\n",
      "HADOOP: \t\tCPU time spent (ms)=193230\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=8175484928\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=54463791104\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=5604638720\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tWord Counter\n",
      "HADOOP: \t\tPages Reviewed=188418\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=97450287\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=348051549\n",
      "HADOOP: Output directory: hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_text.vagrant.20160410.031026.176339/step-output/1\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar] /tmp/streamjob471265278359130035.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:3\n",
      "HADOOP: Submitting tokens for job: job_1457060327319_0044\n",
      "HADOOP: Submitted application application_1457060327319_0044\n",
      "HADOOP: The url to track the job: http://alejandro-j-rojas:8088/proxy/application_1457060327319_0044/\n",
      "HADOOP: Running job: job_1457060327319_0044\n",
      "HADOOP: Job job_1457060327319_0044 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 1% reduce 0%\n",
      "HADOOP:  map 2% reduce 0%\n",
      "HADOOP:  map 3% reduce 0%\n",
      "HADOOP:  map 4% reduce 0%\n",
      "HADOOP:  map 5% reduce 0%\n",
      "HADOOP:  map 6% reduce 0%\n",
      "HADOOP:  map 7% reduce 0%\n",
      "HADOOP:  map 8% reduce 0%\n",
      "HADOOP:  map 9% reduce 0%\n",
      "HADOOP:  map 10% reduce 0%\n",
      "HADOOP:  map 11% reduce 0%\n",
      "HADOOP:  map 12% reduce 0%\n",
      "HADOOP:  map 13% reduce 0%\n",
      "HADOOP:  map 14% reduce 0%\n",
      "HADOOP:  map 15% reduce 0%\n",
      "HADOOP:  map 16% reduce 0%\n",
      "HADOOP:  map 17% reduce 0%\n",
      "HADOOP:  map 18% reduce 0%\n",
      "HADOOP:  map 19% reduce 0%\n",
      "HADOOP:  map 20% reduce 0%\n",
      "HADOOP:  map 21% reduce 0%\n",
      "HADOOP:  map 22% reduce 0%\n",
      "HADOOP:  map 23% reduce 0%\n",
      "HADOOP:  map 24% reduce 0%\n",
      "HADOOP:  map 25% reduce 0%\n",
      "HADOOP:  map 26% reduce 0%\n",
      "HADOOP:  map 27% reduce 0%\n",
      "HADOOP:  map 28% reduce 0%\n",
      "HADOOP:  map 29% reduce 0%\n",
      "HADOOP:  map 30% reduce 0%\n",
      "HADOOP:  map 31% reduce 0%\n",
      "HADOOP:  map 32% reduce 0%\n",
      "HADOOP:  map 33% reduce 0%\n",
      "HADOOP:  map 34% reduce 0%\n",
      "HADOOP:  map 35% reduce 0%\n",
      "HADOOP:  map 36% reduce 0%\n",
      "HADOOP:  map 37% reduce 0%\n",
      "HADOOP:  map 38% reduce 0%\n",
      "HADOOP:  map 39% reduce 0%\n",
      "HADOOP:  map 40% reduce 0%\n",
      "HADOOP:  map 41% reduce 0%\n",
      "HADOOP:  map 42% reduce 0%\n",
      "HADOOP:  map 43% reduce 0%\n",
      "HADOOP:  map 44% reduce 0%\n",
      "HADOOP:  map 45% reduce 0%\n",
      "HADOOP:  map 46% reduce 0%\n",
      "HADOOP:  map 47% reduce 0%\n",
      "HADOOP:  map 48% reduce 0%\n",
      "HADOOP:  map 50% reduce 0%\n",
      "HADOOP:  map 55% reduce 0%\n",
      "HADOOP:  map 60% reduce 0%\n",
      "HADOOP:  map 61% reduce 0%\n",
      "HADOOP:  map 61% reduce 11%\n",
      "HADOOP:  map 62% reduce 11%\n",
      "HADOOP:  map 63% reduce 11%\n",
      "HADOOP:  map 64% reduce 11%\n",
      "HADOOP:  map 65% reduce 11%\n",
      "HADOOP:  map 66% reduce 11%\n",
      "HADOOP:  map 67% reduce 11%\n",
      "HADOOP:  map 68% reduce 11%\n",
      "HADOOP:  map 69% reduce 11%\n",
      "HADOOP:  map 70% reduce 11%\n",
      "HADOOP:  map 71% reduce 11%\n",
      "HADOOP:  map 72% reduce 11%\n",
      "HADOOP:  map 73% reduce 11%\n",
      "HADOOP:  map 74% reduce 11%\n",
      "HADOOP:  map 75% reduce 11%\n",
      "HADOOP:  map 76% reduce 11%\n",
      "HADOOP:  map 77% reduce 11%\n",
      "HADOOP:  map 79% reduce 11%\n",
      "HADOOP:  map 82% reduce 11%\n",
      "HADOOP:  map 84% reduce 11%\n",
      "HADOOP:  map 87% reduce 11%\n",
      "HADOOP:  map 89% reduce 11%\n",
      "HADOOP:  map 89% reduce 22%\n",
      "HADOOP:  map 91% reduce 22%\n",
      "HADOOP:  map 94% reduce 22%\n",
      "HADOOP:  map 96% reduce 22%\n",
      "HADOOP:  map 99% reduce 22%\n",
      "HADOOP:  map 100% reduce 22%\n",
      "HADOOP:  map 100% reduce 67%\n",
      "HADOOP:  map 100% reduce 68%\n",
      "HADOOP:  map 100% reduce 69%\n",
      "HADOOP:  map 100% reduce 70%\n",
      "HADOOP:  map 100% reduce 71%\n",
      "HADOOP:  map 100% reduce 72%\n",
      "HADOOP:  map 100% reduce 73%\n",
      "HADOOP:  map 100% reduce 74%\n",
      "HADOOP:  map 100% reduce 75%\n",
      "HADOOP:  map 100% reduce 76%\n",
      "HADOOP:  map 100% reduce 77%\n",
      "HADOOP:  map 100% reduce 78%\n",
      "HADOOP:  map 100% reduce 79%\n",
      "HADOOP:  map 100% reduce 80%\n",
      "HADOOP:  map 100% reduce 81%\n",
      "HADOOP:  map 100% reduce 82%\n",
      "HADOOP:  map 100% reduce 83%\n",
      "HADOOP:  map 100% reduce 84%\n",
      "HADOOP:  map 100% reduce 85%\n",
      "HADOOP:  map 100% reduce 86%\n",
      "HADOOP:  map 100% reduce 87%\n",
      "HADOOP:  map 100% reduce 88%\n",
      "HADOOP:  map 100% reduce 89%\n",
      "HADOOP:  map 100% reduce 90%\n",
      "HADOOP:  map 100% reduce 91%\n",
      "HADOOP:  map 100% reduce 92%\n",
      "HADOOP:  map 100% reduce 93%\n",
      "HADOOP:  map 100% reduce 94%\n",
      "HADOOP:  map 100% reduce 95%\n",
      "HADOOP:  map 100% reduce 97%\n",
      "HADOOP:  map 100% reduce 98%\n",
      "HADOOP:  map 100% reduce 99%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1457060327319_0044 completed successfully\n",
      "HADOOP: Counters: 52\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=1188854304\n",
      "HADOOP: \t\tFILE: Number of bytes written=1788365211\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=348060284\n",
      "HADOOP: \t\tHDFS: Number of bytes written=11814343\n",
      "HADOOP: \t\tHDFS: Number of read operations=12\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tKilled map tasks=2\n",
      "HADOOP: \t\tLaunched map tasks=5\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=5\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=1817598\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=544066\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=1817598\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=544066\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=1817598\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=544066\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=1861220352\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=557123584\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=188418\n",
      "HADOOP: \t\tMap output records=50663401\n",
      "HADOOP: \t\tMap output bytes=493100208\n",
      "HADOOP: \t\tMap output materialized bytes=594427110\n",
      "HADOOP: \t\tInput split bytes=543\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=863952\n",
      "HADOOP: \t\tReduce shuffle bytes=594427110\n",
      "HADOOP: \t\tReduce input records=50663401\n",
      "HADOOP: \t\tReduce output records=863952\n",
      "HADOOP: \t\tSpilled Records=151990203\n",
      "HADOOP: \t\tShuffled Maps =3\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=3\n",
      "HADOOP: \t\tGC time elapsed (ms)=7681\n",
      "HADOOP: \t\tCPU time spent (ms)=2271100\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=1001607168\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=7744724992\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=741867520\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tWord Counter\n",
      "HADOOP: \t\tTotal words=50663401\n",
      "HADOOP: \t\tUnique words=863952\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=348059741\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=11814343\n",
      "HADOOP: Output directory: hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_text.vagrant.20160410.031026.176339/step-output/2\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar] /tmp/streamjob7747759013988868476.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1457060327319_0045\n",
      "HADOOP: Submitted application application_1457060327319_0045\n",
      "HADOOP: The url to track the job: http://alejandro-j-rojas:8088/proxy/application_1457060327319_0045/\n",
      "HADOOP: Running job: job_1457060327319_0045\n",
      "HADOOP: Job job_1457060327319_0045 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 50% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1457060327319_0045 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=3452\n",
      "HADOOP: \t\tFILE: Number of bytes written=336731\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=11818062\n",
      "HADOOP: \t\tHDFS: Number of bytes written=3420\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=15295\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=2538\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=15295\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=2538\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=15295\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=2538\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=15662080\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=2598912\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=863952\n",
      "HADOOP: \t\tMap output records=2\n",
      "HADOOP: \t\tMap output bytes=3438\n",
      "HADOOP: \t\tMap output materialized bytes=3458\n",
      "HADOOP: \t\tInput split bytes=362\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=1\n",
      "HADOOP: \t\tReduce shuffle bytes=3458\n",
      "HADOOP: \t\tReduce input records=2\n",
      "HADOOP: \t\tReduce output records=2\n",
      "HADOOP: \t\tSpilled Records=4\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=223\n",
      "HADOOP: \t\tCPU time spent (ms)=3540\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=730349568\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=5796167680\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=545259520\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=11817700\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=3420\n",
      "HADOOP: Output directory: hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_text.vagrant.20160410.031026.176339/output\n",
      "Counters from step 3:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_text.vagrant.20160410.031026.176339/output\n",
      "[('the', 1580125), ('gt', 1209990), ('lt', 1204065), ('of', 947501), ('in', 647365), ('and', 619775), ('a', 573445), ('to', 445509), ('text', 415125), ('is', 405172), ('ref', 370025), ('category', 325598), ('s', 290253), ('amp', 242240), ('1', 234970), ('http', 216905), ('it', 215621), ('0', 214061), ('was', 212996), ('for', 209633), ('2', 197639), ('space', 196908), ('xml', 189058), ('preserve', 188763), ('on', 186280), ('name', 177141), ('8211', 166401), ('br', 166156), ('that', 158634), ('as', 156866), ('www', 156145), ('font', 153481), ('align', 148625), ('by', 148339), ('user', 146747), ('from', 143781), ('style', 136146), ('are', 133868), ('with', 131565), ('he', 126088), ('com', 122505), ('this', 122220), ('color', 120393), ('3', 118582), ('i', 117940), ('title', 116026), ('talk', 112782), ('at', 111698), ('center', 111281), ('be', 108703), ('or', 105956), ('small', 102606), ('an', 100372), ('not', 93363), ('new', 90612), ('people', 89867), ('nbsp', 89044), ('4', 88886), ('football', 88841), ('b', 84799), ('united', 83022), ('cite', 83011), ('first', 83006), ('date', 81969), ('5', 80076), ('other', 80060), ('url', 79569), ('they', 76638), ('if', 76576), ('his', 75649), ('span', 74704), ('american', 70481), ('one', 70421), ('states', 69952), ('have', 68696), ('stub', 67914), ('utc', 67506), ('jpg', 67415), ('c', 66282), ('has', 65234), ('accessdate', 65074), ('right', 64045), ('6', 63688), ('10', 62931), ('also', 62716), ('web', 62273), ('2009', 61422), ('233', 61025), ('2008', 60715), ('bgcolor', 60569), ('publisher', 60197), ('player', 60089), ('image', 59891), ('can', 58981), ('d', 57408), ('n', 57218), ('2010', 56991), ('which', 56053), ('sup', 55723), ('but', 55200)]\t\n",
      "null\t[[\"the\", 1580125], [\"gt\", 1209990], [\"lt\", 1204065], [\"of\", 947501], [\"in\", 647365], [\"and\", 619775], [\"a\", 573445], [\"to\", 445509], [\"text\", 415125], [\"is\", 405172], [\"ref\", 370025], [\"category\", 325598], [\"s\", 290253], [\"amp\", 242240], [\"1\", 234970], [\"http\", 216905], [\"it\", 215621], [\"0\", 214061], [\"was\", 212996], [\"for\", 209633], [\"2\", 197639], [\"space\", 196908], [\"xml\", 189058], [\"preserve\", 188763], [\"on\", 186280], [\"name\", 177141], [\"8211\", 166401], [\"br\", 166156], [\"that\", 158634], [\"as\", 156866], [\"www\", 156145], [\"font\", 153481], [\"align\", 148625], [\"by\", 148339], [\"user\", 146747], [\"from\", 143781], [\"style\", 136146], [\"are\", 133868], [\"with\", 131565], [\"he\", 126088], [\"com\", 122505], [\"this\", 122220], [\"color\", 120393], [\"3\", 118582], [\"i\", 117940], [\"title\", 116026], [\"talk\", 112782], [\"at\", 111698], [\"center\", 111281], [\"be\", 108703], [\"or\", 105956], [\"small\", 102606], [\"an\", 100372], [\"not\", 93363], [\"new\", 90612], [\"people\", 89867], [\"nbsp\", 89044], [\"4\", 88886], [\"football\", 88841], [\"b\", 84799], [\"united\", 83022], [\"cite\", 83011], [\"first\", 83006], [\"date\", 81969], [\"5\", 80076], [\"other\", 80060], [\"url\", 79569], [\"they\", 76638], [\"if\", 76576], [\"his\", 75649], [\"span\", 74704], [\"american\", 70481], [\"one\", 70421], [\"states\", 69952], [\"have\", 68696], [\"stub\", 67914], [\"utc\", 67506], [\"jpg\", 67415], [\"c\", 66282], [\"has\", 65234], [\"accessdate\", 65074], [\"right\", 64045], [\"6\", 63688], [\"10\", 62931], [\"also\", 62716], [\"web\", 62273], [\"2009\", 61422], [\"233\", 61025], [\"2008\", 60715], [\"bgcolor\", 60569], [\"publisher\", 60197], [\"player\", 60089], [\"image\", 59891], [\"can\", 58981], [\"d\", 57408], [\"n\", 57218], [\"2010\", 56991], [\"which\", 56053], [\"sup\", 55723], [\"but\", 55200]]\n",
      "removing tmp directory /tmp/top100_words_simple_text.vagrant.20160410.031026.176339\n",
      "deleting hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_text.vagrant.20160410.031026.176339 from HDFS\n"
     ]
    }
   ],
   "source": [
    "! python top100_words_simple_text.py -r hadoop s3n://thedataincubator-course/mrdata/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top100_words_simple_no_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top100_words_simple_no_metadata.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top100_words_simple_no_metadata.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "import re\n",
    "import lxml\n",
    "from lxml import etree\n",
    "import mwparserfromhell\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"\\w+\")\n",
    "\n",
    "\n",
    "class MRSimpleNoMeta(MRJob):\n",
    "    \n",
    "    #### Step 1 : Consolidate pages ####\n",
    "    def mapper_page_init(self):\n",
    "        self.page = \"\"\n",
    "        self.counter = 0 \n",
    "    def mapper_page (self,_,line):\n",
    "        if '<page>' in line : \n",
    "            self.page = line\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            self.page = self.page +\" \"+ line\n",
    "        \n",
    "        if '</page>' in line :           \n",
    "            yield (self.counter,self.page)\n",
    "    def reducer_page(self, _, pchunk):\n",
    "        for chunk in pchunk:\n",
    "            self.increment_counter(\"Word Counter\", \"Pages Reviewed\", amount=1)\n",
    "            texts = etree.fromstring(chunk)\n",
    "            ##if '<revision>'in texts:\n",
    "            texts = texts.findall(\".//revision\")[-1]\n",
    "            texts = texts.findall(\".//text\")\n",
    "            text_string = etree.tostring(texts[0])\n",
    "            wikicode = mwparserfromhell.parse(text_string)\n",
    "            text = \" \".join(\" \".join(fragment.value.split())\n",
    "                      for fragment in wikicode.filter_text())\n",
    "            text = text.encode('utf-8') \n",
    "            text = re.sub(r'http', ' ', text, flags=re.IGNORECASE)\n",
    "            text = re.sub(r'www', ' ', text, flags=re.IGNORECASE)\n",
    "            yield (None,text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### Step 2 : Count words   #########\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            self.increment_counter(\"Word Counter\", \"Total words\", amount=1)\n",
    "            yield (word.lower(), 1)\n",
    "    # input is: key, value\n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter(\"Word Counter\", \"Unique words\", amount=1)\n",
    "        yield (word, sum(counts))\n",
    " \n",
    "    #### Step 3 : Yield top 100 words ####\n",
    "    def mapper2_init(self):\n",
    "        self.heap = []\n",
    "        self.nlargest=[]\n",
    "    def mapper2(self,word,counts):\n",
    "        #for pair in pairs :\n",
    "        heapq.heappush(self.heap, (word,counts))\n",
    "#        self.n_largest = heapq.nlargest(100,self.heap,key=lambda e:e[1])\n",
    "    def mapper2_final(self):\n",
    "        n_largest = heapq.nlargest(100,self.heap,key=lambda e:e[1])\n",
    "        yield (None,n_largest)\n",
    "#        yield (None,self.n_largest)\n",
    "    \n",
    "    def reducer2_init(self):\n",
    "        self.new_heap=[]\n",
    "        self.final=[]\n",
    "    \n",
    "    def reducer2(self,_,heap):\n",
    "        count=0\n",
    "        for h in heap:\n",
    "            self.new_heap=heapq.chain(self.new_heap,h)\n",
    "        self.final=heapq.nlargest(100,self.new_heap,key=lambda e:e[1])\n",
    "    def reducer2_final(self):\n",
    "        print [(i[0], i[1]) for i in self.final]\n",
    "        yield (None,self.final)\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_page_init,\n",
    "                   mapper=self.mapper_page,\n",
    "                   reducer=self.reducer_page\n",
    "                  ),\n",
    "            MRStep(mapper=self.mapper,\n",
    "            #       combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(mapper_init=self.mapper2_init, mapper=self.mapper2\n",
    "                 ,mapper_final=self.mapper2_final\n",
    "            ,reducer_init=self.reducer2_init\n",
    "                 , reducer=self.reducer2,reducer_final=self.reducer2_final)\n",
    "            ]\n",
    "if __name__ == '__main__':\n",
    "    MRSimpleNoMeta.run()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 23), ('b', 12), ('marathon', 11), ('it', 10), ('of', 10), ('new', 9), ('category', 8), ('in', 8), ('york', 8), ('a', 7), ('ref', 6), ('text', 6), ('was', 6), ('city', 5), ('ing', 5), ('is', 5), ('to', 5), ('and', 4), ('bism', 4), ('by', 4), ('november', 4), ('2012', 3), ('bridge', 3), ('cancelled', 3), ('com', 3), ('for', 3), ('on', 3), ('preserve', 3), ('religion', 3), ('space', 3), ('sports', 3), ('title', 3), ('verrazano', 3), ('xml', 3), ('2010', 2), ('2013', 2), ('at', 2), ('because', 2), ('been', 2), ('cite', 2), ('has', 2), ('he', 2), ('htm', 2), ('interpretation', 2), ('islam', 2), ('narrows', 2), ('nyc', 2), ('nycmarathon', 2), ('official', 2), ('org', 2), ('other', 2), ('people', 2), ('persia', 2), ('publisher', 2), ('race', 2), ('religions', 2), ('runners', 2), ('stub', 2), ('suffolk', 2), ('thumb', 2), ('url', 2), ('used', 2), ('web', 2), ('11', 1), ('1844', 1), ('1970', 1), ('19th', 1), ('2', 1), ('219', 1), ('255px', 1), ('26', 1), ('304', 1), ('50', 1), ('58658', 1), ('7', 1), ('about', 1), ('abrahamic', 1), ('ali', 1), ('all', 1), ('an', 1), ('as', 1), ('ash', 1), ('atlantic', 1), ('azali', 1), ('babism', 1), ('bah', 1), ('based', 1), ('be', 1), ('borough', 1), ('break', 1), ('but', 1), ('called', 1), ('century', 1), ('circle', 1), ('claimed', 1), ('columbus', 1), ('commons', 1), ('company', 1), ('continued', 1), ('creation', 1)]\n",
      "[{'Word Counter': {'Pages Reviewed': 3}}, {'Word Counter': {'Total words': 414, 'Unique words': 220}}, {}]\n",
      "--- 0.00218043327332 min ---\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 4\n",
    "from top100_words_simple_no_metadata import MRSimpleNoMeta\n",
    "filename = 'sample.xml'\n",
    "##filename = 'part-00026.xml'\n",
    "##filename = 'simple'\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "mr_job = MRSimpleNoMeta(args=[filename])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    print runner.counters()\n",
    "    ##for line in runner.stream_output():\n",
    "        ##print line\n",
    "\n",
    "print(\"--- %s min ---\" % (((time.time() - start_time))/60)  )  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top100_words_simple_no_metadata.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top100_words_simple_no_metadata.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "import re\n",
    "import lxml\n",
    "from lxml import etree\n",
    "import sys\n",
    "##sys.path.append('/home/vagrant/mwparserfromhell')\n",
    "##hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_no_metadata.vagrant.20160409.193526.490917/files/\n",
    "sys.path.append('/opt/conda/lib/python2.7/site-packages/mwparserfromhell-0.5.dev0-py2.7-linux-x86_64.egg')\n",
    "import mwparserfromhell\n",
    "\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"\\w+\")\n",
    "\n",
    "\n",
    "class MRSimpleTextNoMeta(MRJob):\n",
    "    \n",
    "    def mapper_page_init(self):\n",
    "        self.page = \"\"\n",
    "        self.counter = 0\n",
    "    \n",
    "    def mapper_page (self,_,line):\n",
    "        if '<page>' in line : \n",
    "            self.page = line\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            self.page = self.page +\" \"+ line\n",
    "        \n",
    "        if '</page>' in line :           \n",
    "            yield (self.counter,self.page)\n",
    "        \n",
    "\n",
    "\n",
    "    def reducer_page(self, _, pchunk):\n",
    "        for chunk in pchunk:\n",
    "            self.increment_counter(\"Word Counter\", \"Pages Reviewed\", amount=1)\n",
    "            texts = etree.fromstring(chunk)\n",
    "            ##if '<revision>'in texts:\n",
    "            texts = texts.findall(\".//revision\")[-1]\n",
    "            texts = texts.findall(\".//text\")\n",
    "            texts = etree.tostring(texts[0])\n",
    "            wikicode = mwparserfromhell.parse(texts)\n",
    "            texts = \" \".join(\" \".join(fragment.value.split())\n",
    "                      for fragment in wikicode.filter_text())\n",
    "            texts = str(texts).decode(\"ISO-8859-1\").encode(\"utf-8\")\n",
    "            \n",
    "            ##wikicode = mwparserfromhell.parse(texts)\n",
    "            ##texts = wikicode.filter_text()\n",
    "            ##for text in texts:   \n",
    "              ##  if '://' not in str(text):\n",
    "                ##    print text\n",
    "                 ##   yield (None,str(text))\n",
    "            yield(None,texts)\n",
    "\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            ##if ' ' in word:\n",
    "                ##word=' '.join(word.split())\n",
    "            self.increment_counter(\"Word Counter\", \"Total Words\", amount=1)\n",
    "            yield (word.lower(), 1)\n",
    "    # input is: key, value\n",
    "    def reducer(self, word, counts):\n",
    "        self.increment_counter(\"Word Counter\", \"Unique Words\", amount=1)\n",
    "        yield (word, sum(counts))\n",
    "\n",
    " \n",
    "    def mapper2_init(self):\n",
    "        self.heap = []\n",
    "        self.nlargest=[]\n",
    "    def mapper2(self,word,counts):\n",
    "        #for pair in pairs :\n",
    "        heapq.heappush(self.heap, (word,counts))\n",
    "#        self.n_largest = heapq.nlargest(100,self.heap,key=lambda e:e[1])\n",
    "    def mapper2_final(self):\n",
    "        n_largest = heapq.nlargest(100,self.heap,key=lambda e:e[1])\n",
    "        yield (None,n_largest)\n",
    "#        yield (None,self.n_largest)\n",
    "    \n",
    "    def reducer2_init(self):\n",
    "        self.new_heap=[]\n",
    "        self.final=[]\n",
    "    \n",
    "    def reducer2(self,_,heap):\n",
    "        count=0\n",
    "        for h in heap:\n",
    "            self.new_heap=heapq.chain(self.new_heap,h)\n",
    "        self.final=heapq.nlargest(100,self.new_heap,key=lambda e:e[1])\n",
    "    def reducer2_final(self):\n",
    "        print [(i[0], i[1]) for i in self.final]\n",
    "        yield (None,self.final)\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_page_init,\n",
    "                   mapper=self.mapper_page,\n",
    "                   reducer=self.reducer_page\n",
    "                  ),\n",
    "            MRStep(mapper=self.mapper,\n",
    "            #       combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(mapper_init=self.mapper2_init, mapper=self.mapper2\n",
    "                 ,mapper_final=self.mapper2_final\n",
    "            ,reducer_init=self.reducer2_init\n",
    "                 , reducer=self.reducer2,reducer_final=self.reducer2_final)\n",
    "            ]\n",
    "if __name__ == '__main__':\n",
    "    MRSimpleTextNoMeta.run()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /home/vagrant/.mrjob.conf\n",
      "creating tmp directory /tmp/top100_words_simple_no_metadata.vagrant.20160410.193553.908170\n",
      "writing wrapper script to /tmp/top100_words_simple_no_metadata.vagrant.20160410.193553.908170/setup-wrapper.sh\n",
      "Using Hadoop version 2.5.0\n",
      "Copying local files into hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_no_metadata.vagrant.20160410.193553.908170/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar] /tmp/streamjob8577741307769624639.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Permissions on staging directory /tmp/hadoop-yarn/staging/vagrant/.staging are incorrect: rwxrwxrwt. Fixing permissions to correct value rwx------\n",
      "HADOOP: Total input paths to process : 27\n",
      "HADOOP: number of splits:27\n",
      "HADOOP: Submitting tokens for job: job_1457060327319_0056\n",
      "HADOOP: Submitted application application_1457060327319_0056\n",
      "HADOOP: The url to track the job: http://alejandro-j-rojas:8088/proxy/application_1457060327319_0056/\n",
      "HADOOP: Running job: job_1457060327319_0056\n",
      "HADOOP: Job job_1457060327319_0056 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000000_0, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000001_0, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000002_0, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP:  map 4% reduce 0%\n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000003_0, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000004_0, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000005_0, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000000_1, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000001_1, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000002_1, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000003_1, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000004_1, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000005_1, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000006_0, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000000_2, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000001_2, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000002_2, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000003_2, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000004_2, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000005_2, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0056_m_000006_1, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1457060327319_0056 failed with state FAILED due to: Task failed task_1457060327319_0056_m_000000\n",
      "HADOOP: Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "HADOOP: \n",
      "HADOOP: Counters: 13\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tFailed map tasks=21\n",
      "HADOOP: \t\tKilled map tasks=4\n",
      "HADOOP: \t\tLaunched map tasks=25\n",
      "HADOOP: \t\tOther local map tasks=18\n",
      "HADOOP: \t\tRack-local map tasks=7\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=110701\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=110701\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=110701\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=113357824\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tCPU time spent (ms)=0\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=0\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=0\n",
      "HADOOP: Job not successful!\n",
      "HADOOP: Streaming Command Failed!\n",
      "Job failed with return code 256: ['/usr/bin/hadoop', 'jar', '/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar', '-files', 'hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_no_metadata.vagrant.20160410.193553.908170/files/setup-wrapper.sh#setup-wrapper.sh,hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_no_metadata.vagrant.20160410.193553.908170/files/top100_words_simple_no_metadata.py#top100_words_simple_no_metadata.py', '-archives', 'hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_no_metadata.vagrant.20160410.193553.908170/files/mrjob.tar.gz#mrjob.tar.gz', '-input', 's3n://thedataincubator-course/mrdata/simple/', '-output', 'hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_no_metadata.vagrant.20160410.193553.908170/step-output/1', '-mapper', 'sh -ex setup-wrapper.sh python top100_words_simple_no_metadata.py --step-num=0 --mapper', '-reducer', 'sh -ex setup-wrapper.sh python top100_words_simple_no_metadata.py --step-num=0 --reducer']\n",
      "Scanning logs for probable cause of failure\n",
      "Traceback (most recent call last):\n",
      "  File \"top100_words_simple_no_metadata.py\", line 113, in <module>\n",
      "    MRSimpleTextNoMeta.run()         \n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/job.py\", line 461, in run\n",
      "    mr_job.execute()\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/job.py\", line 479, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/launch.py\", line 153, in execute\n",
      "    self.run_job()\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/launch.py\", line 216, in run_job\n",
      "    runner.run()\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/runner.py\", line 470, in run\n",
      "    self._run()\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/hadoop.py\", line 237, in _run\n",
      "    self._run_job_in_hadoop()\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/hadoop.py\", line 372, in _run_job_in_hadoop\n",
      "    raise CalledProcessError(returncode, step_args)\n",
      "subprocess.CalledProcessError: Command '['/usr/bin/hadoop', 'jar', '/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar', '-files', 'hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_no_metadata.vagrant.20160410.193553.908170/files/setup-wrapper.sh#setup-wrapper.sh,hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_no_metadata.vagrant.20160410.193553.908170/files/top100_words_simple_no_metadata.py#top100_words_simple_no_metadata.py', '-archives', 'hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_no_metadata.vagrant.20160410.193553.908170/files/mrjob.tar.gz#mrjob.tar.gz', '-input', 's3n://thedataincubator-course/mrdata/simple/', '-output', 'hdfs:///user/vagrant/tmp/mrjob/top100_words_simple_no_metadata.vagrant.20160410.193553.908170/step-output/1', '-mapper', 'sh -ex setup-wrapper.sh python top100_words_simple_no_metadata.py --step-num=0 --mapper', '-reducer', 'sh -ex setup-wrapper.sh python top100_words_simple_no_metadata.py --step-num=0 --reducer']' returned non-zero exit status 256\n"
     ]
    }
   ],
   "source": [
    "! python top100_words_simple_no_metadata.py -r hadoop s3n://thedataincubator-course/mrdata/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wikipedia_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Entropy_(information_theory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data you need is available at:\n",
    "    - https://s3.amazonaws.com/thedataincubator-course/mrdata/simple/part-000\\*\n",
    "    - https://s3.amazonaws.com/thedataincubator-course/mrdata/thai/part-000\\*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.johndcook.com/blog/2013/08/17/calculating-entropy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wikipedia_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wikipedia_entropy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wikipedia_entropy.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "import re\n",
    "import lxml\n",
    "from lxml import etree\n",
    "import mwparserfromhell\n",
    "import unicodedata\n",
    "import math\n",
    "\n",
    "WORD_RE = re.compile(r\"\\w+\")\n",
    "           \n",
    "           \n",
    "           \n",
    "class MREntropyCalc(MRJob):\n",
    "    \n",
    "    #### Step 1 : Consolidate pages ####\n",
    "    def mapper_page_init(self):\n",
    "        self.page = \"\"\n",
    "        self.counter = 0 \n",
    "    def mapper_page (self,_,line):\n",
    "        if '<page>' in line : \n",
    "            self.page = line\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            self.page = self.page +\" \"+ line\n",
    "        \n",
    "        if '</page>' in line :           \n",
    "            ##if self.counter > 0 and self.counter < 100:\n",
    "            yield (self.counter,self.page)     \n",
    "            \n",
    "            \n",
    "    def reducer_page(self, _, pchunk):\n",
    "        for chunk in pchunk:\n",
    "            self.increment_counter(\"Word Counter\", \"Pages Reviewed\", amount=1)\n",
    "            texts = etree.fromstring(chunk)\n",
    "            ##if '<revision>'in texts:\n",
    "            texts = texts.findall(\".//revision\")[-1]\n",
    "            texts = texts.findall(\".//text\")\n",
    "            text_string = etree.tostring(texts[0])\n",
    "            wikicode = mwparserfromhell.parse(text_string)\n",
    "            text = \" \".join(\" \".join(fragment.value.split())\n",
    "                      for fragment in wikicode.filter_text())\n",
    "            text = text.encode('utf-8') \n",
    "            text = re.sub(r'http', ' ', text, flags=re.IGNORECASE)\n",
    "            text = re.sub(r'www', ' ', text, flags=re.IGNORECASE)\n",
    "            yield (None,text)\n",
    "    \n",
    "    \n",
    "\n",
    "    ##### Step 2 : Count characters   #########\n",
    "    def mapper_init_charcount(self):\n",
    "        self.unigram = None\n",
    "        self.twogram = None\n",
    "        self.threegram = None\n",
    "        \n",
    "      \n",
    "    def mapper_charcount(self, _, line):\n",
    "        for char in line:\n",
    "            if self.twogram:\n",
    "                self.threegram = self.twogram + char.encode('utf-8')\n",
    "            if self.unigram:\n",
    "                self.twogram = self.unigram +  char.encode('utf-8')\n",
    "            self.unigram = char.encode('utf-8')      \n",
    "            if self.unigram:\n",
    "                self.increment_counter(\"Char Counter\", \"Total unigram\", amount=1)\n",
    "                self.increment_counter(\"Char Counter\", \"Total\", amount=1)\n",
    "                yield (self.unigram,1)\n",
    "            if self.twogram:\n",
    "                self.increment_counter(\"Char Counter\", \"Total twogram\", amount=1)\n",
    "                self.increment_counter(\"Char Counter\", \"Total\", amount=1)\n",
    "                yield (self.twogram,1)\n",
    "\n",
    "            if self.threegram:\n",
    "                self.increment_counter(\"Char Counter\", \"Total threegram\", amount=1)\n",
    "                self.increment_counter(\"Char Counter\", \"Total\", amount=1)\n",
    "                yield (self.threegram,1)\n",
    "    # input is: key, value\n",
    "    def reducer_init_charcount(self):\n",
    "        self.total_uni_N = 0\n",
    "        self.total_uni_ngram_sum = 0\n",
    "        self.total_two_N = 0\n",
    "        self.total_two_ngram_sum = 0\n",
    "        self.total_three_N = 0\n",
    "        self.total_three_ngram_sum = 0  \n",
    "\n",
    "           \n",
    "    \n",
    "    def reducer_charcount(self, char,counts):\n",
    "        self.increment_counter(\"Char Counter\", \"Unique characters\", amount=1)\n",
    "        self.n = sum(counts)\n",
    "        self.ngram_sum = self.n*math.log(self.n,2)\n",
    "        \n",
    "        if len(char) == 1:\n",
    "            self.increment_counter(\"Char Counter\", \"Unique unigrams\", amount=1)\n",
    "            ##yield(None,(char,self.n,self.ngram_sum))\n",
    "            self.total_uni_N += self.n\n",
    "            self.total_uni_ngram_sum += self.ngram_sum\n",
    "            self.uni_entropy= math.log(self.total_uni_N,2)- self.total_uni_ngram_sum/self.total_uni_N\n",
    "        elif len(char) == 2:\n",
    "            self.increment_counter(\"Char Counter\", \"Unique twograms\", amount=1)\n",
    "            self.total_two_N += self.n\n",
    "            self.total_two_ngram_sum += self.ngram_sum\n",
    "            self.two_entropy= (math.log(self.total_two_N,2)- self.total_two_ngram_sum/self.total_two_N)/2\n",
    "        else:\n",
    "            self.increment_counter(\"Char Counter\", \"Unique threegrams\", amount=1)\n",
    "            self.total_three_N += self.n\n",
    "            self.total_three_ngram_sum += self.ngram_sum\n",
    "            self.three_entropy= (math.log(self.total_three_N,2)- self.total_three_ngram_sum/self.total_three_N)/3\n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "    def reducer_charcount_final(self):\n",
    "        \n",
    "        print self.uni_entropy\n",
    "        print self.two_entropy\n",
    "        print self.three_entropy\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_page_init,\n",
    "                   mapper=self.mapper_page,\n",
    "                   reducer=self.reducer_page\n",
    "                  ),\n",
    "            MRStep(mapper_init=self.mapper_init_charcount,\n",
    "                   mapper=self.mapper_charcount,\n",
    "            #       combiner=self.combiner,\n",
    "                   reducer_init=self.reducer_init_charcount,\n",
    "                   reducer=self.reducer_charcount,\n",
    "                   reducer_final=self.reducer_charcount_final)\n",
    "            ]\n",
    "if __name__ == '__main__':\n",
    "    MREntropyCalc.run()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.08946972558\n",
      "4.55384200601\n",
      "4.05304806367\n",
      "[{'Word Counter': {'Pages Reviewed': 12402}}, {'Char Counter': {'Total twogram': 6044723, 'Unique unigrams': 92, 'Unique characters': 84544, 'Unique threegrams': 77798, 'Total unigram': 6044724, 'Total': 18134169, 'Total threegram': 6044722, 'Unique twograms': 6654}}]\n",
      "--- 11.632469217 min ---\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 4\n",
    "from wikipedia_entropy import MREntropyCalc\n",
    "##filename = 'sample.xml'\n",
    "##filename = 'part-00026.xml'\n",
    "##filename = 'simple'\n",
    "filename = 'thaipart-00027.xml'\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "mr_job = MREntropyCalc(args=[filename])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    print runner.counters()\n",
    "    ##for line in runner.stream_output():\n",
    "        ##print line\n",
    "\n",
    "print(\"--- %s min ---\" % (((time.time() - start_time))/60)  )  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /home/vagrant/.mrjob.conf\n",
      "creating tmp directory /tmp/wikipedia_entropy.vagrant.20160411.233644.093260\n",
      "writing wrapper script to /tmp/wikipedia_entropy.vagrant.20160411.233644.093260/setup-wrapper.sh\n",
      "Using Hadoop version 2.5.0\n",
      "Copying local files into hdfs:///user/vagrant/tmp/mrjob/wikipedia_entropy.vagrant.20160411.233644.093260/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar] /tmp/streamjob9021901117161648229.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Permissions on staging directory /tmp/hadoop-yarn/staging/vagrant/.staging are incorrect: rwxrwxrwt. Fixing permissions to correct value rwx------\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1457060327319_0061\n",
      "HADOOP: Submitted application application_1457060327319_0061\n",
      "HADOOP: The url to track the job: http://alejandro-j-rojas:8088/proxy/application_1457060327319_0061/\n",
      "HADOOP: Running job: job_1457060327319_0061\n",
      "HADOOP: Job job_1457060327319_0061 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP: Task Id : attempt_1457060327319_0061_m_000000_0, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Container killed by the ApplicationMaster.\n",
      "HADOOP: Container killed on request. Exit code is 143\n",
      "HADOOP: Container exited with a non-zero exit code 143\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0061_m_000001_0, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0061_m_000000_1, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0061_m_000001_1, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0061_m_000000_2, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1457060327319_0061_m_000001_2, Status : FAILED\n",
      "HADOOP: Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "HADOOP: \tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n",
      "HADOOP: \tat java.security.AccessController.doPrivileged(Native Method)\n",
      "HADOOP: \tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "HADOOP: \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n",
      "HADOOP: \tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n",
      "HADOOP: \n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1457060327319_0061 failed with state FAILED due to: Task failed task_1457060327319_0061_m_000000\n",
      "HADOOP: Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "HADOOP: \n",
      "HADOOP: Counters: 13\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tFailed map tasks=7\n",
      "HADOOP: \t\tKilled map tasks=1\n",
      "HADOOP: \t\tLaunched map tasks=8\n",
      "HADOOP: \t\tOther local map tasks=6\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=21268\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=21268\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=21268\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=21778432\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tCPU time spent (ms)=0\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=0\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=0\n",
      "HADOOP: Job not successful!\n",
      "HADOOP: Streaming Command Failed!\n",
      "Job failed with return code 256: ['/usr/bin/hadoop', 'jar', '/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar', '-files', 'hdfs:///user/vagrant/tmp/mrjob/wikipedia_entropy.vagrant.20160411.233644.093260/files/wikipedia_entropy.py#wikipedia_entropy.py,hdfs:///user/vagrant/tmp/mrjob/wikipedia_entropy.vagrant.20160411.233644.093260/files/setup-wrapper.sh#setup-wrapper.sh', '-archives', 'hdfs:///user/vagrant/tmp/mrjob/wikipedia_entropy.vagrant.20160411.233644.093260/files/mrjob.tar.gz#mrjob.tar.gz', '-input', 'hdfs:///user/vagrant/tmp/mrjob/wikipedia_entropy.vagrant.20160411.233644.093260/files/sample.xml', '-output', 'hdfs:///user/vagrant/tmp/mrjob/wikipedia_entropy.vagrant.20160411.233644.093260/step-output/1', '-mapper', 'sh -ex setup-wrapper.sh python wikipedia_entropy.py --step-num=0 --mapper', '-reducer', 'sh -ex setup-wrapper.sh python wikipedia_entropy.py --step-num=0 --reducer']\n",
      "Scanning logs for probable cause of failure\n",
      "Traceback (most recent call last):\n",
      "  File \"wikipedia_entropy.py\", line 126, in <module>\n",
      "    MREntropyCalc.run()         \n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/job.py\", line 461, in run\n",
      "    mr_job.execute()\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/job.py\", line 479, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/launch.py\", line 153, in execute\n",
      "    self.run_job()\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/launch.py\", line 216, in run_job\n",
      "    runner.run()\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/runner.py\", line 470, in run\n",
      "    self._run()\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/hadoop.py\", line 237, in _run\n",
      "    self._run_job_in_hadoop()\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/mrjob/hadoop.py\", line 372, in _run_job_in_hadoop\n",
      "    raise CalledProcessError(returncode, step_args)\n",
      "subprocess.CalledProcessError: Command '['/usr/bin/hadoop', 'jar', '/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar', '-files', 'hdfs:///user/vagrant/tmp/mrjob/wikipedia_entropy.vagrant.20160411.233644.093260/files/wikipedia_entropy.py#wikipedia_entropy.py,hdfs:///user/vagrant/tmp/mrjob/wikipedia_entropy.vagrant.20160411.233644.093260/files/setup-wrapper.sh#setup-wrapper.sh', '-archives', 'hdfs:///user/vagrant/tmp/mrjob/wikipedia_entropy.vagrant.20160411.233644.093260/files/mrjob.tar.gz#mrjob.tar.gz', '-input', 'hdfs:///user/vagrant/tmp/mrjob/wikipedia_entropy.vagrant.20160411.233644.093260/files/sample.xml', '-output', 'hdfs:///user/vagrant/tmp/mrjob/wikipedia_entropy.vagrant.20160411.233644.093260/step-output/1', '-mapper', 'sh -ex setup-wrapper.sh python wikipedia_entropy.py --step-num=0 --mapper', '-reducer', 'sh -ex setup-wrapper.sh python wikipedia_entropy.py --step-num=0 --reducer']' returned non-zero exit status 256\n"
     ]
    }
   ],
   "source": [
    "! python wikipedia_entropy.py -r hadoop sample.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Top NGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_ngrams_entropy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_ngrams_entropy.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "import re\n",
    "import lxml\n",
    "from lxml import etree\n",
    "import mwparserfromhell\n",
    "import unicodedata\n",
    "import math\n",
    "\n",
    "WORD_RE = re.compile(r\"\\w+\")\n",
    "\n",
    "\n",
    "class MRTopNGrams(MRJob):\n",
    "    \n",
    "    #### Step 1 : Consolidate pages ####\n",
    "    def mapper_page_init(self):\n",
    "        self.page = \"\"\n",
    "        self.counter = 0 \n",
    "    def mapper_page (self,_,line):\n",
    "        if '<page>' in line : \n",
    "            self.page = line\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            self.page = self.page +\" \"+ line\n",
    "        \n",
    "        if '</page>' in line :           \n",
    "            if self.counter > 0 and self.counter < 104:\n",
    "                yield (self.counter,self.page)     \n",
    "            \n",
    "            \n",
    "    def reducer_page(self, _, pchunk):\n",
    "        for chunk in pchunk:\n",
    "            self.increment_counter(\"Word Counter\", \"Pages Reviewed\", amount=1)\n",
    "            texts = etree.fromstring(chunk)\n",
    "            ##if '<revision>'in texts:\n",
    "            texts = texts.findall(\".//revision\")[-1]\n",
    "            texts = texts.findall(\".//text\")\n",
    "            text_string = etree.tostring(texts[0])\n",
    "            wikicode = mwparserfromhell.parse(text_string)\n",
    "            text = \" \".join(\" \".join(fragment.value.split())\n",
    "                      for fragment in wikicode.filter_text())\n",
    "            text = text.encode('utf-8') \n",
    "            text = re.sub(r'http', ' ', text, flags=re.IGNORECASE)\n",
    "            text = re.sub(r'www', ' ', text, flags=re.IGNORECASE)\n",
    "            yield (None,text)\n",
    "    \n",
    "    \n",
    "\n",
    "    ##### Step 2 : Count characters   #########\n",
    "    def mapper_init_charcount(self):\n",
    "        None\n",
    "      \n",
    "    def mapper_charcount(self, _, line):\n",
    "        char_count = len(line)\n",
    "        self.increment_counter(\"Word Counter\", \"Lines reviewed\", amount=1)\n",
    "   \n",
    "        yield(None, char_count)\n",
    "    # input is: key, value\n",
    "    \n",
    "    \n",
    "    def reducer_charcount(self, _,char_count):\n",
    "        n = sum(char_count)\n",
    "        print n\n",
    "        uni_nsample = 6044724\n",
    "        uni_ksample = 92\n",
    "        two_ksample = 6654\n",
    "        three_ksample = 77798\n",
    "        \n",
    "        def prob_calc(k,n):\n",
    "            k_sum = 0 \n",
    "            for i in range (1,k+1):\n",
    "                k_sum += 1.0/i             \n",
    "            return 1.0/k_sum\n",
    "        \n",
    "        def entropy_calc(ksample,nsample,char,nactual):\n",
    "            ngram_sum = 0\n",
    "            max_count = prob_calc(ksample,nsample)*nactual\n",
    "            for i in range(1,ksample+1):\n",
    "                ncount =  max_count*1.0/i  \n",
    "                ngram_sum += ncount*math.log(ncount,2)\n",
    "            entropy = math.log(nactual,2)- (ngram_sum/nactual)\n",
    "            return entropy/char\n",
    "        \n",
    "        one = entropy_calc(uni_ksample, uni_nsample, 1,n )\n",
    "        two = entropy_calc(two_ksample, uni_nsample-1, 2,n )\n",
    "        three = entropy_calc(three_ksample, uni_nsample-2, 3,n )\n",
    "        yield(None, (one,two,three))\n",
    "        print one, two, three\n",
    "\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_page_init,\n",
    "                   mapper=self.mapper_page,\n",
    "                   reducer=self.reducer_page\n",
    "                  ),\n",
    "            MRStep(mapper_init=self.mapper_init_charcount,\n",
    "                   mapper=self.mapper_charcount,\n",
    "            #       combiner=self.combiner,\n",
    "                   reducer=self.reducer_charcount)\n",
    "            ]\n",
    "if __name__ == '__main__':\n",
    "    MRTopNGrams.run()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mrjob.runner:\n",
      "WARNING:mrjob.runner:PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "WARNING:mrjob.runner:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2525795\n",
      "5.22757306473 4.58886758288 3.76142266511\n",
      "[{'Word Counter': {'Pages Reviewed': 3914}}, {'Word Counter': {'Lines reviewed': 3914}}]\n",
      "--- 2.99097809792 min ---\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 4\n",
    "from top_ngrams_entropy import MRTopNGrams\n",
    "##filename = 'sample.xml'\n",
    "##filename = 'part-00026.xml'\n",
    "##filename = 'simple'\n",
    "filename = 'wikithai'\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "mr_job = MRTopNGrams(args=[filename])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    print runner.counters()\n",
    "    ##for line in runner.stream_output():\n",
    "        ##print line\n",
    "\n",
    "print(\"--- %s min ---\" % (((time.time() - start_time))/60)  )  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.195908291588\n",
      "1683711.2826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.227573064725462"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "uni_nsample = 8594385\n",
    "uni_ksample = 92\n",
    "two_ksample = 6424\n",
    "three_ksample = 72879\n",
    "        \n",
    "def prob_calc(k,n):\n",
    "    k_sum = 0 \n",
    "    for i in range (1,k+1):\n",
    "        k_sum += 1.0/i \n",
    "    return 1.0/k_sum\n",
    "        \n",
    "print prob_calc(uni_ksample,uni_nsample)\n",
    "\n",
    "def entropy_calc(ksample,nsample,char,nactual):\n",
    "    ngram_sum = 0\n",
    "    max_count = prob_calc(ksample,nsample)*nactual\n",
    "    print max_count\n",
    "    for i in range(1,ksample+1):\n",
    "        ncount =  max_count*1.0/i  \n",
    "        ngram_sum += ncount*math.log(ncount,2)\n",
    "    entropy = math.log(nactual,2)- (ngram_sum/nactual)\n",
    "    return entropy/char\n",
    "entropy_calc(uni_ksample,uni_nsample,1,8594385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using configs in /home/vagrant/.mrjob.conf\n",
      "creating tmp directory /tmp/most_common_char_en.vagrant.20160410.003026.711509\n",
      "writing wrapper script to /tmp/most_common_char_en.vagrant.20160410.003026.711509/setup-wrapper.sh\n",
      "Using Hadoop version 2.5.0\n",
      "Copying local files into hdfs:///user/vagrant/tmp/mrjob/most_common_char_en.vagrant.20160410.003026.711509/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar] /tmp/streamjob4340401832827294728.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1457060327319_0030\n",
      "HADOOP: Submitted application application_1457060327319_0030\n",
      "HADOOP: The url to track the job: http://alejandro-j-rojas:8088/proxy/application_1457060327319_0030/\n",
      "HADOOP: Running job: job_1457060327319_0030\n",
      "HADOOP: Job job_1457060327319_0030 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 50% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1457060327319_0030 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=4511\n",
      "HADOOP: \t\tFILE: Number of bytes written=338646\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=6622\n",
      "HADOOP: \t\tHDFS: Number of bytes written=2909\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=5422\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=2662\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=5422\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=2662\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=5422\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=2662\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=5552128\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=2725888\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=79\n",
      "HADOOP: \t\tMap output records=3\n",
      "HADOOP: \t\tMap output bytes=4493\n",
      "HADOOP: \t\tMap output materialized bytes=4517\n",
      "HADOOP: \t\tInput split bytes=334\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=3\n",
      "HADOOP: \t\tReduce shuffle bytes=4517\n",
      "HADOOP: \t\tReduce input records=3\n",
      "HADOOP: \t\tReduce output records=3\n",
      "HADOOP: \t\tSpilled Records=6\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=178\n",
      "HADOOP: \t\tCPU time spent (ms)=2410\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=733876224\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=5802577920\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=551550976\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=6288\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=2909\n",
      "HADOOP: Output directory: hdfs:///user/vagrant/tmp/mrjob/most_common_char_en.vagrant.20160410.003026.711509/step-output/1\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar] /tmp/streamjob2487103175525879410.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1457060327319_0031\n",
      "HADOOP: Submitted application application_1457060327319_0031\n",
      "HADOOP: The url to track the job: http://alejandro-j-rojas:8088/proxy/application_1457060327319_0031/\n",
      "HADOOP: Running job: job_1457060327319_0031\n",
      "HADOOP: Job job_1457060327319_0031 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 50% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1457060327319_0031 completed successfully\n",
      "HADOOP: Counters: 50\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=16710\n",
      "HADOOP: \t\tFILE: Number of bytes written=363035\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=4714\n",
      "HADOOP: \t\tHDFS: Number of bytes written=379\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=5583\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=2569\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=5583\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=2569\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=5583\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=2569\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=5716992\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=2630656\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=3\n",
      "HADOOP: \t\tMap output records=2088\n",
      "HADOOP: \t\tMap output bytes=12528\n",
      "HADOOP: \t\tMap output materialized bytes=16716\n",
      "HADOOP: \t\tInput split bytes=350\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=56\n",
      "HADOOP: \t\tReduce shuffle bytes=16716\n",
      "HADOOP: \t\tReduce input records=2088\n",
      "HADOOP: \t\tReduce output records=56\n",
      "HADOOP: \t\tSpilled Records=4176\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=184\n",
      "HADOOP: \t\tCPU time spent (ms)=2770\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=735236096\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=5802676224\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=547356672\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tWord Counter\n",
      "HADOOP: \t\tUnique Characters=56\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=4364\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=379\n",
      "HADOOP: Output directory: hdfs:///user/vagrant/tmp/mrjob/most_common_char_en.vagrant.20160410.003026.711509/step-output/2\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "HADOOP: packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.5.jar] /tmp/streamjob3159042746824699413.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Connecting to ResourceManager at alejandro-j-rojas/127.0.1.1:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1457060327319_0032\n",
      "HADOOP: Submitted application application_1457060327319_0032\n",
      "HADOOP: The url to track the job: http://alejandro-j-rojas:8088/proxy/application_1457060327319_0032/\n",
      "HADOOP: Running job: job_1457060327319_0032\n",
      "HADOOP: Job job_1457060327319_0032 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1457060327319_0032 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=633\n",
      "HADOOP: \t\tFILE: Number of bytes written=330860\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=919\n",
      "HADOOP: \t\tHDFS: Number of bytes written=1214\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=5552\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=2718\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=5552\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=2718\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=5552\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=2718\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=5685248\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=2783232\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=56\n",
      "HADOOP: \t\tMap output records=2\n",
      "HADOOP: \t\tMap output bytes=619\n",
      "HADOOP: \t\tMap output materialized bytes=639\n",
      "HADOOP: \t\tInput split bytes=350\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=1\n",
      "HADOOP: \t\tReduce shuffle bytes=639\n",
      "HADOOP: \t\tReduce input records=2\n",
      "HADOOP: \t\tReduce output records=2\n",
      "HADOOP: \t\tSpilled Records=4\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=215\n",
      "HADOOP: \t\tCPU time spent (ms)=2550\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=741367808\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=5802004480\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=546308096\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=569\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=1214\n",
      "HADOOP: Output directory: hdfs:///user/vagrant/tmp/mrjob/most_common_char_en.vagrant.20160410.003026.711509/output\n",
      "Counters from step 3:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/vagrant/tmp/mrjob/most_common_char_en.vagrant.20160410.003026.711509/output\n",
      "[('e', 215), ('t', 183), ('a', 152), ('r', 141), ('o', 131), ('i', 126), ('n', 112), ('s', 104), ('h', 81), ('l', 76), ('m', 57), ('c', 52), ('g', 45), ('w', 44), ('b', 42), ('p', 41), ('d', 40), ('2', 40), ('f', 37), ('u', 37), ('y', 37), ('5', 20), ('k', 19), ('N', 19), ('0', 18), ('1', 18), ('C', 16), ('I', 16), ('v', 15), ('x', 14), ('B', 13), ('3', 10), ('M', 10), ('S', 10), ('Y', 10), ('7', 8), ('8', 8), ('R', 8), ('z', 7), ('T', 7), ('6', 5), ('G', 5), ('H', 5), ('4', 4), ('A', 4), ('F', 4), ('_', 3), ('9', 3), ('O', 3), ('P', 3), ('V', 3), ('j', 2), ('E', 2), ('D', 1), ('Q', 1), ('W', 1)]\t\n",
      "null\t[[\"e\", 215], [\"t\", 183], [\"a\", 152], [\"r\", 141], [\"o\", 131], [\"i\", 126], [\"n\", 112], [\"s\", 104], [\"h\", 81], [\"l\", 76], [\"m\", 57], [\"c\", 52], [\"g\", 45], [\"w\", 44], [\"b\", 42], [\"p\", 41], [\"d\", 40], [\"2\", 40], [\"f\", 37], [\"u\", 37], [\"y\", 37], [\"5\", 20], [\"k\", 19], [\"N\", 19], [\"0\", 18], [\"1\", 18], [\"C\", 16], [\"I\", 16], [\"v\", 15], [\"x\", 14], [\"B\", 13], [\"3\", 10], [\"M\", 10], [\"S\", 10], [\"Y\", 10], [\"7\", 8], [\"8\", 8], [\"R\", 8], [\"z\", 7], [\"T\", 7], [\"6\", 5], [\"G\", 5], [\"H\", 5], [\"4\", 4], [\"A\", 4], [\"F\", 4], [\"_\", 3], [\"9\", 3], [\"O\", 3], [\"P\", 3], [\"V\", 3], [\"j\", 2], [\"E\", 2], [\"D\", 1], [\"Q\", 1], [\"W\", 1]]\n",
      "removing tmp directory /tmp/most_common_char_en.vagrant.20160410.003026.711509\n",
      "deleting hdfs:///user/vagrant/tmp/mrjob/most_common_char_en.vagrant.20160410.003026.711509 from HDFS\n"
     ]
    }
   ],
   "source": [
    "! python most_common_char_en.py -r hadoop sample.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<page>\r\n",
      "    <title>New York City Marathon</title>\r\n",
      "    <ns>0</ns>\r\n",
      "    <id>423724</id>\r\n",
      "    <revision>\r\n",
      "      <id>4615420</id>\r\n",
      "      <parentid>4615295</parentid>\r\n",
      "      <timestamp>2013-11-10T08:18:23Z</timestamp>\r\n",
      "      <contributor>\r\n",
      "        <username>Osiris</username>\r\n",
      "        <id>209999</id>\r\n",
      "      </contributor>\r\n",
      "      <minor />\r\n",
      "      <comment>added [[Category:Sports events]] using [[Help:Gadget-HotCat|HotCat]]</comment>\r\n",
      "      <text xml:space=\"preserve\">[[Image:New York marathon Verrazano bridge.jpg|thumb|255px|Marathon runners on Verrazano-Narrows Bridge.]]\r\n",
      "The '''New York City Marathon''' (called the '''ING New York City Marathon''' because it is [[sponsor]]ed by [[financial]] company [[ING Group]]) is a yearly [[marathon]] that goes through all five [[borough]]s of [[New York City]]. The distance of the race is 26.219 miles. It starts at the [[Verrazano-Narrows Bridge]] and ends at [[Columbus Circle]]. It takes place on the first Sunday of November.&lt;ref&gt;http://gonyc.about.com/od/planyourtriptonyc/a/november.htm&lt;/ref&gt; It is one of the largest marathons in the world. 50,304 people ran the marathon in 2013.&lt;ref&gt;{{cite web | url = http://www.nycmarathon.org/Results.htm | title = ING New York City Marathon 2010, Finisher Demographics, November 7, 2010| publisher = New York Road Runners}}&lt;/ref&gt; It has been held every year since 1970 except for 2012, when the race was cancelled because of [[Hurricane Sandy]].&lt;ref&gt;{{cite web |publisher=Atlantic Wire |title=Reports: The New York Marathon Has Been Cancelled|url=http://www.theatlanticwire.com/national/2012/11/marathon-cancelled-fox-news-thinks-so/58658/|date=November 2, 2012}}&lt;/ref&gt;\r\n",
      "​\r\n",
      "==References==\r\n",
      "{{reflist}}\r\n",
      "​\r\n",
      "==Other websites==\r\n",
      "{{commons category}}\r\n",
      "*{{official website|http://www.nycmarathon.org/}}\r\n",
      "*[http://www.crowdrise.com/ing-nyc-marathon-2013 Official ING NYC Marathon Fundraising Site]\r\n",
      "*{{NYTtopic|subjects/n/new_york_city_marathon}}\r\n",
      "{{sports-stub}}\r\n",
      "[[Category:Running]]\r\n",
      "[[Category:Sports in New York City]]\r\n",
      "[[Category:Sports events]]</text>\r\n",
      "      <sha1>rpp1zws5m15n7gkfk4a59z8837ubid7</sha1>\r\n",
      "      <model>wikitext</model>\r\n",
      "      <format>text/x-wiki</format>\r\n",
      "    </revision>\r\n",
      "  </page>\r\n",
      "  <page>\r\n",
      "    <title>Category:Sportspeople from Suffolk</title>\r\n",
      "    <ns>14</ns>\r\n",
      "    <id>423726</id>\r\n",
      "    <revision>\r\n",
      "      <id>4615303</id>\r\n",
      "      <timestamp>2013-11-10T00:10:08Z</timestamp>\r\n",
      "      <contributor>\r\n",
      "        <username>Jim Michael</username>\r\n",
      "        <id>109566</id>\r\n",
      "      </contributor>\r\n",
      "      <comment>Created page with &quot;[[Category:English sportspeople by location of origin|Suffolk]] [[Category:People from Suffolk]]&quot;</comment>\r\n",
      "      <text xml:space=\"preserve\">[[Category:English sportspeople by location of origin|Suffolk]]\r\n",
      "[[Category:People from Suffolk]]</text>\r\n",
      "      <sha1>ruhx2g92ewbtex8mnavrc0fpdx2rsst</sha1>\r\n",
      "      <model>wikitext</model>\r\n",
      "      <format>text/x-wiki</format>\r\n",
      "    </revision>\r\n",
      "  </page>\r\n",
      "  <page>\r\n",
      "    <title>Bábism</title>\r\n",
      "    <ns>0</ns>\r\n",
      "    <id>423730</id>\r\n",
      "    <revision>\r\n",
      "      <id>4616137</id>\r\n",
      "      <parentid>4616107</parentid>\r\n",
      "      <timestamp>2013-11-11T02:41:46Z</timestamp>\r\n",
      "      <contributor>\r\n",
      "        <username>Islam90</username>\r\n",
      "        <id>330257</id>\r\n",
      "      </contributor>\r\n",
      "      <text xml:space=\"preserve\">[[File:Haykal2.gif|thumb|upright|The [[pentagram]] used as a symbol for Bábism. It was made by the Báb.]]\r\n",
      "'''Bábism''' is a [[religion]]. It was started in [[Persia]] in the middle of the 19th century. The founder was [[Báb|Mirza Ali Muhamed Reza ash-Shirazi]]. He claimed to be a kind of [[Mahdi|savior]] for [[Shia Islam|Shi'a Muslims]]. He took the title ''Báb'' ({{lang-fa|الباب}}), meaning &quot;gate&quot;. Bábism was originally based on his [[wikt:interpretation|interpretation]] of the [[Qur'an]] and other Islamic texts. There were similar movements like this, but the Bábí movement tried to break with Islam to start a new religion. It was opposed by the government in Persia, which used laws and violence to destroy it. Bábism continued in [[exile]] in the [[Ottoman Empire]]. It led to the creation of [[Azali Babism]] and the [[Bahá'í Faith]].\r\n",
      "​\r\n",
      "{{Religions}}\r\n",
      "​\r\n",
      "[[Category:Abrahamic religions]]\r\n",
      "[[Category:1844 establishments]]\r\n",
      "​\r\n",
      "{{religion-stub}}</text>\r\n",
      "      <sha1>qksy9krq3fdrbvwghkgy28z8006x6j4</sha1>\r\n",
      "      <model>wikitext</model>\r\n",
      "      <format>text/x-wiki</format>\r\n",
      "    </revision>\r\n",
      "  </page>\r\n"
     ]
    }
   ],
   "source": [
    "! cat sample.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "       wikicode = mwparserfromhell.parse(text)\n",
    "        meta_text = wikicode.filter_templates()\n",
    "        text =  meta_text.filter_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_test = '<comment>Created page with &quot;[[Category:English sportspeople by location of origin|Suffolk]] [[Category:People from Suffolk]]&quot;</comment><text xml:space=\"preserve\">[[Category:English sportspeople by location of origin|Suffolk]][[Category:People from Suffolk]]</text>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikicode = mwparserfromhell.parse(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'<comment>Created page with &quot;[[Category:English sportspeople by location of origin|Suffolk]] [[Category:People from Suffolk]]&quot;</comment><text xml:space=\"preserve\">[[Category:English sportspeople by location of origin|Suffolk]][[Category:People from Suffolk]]</text>'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_text = wikicode.filter_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'comment',\n",
       " u'Created page with ',\n",
       " u'Category:English sportspeople by location of origin',\n",
       " u'Suffolk',\n",
       " u' ',\n",
       " u'Category:People from Suffolk',\n",
       " u'comment',\n",
       " u'text',\n",
       " u'xml:space',\n",
       " u'preserve',\n",
       " u'Category:English sportspeople by location of origin',\n",
       " u'Suffolk',\n",
       " u'Category:People from Suffolk',\n",
       " u'text']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comment'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(meta_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_text = '{commons category}}*{{official website|http://www.nycmarathon.org/}}*[http://www.crowdrise.com/ing-nyc-marathon-2013 Official ING NYC Marathon Fundraising Site]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikicode = mwparserfromhell.parse(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_text = wikicode.filter_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'{commons category}}*',\n",
       " u'official website',\n",
       " u'http://www.nycmarathon.org/',\n",
       " u'*',\n",
       " u'http://www.crowdrise.com/ing-nyc-marathon-2013',\n",
       " u'Official ING NYC Marathon Fundraising Site']"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textst = \" \".join(\" \".join(fragment.value.split())\n",
    "                      for fragment in wikicode.filter_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'{commons category}}* official website http://www.nycmarathon.org/ * http://www.crowdrise.com/ing-nyc-marathon-2013 Official ING NYC Marathon Fundraising Site'"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textst = str(textst).decode(\"ISO-8859-1\").encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{commons category}}* official website http://www.nycmarathon.org/ * http://www.crowdrise.com/ing-nyc-marathon-2013 Official ING NYC Marathon Fundraising Site'"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textst = re.sub(r'http', ' ', textst, flags=re.IGNORECASE)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textst = re.sub(r'www', ' ', textst, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{commons category}}* official website  :// .nycmarathon.org/ *  :// .crowdrise.com/ing-nyc-marathon-2013 Official ING NYC Marathon Fundraising Site'"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikicode = mwparserfromhell.parse(text_string)\n",
    "        texts = [\" \".join(\" \".join(fragment.value.split())\n",
    "                      for fragment in wikicode.filter_text())]\n",
    "        for text in texts:\n",
    "            text = str(text).decode(\"ISO-8859-1\").encode(\"utf-8\")\n",
    "            yield (None,text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, '{commons category}}*')\n",
      "(None, 'official website')\n",
      "(None, '*')\n",
      "(None, 'Official ING NYC Marathon Fundraising Site')\n"
     ]
    }
   ],
   "source": [
    "for text in meta_text:\n",
    "    if ':' not in text:\n",
    "        print (None,str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top100_words_simple_no_metadata.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top100_words_simple_no_metadata.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import heapq\n",
    "import hashlib\n",
    "from lxml import etree\n",
    "import mwparserfromhell\n",
    "\n",
    "WORD_RE = re.compile(r\"\\w+\")\n",
    "\n",
    "\n",
    "class MRSimpleTextNoMeta(MRJob):\n",
    "\n",
    "    def mapper_page_init(self):\n",
    "\n",
    "        self.page = \"\"\n",
    "        self.counter = 0\n",
    "    \n",
    "    def mapper_page (self,_,line):\n",
    "        #print self.counter\n",
    "        \n",
    "        if '<page>' in line : \n",
    "            self.page = line\n",
    "        else :\n",
    "            self.page = self.page +\" \"+ line\n",
    "        if '</page>' in line :\n",
    "            hash_object = hashlib.md5(self.page).hexdigest()\n",
    "            yield (hash_object,self.page)\n",
    "        #    yield (\"page\",self.page)\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        self.counter=0\n",
    "        def remove_brackets(x):\n",
    "            #rep=\"\\[\\[.*?\\]\\]\"\n",
    "            rep1=\"\\{\\{.*?\\}\\}\"\n",
    "            rep2=\"\\<.*?\\>\"\n",
    "            rep3=\"\\[[.*?\\]]\"\n",
    "            a=re.sub(rep1,' ',x)\n",
    "            a=re.sub(rep2,' ',a)\n",
    "            a=re.sub(rep3,' ',a)\n",
    "            return (a) \n",
    "        self.remove_brackets=remove_brackets\n",
    "        \n",
    "        \n",
    "    def reducer_page(self, key, pchunk):\n",
    "        for chunk in pchunk:\n",
    "            self.counter+=1\n",
    "            #chunk='<page>'+' '+ chunk\n",
    "            #if self.counter<5:\n",
    "                #print chunk\n",
    "            \n",
    "            #if self.counter<5:\n",
    "            #    print \n",
    "            #print self.counter\n",
    "            #print chunk\n",
    "            parser = etree.XMLParser(recover=True)\n",
    "            \n",
    "            \n",
    "            #if self.counter==1032:\n",
    "                #chunk='<page> <revision> <text> aaaa </text> </revision> </page>'\n",
    "                #print chunk\n",
    "            \n",
    "            x = etree.fromstring(chunk,parser=parser)\n",
    "            #print 'xqqqqqqqqqqqqqq'\n",
    "           \n",
    "            #print etree.tostring(x)\n",
    "            if x is not None:\n",
    "                revs = x.findall(\".//revision\")\n",
    "                if len (revs)>0:\n",
    "                    revs=revs[-1]\n",
    "                    texts = revs.findall(\".//text\")\n",
    "                    if len (texts)>0:\n",
    "                        text_string = etree.tostring(texts[0])\n",
    "                        wikicode = mwparserfromhell.parse(text_string)\n",
    "                        text_string = \" \".join(\" \".join(fragment.value.split()) for fragment in wikicode.filter_text())\n",
    "                #print 'XXXXXX'\n",
    "                        ###print text_string\n",
    "                        yield (key,text_string)\n",
    "            #wikicode = mwparserfromhell.parse(text_string)\n",
    "            #text_string = wikicode.filter_text()\n",
    "            #text_string=''.join(str(text_string))\n",
    "            #text_string=self.remove_brackets(text_string)\n",
    "            \n",
    "     \n",
    "    \n",
    "    def get_words(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def sum_words(self, word, counts):\n",
    "        #for word,count in counts:\n",
    "            yield (word, sum(counts))\n",
    "            \n",
    "    def map_init(self):\n",
    "        self.heap=[]\n",
    "       \n",
    "    def map2(self,word,counts):\n",
    "        heapq.heappush(self.heap,(counts,word))\n",
    "        \n",
    "    def map_final(self):\n",
    "        yield (\"batch\",self.heap)\n",
    "    \n",
    "    def reducer2_init(self):\n",
    "        self.new_heap=[]\n",
    "    \n",
    "    def reducer2(self,_,heap):\n",
    "        count=0\n",
    "        for h in heap:\n",
    "            self.new_heap=heapq.chain(self.new_heap,h)\n",
    "           \n",
    "        n_largest=heapq.nlargest(100,self.new_heap)\n",
    "        #for i in n_largest:\n",
    "        #    print i[::1]\n",
    "        q=[(i[1],i[0]) for i in n_largest]\n",
    "        print q\n",
    "        #print n_largest\n",
    "        #print n_largest\n",
    "        \n",
    "        yield (None, q)\n",
    "    \n",
    "                       \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.mapper_page_init,\n",
    "                       mapper=self.mapper_page,\n",
    "                       reducer_init=self.reducer_init,\n",
    "                       reducer=self.reducer_page),\n",
    "                MRStep(mapper=self.get_words,\n",
    "                       reducer=self.sum_words),\n",
    "                MRStep(mapper_init=self.map_init,\n",
    "                       mapper=self.map2,\n",
    "                       mapper_final=self.map_final,\n",
    "                       reducer_init=self.reducer2_init,\n",
    "                       reducer=self.reducer2)]\n",
    "                       \n",
    "if __name__ == '__main__':\n",
    "    #x = re.findall(\"<page>[\\s\\S]*?<\\/page>\",part)\n",
    "    #len(x)\n",
    "    MRSimpleTextNoMeta.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile top_ngrams_entropy.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "import re\n",
    "import lxml\n",
    "from lxml import etree\n",
    "import mwparserfromhell\n",
    "import unicodedata\n",
    "import math\n",
    "\n",
    "WORD_RE = re.compile(r\"\\w+\")\n",
    "\n",
    "\n",
    "class DoubleLinks(MRJob):\n",
    "    \n",
    "    #### Step 1 : Consolidate pages ####\n",
    "    def mapper_page_init(self):\n",
    "        self.page = \"\"\n",
    "        self.counter = 0 \n",
    "    def mapper_page (self,_,line):\n",
    "        if '<page>' in line : \n",
    "            self.page = line\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            self.page = self.page +\" \"+ line\n",
    "        \n",
    "        if '</page>' in line :           \n",
    "            if self.counter > 0 and self.counter < 104:\n",
    "                yield (self.counter,self.page)     \n",
    "            \n",
    "            \n",
    "    def reducer_page(self, _, pchunk):\n",
    "        for chunk in pchunk:\n",
    "            self.increment_counter(\"Word Counter\", \"Pages Reviewed\", amount=1)\n",
    "            texts = etree.fromstring(chunk)\n",
    "            ##if '<revision>'in texts:\n",
    "            texts = texts.findall(\".//revision\")[1]\n",
    "            texts = texts.findall(\".//text\")\n",
    "            text_string = etree.tostring(texts[0])\n",
    "            wikicode = mwparserfromhell.parse(text_string)\n",
    "            text = \" \".join(\" \".join(fragment.value.split())\n",
    "                      for fragment in wikicode.filter_wikilinks())\n",
    "            text = text.encode('utf-8') \n",
    "            yield (None,text)\n",
    "    \n",
    "\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_page_init,\n",
    "                   mapper=self.mapper_page,\n",
    "                   reducer=self.reducer_page\n",
    "                  ),\n",
    "            MRStep(mapper_init=self.mapper_init_charcount,\n",
    "                   mapper=self.mapper_charcount,\n",
    "            #       combiner=self.combiner,\n",
    "                   reducer=self.reducer_charcount)\n",
    "            ]\n",
    "if __name__ == '__main__':\n",
    "    MRTopNGrams.run()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 4\n",
    "from top_ngrams_entropy import MRTopNGrams\n",
    "##filename = 'sample.xml'\n",
    "##filename = 'part-00026.xml'\n",
    "filename = 'simple'\n",
    "##filename = 'wikithai'\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "mr_job = MRTopNGrams(args=[filename])\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    print runner.counters()\n",
    "    ##for line in runner.stream_output():\n",
    "        ##print line\n",
    "\n",
    "print(\"--- %s min ---\" % (((time.time() - start_time))/60)  )  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer = [(('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department', 'france'), 0.06825002019931593), (('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department', 'departments of france'), 0.06816959025546497), (('communes of the vend\\\\u00e9e department', 'france'), 0.06732691607213888), (('illinois', 'list of cities in illinois'), 0.06709503610594328), (('communes of the allier department', 'france'), 0.0669888479020342), (('communes of the allier department', 'departments of france'), 0.0667632804953407), (('communes of the yonne department', 'france'), 0.0665933570329292), (('communes of the yonne department', 'departments of france'), 0.06649728320993599), (('list of cities in illinois', 'united states'), 0.06642777388943956), (('city', 'list of cities in illinois'), 0.06486918837762555), (('communes of the mayenne department', 'france'), 0.06441469801963563), (('list of cities in indiana', 'united states'), 0.0642825118200217), (('dolj county', 'romania'), 0.061818313540956286), (('communes of the pas-de-calais department', 'france'), 0.06063394551773228), (('communes of the pas-de-calais department', 'departments of france'), 0.060534167840750566), (('iowa', 'list of cities in iowa'), 0.060249012058122484), (('communes of the aisne department', 'france'), 0.06018953539234461), (('communes of the aisne department', 'departments of france'), 0.06013498206908435), (('list of cities in iowa', 'united states'), 0.05996983823372155), (('city', 'list of cities in iowa'), 0.05989148958097098), (('communes of the gironde department', 'france'), 0.059672265545566656), (('communes of the gironde department', 'departments of france'), 0.05954958923801437), (('communes of the sarthe department', 'france'), 0.058845936015815774), (('communes of the sarthe department', 'departments of france'), 0.05873030518050841), (('list of cities in oklahoma', 'united states'), 0.05837812057467899), (('list of cities in oklahoma', 'oklahoma'), 0.05835728724134566), (('communes of the ard\\\\u00e8che department', 'france'), 0.05825406944175671), (('communes of the ain department', 'france'), 0.05800335494685684), (('communes of the ain department', 'departments of france'), 0.057969869439630416), (('ain', 'communes of the ain department'), 0.05786664097179347), (('communes of the vaucluse department', 'france'), 0.05769413809053141), (('communes of the calvados department', 'france'), 0.05763990496368279), (('communes of the calvados department', 'departments of france'), 0.057525982373471694), (('communes of the ard\\\\u00e8che department', 'departments of france'), 0.05622435402990622), (('communes of the ard\\\\u00e8che department', 'departments of france'), 0.056224354029906214), (('arkansas', 'list of cities in arkansas'), 0.05553318554754128), (('idaho', 'list of cities in idaho'), 0.05552855951510441), (('list of cities in idaho', 'united states'), 0.05539981894736573), (('communes of the alpes-maritimes department', 'france'), 0.05537579260779012), (('list of cities in arkansas', 'united states'), 0.05535927250406302), (('city', 'list of cities in arkansas'), 0.05466340875710438), (('communes of the alpes-maritimes department', 'departments of france'), 0.05464413681987893), (('city', 'list of cities in idaho'), 0.05444266001148339), (('essex', 'list of civil parishes in essex'), 0.053060339890832905), (('kentucky', 'list of cities in kentucky'), 0.05182799953150506), (('belgium', 'list of cities in belgium'), 0.051657947794183354), (('list of cities in kentucky', 'united states'), 0.05153528339808095), (('city', 'list of cities in kentucky'), 0.05153495768746181), (('arrondissement of largenti\\\\u00e8re', 'france'), 0.05073709613649795), (('communes of the bouches-du-rh\\\\u00f4ne department', 'france'), 0.050382819755594985), (('romania', 'suceava county'), 0.049781900828279074), (('arrondissement of bayeux', 'france'), 0.049682358935990927), (('arrondissement of largenti\\\\u00e8re', 'departments of france'), 0.04961734228005818), (('arrondissement of bayeux', 'departments of france'), 0.04922972859556402), (('belgium', 'flemish brabant'), 0.04915669612643677), (('arrondissement of privas', 'france'), 0.04894222203599536), (('communes of the bouches-du-rh\\\\u00f4ne department', 'departments of france'), 0.0488949842822584), (('arrondissement of belley', 'france'), 0.048547418874361), (('belgium', 'hainaut (province)'), 0.0484770605606388), (('arrondissement of belley', 'departments of france'), 0.04772628836265976), (('japan', 'list of japan international footballers'), 0.0467712707691298), (('arrondissement of privas', 'departments of france'), 0.04607602212495078), (('belgium', 'telephone numbers in belgium'), 0.045023654710355465), (('list of towns in virginia', 'united states'), 0.044572460127338676), (('japan', 'list of japanese footballers'), 0.044162435449358384), (('florida', 'list of settlements in florida'), 0.04412688848396136), (('belgium', 'west flanders'), 0.04333382305659213), (('belgium', 'li\\\\u00e8ge (province)'), 0.042843589879673336), (('france', 'list of french departments by population'), 0.042772662573012106), (('antwerp (province)', 'belgium'), 0.04252577877688411), (('departments of france', 'list of french departments by population'), 0.03893392592316101), (('belgium', 'east flanders'), 0.03860339181807095), (('belgium', 'luxembourg (belgium)'), 0.03852015151158596), (('belgium', 'namur (province)'), 0.03823283481967761), (('communes of the aube department', 'departments of france'), 0.03702625547753828), (('communes of the aube department', 'departments of france'), 0.037026255477538274), (('communes of the aube department', 'france'), 0.03692045600855769), (('list of settlements in florida', 'united states'), 0.03589471350268824), (('belgium', 'limburg (belgium)'), 0.03575355189930803), (('idaho', 'united states'), 0.0355952687447257), (('france', 'list of cities in france'), 0.03467150578983111), (('departments of france', 'france'), 0.034202524418152476), (('city', 'list of settlements in florida'), 0.03412649689115581), (('arrondissement of privas', 'communes of the ard\\\\u00e8che department'), 0.032148621579164934), (('arrondissement of largenti\\\\u00e8re', 'communes of the ard\\\\u00e8che department'), 0.03202904340084403), (('italy', 'list of cities in italy'), 0.03142299657472391), (('germany', 'list of cities in germany with more than 100,000 inhabitants'), 0.03024578804990069), (('1999', 'communes of the vaucluse department'), 0.030169316658287204), (('chicago', 'mayor of chicago'), 0.02985943456704292), (('1999', 'communes of the mayenne department'), 0.02954448927585469), (('germany', 'north rhine-westphalia'), 0.027397240632923807), (('germany', 'urban districts of germany'), 0.026757625398486154), (('mayor of chicago', 'united states'), 0.026026886831909894), (('italy', 'regions of italy'), 0.025849422866570482), (('2006', 'list of french departments by population'), 0.025117418378834803), (('kentucky', 'united states'), 0.025075577757247232), (('list of ioc country codes', 'summer olympic games'), 0.024115226450299853), (('list of tallest buildings in the united states', 'united states'), 0.024013933488072663), (('france', 'lists of cantons of france'), 0.02356991173497723), (('italy', 'provinces of italy'), 0.023345087270139692), (('british rail', 'list of british rail modern traction locomotive classes'), 0.02332854018752648), (('germany', 'saxony'), 0.0228601423754174), (('france', 'lists of communes of france'), 0.022692780316570047), (('segunda divisi\\\\u00f3n', 'spain'), 0.02257759773050486), (('japan', 'list of j. league players'), 0.022093835189117107), (('departments of france', 'lists of cantons of france'), 0.02198406494134653), (('japan', 'list of provinces of japan'), 0.02123295650033441), (('provinces of italy', 'regions of italy'), 0.021068256192122024), (('japan', 'list of yokohama f. marinos players'), 0.02081878738775042), (('japan', 'prefectures of japan'), 0.020596698796665633), (('british rail', 'list of british rail diesel multiple unit classes'), 0.020319967541153536), (('alaska', 'list of cities in alaska'), 0.02024830581106988), (('departments of france', 'lists of communes of france'), 0.02018757569860292), (('departments of france', 'lists of communes of france'), 0.02018757569860292), (('belgium', 'list of rivers of belgium'), 0.019806210571943747), (('japan', 'list of sanfrecce hiroshima players'), 0.019733975730893114), (('list of united states cities by population', 'united states'), 0.019693456614132902), (('list of ioc country codes', 'winter olympic games'), 0.01909340727062727), (('france', 'french civil aviation university'), 0.01875333688327837), (('2011\\\\u201312 juventus f.c. season', 'italy'), 0.01838422721850844), (('japan', 'list of japanese football teams'), 0.01810377378063885), (('italy', 'serie d'), 0.017935797129511377), (('administrative divisions of azerbaijan', 'azerbaijan'), 0.01785993143969559), (('germany', 'zwickau rural district'), 0.01735666177041608), (('ichinomiya', 'list of provinces of japan'), 0.017143720078395125), (('india', 'uttar pradesh'), 0.01685745703154031), (('list of cities in italy', 'regions of italy'), 0.016731541709562366), ((\"list of u.s. states' largest cities\", 'united states'), 0.016672455272457105), ((\"list of u.s. states' largest cities\", 'united states'), 0.016672455272457105), (('echl', 'national hockey league'), 0.01657206572606903), (('comparison of regions of japan', 'japan'), 0.016537851818537046), (('australia', 'new south wales'), 0.016477189883082482), (('american hockey league', 'echl'), 0.01640276835507372), (('core cities of japan', 'japan'), 0.01631532547343719), (('germany', 'list of administrative divisions of germany'), 0.016226527348799185), (('2007-08 nhl season', 'national hockey league'), 0.01592716797143552), (('departments of france', 'list of cities in france'), 0.01586932159855949), (('germany', 'germany national football team'), 0.015757657564001014), (('japan', 'list of the 53 stations of the t\\\\u014dkaid\\\\u014d'), 0.01575185688102915), (('british rail', 'list of british rail electric multiple unit classes'), 0.015734483269504558), (('list of cities in italy', 'provinces of italy'), 0.015659268377260872), (('city', 'list of united states cities by population'), 0.015406911810653432), (('docg', 'italy'), 0.015154047071346266), (('list of ioc country codes', 'olympic games'), 0.01502037056783245), (('association of american universities', 'united states'), 0.014953804116004736), (('australia', 'list of cities in australia'), 0.014855728616322485), (('list of u.s. states', 'united states'), 0.01475006903493294), (('2009\\\\u201310 nhl season', 'national hockey league'), 0.014660947055393071), (('france', 'list of bastides'), 0.014618597421911461), (('list of u.s. state capitals', 'united states'), 0.01443583158277038), (('city', 'list of cities in alaska'), 0.01429093196706684), (('germany', 'hesse'), 0.014284916160248663), (('friends', 'united states'), 0.014226047814786277), (('england', 'list of english football stadiums by capacity'), 0.014083960626477198), (('australia', 'sydney'), 0.013989488275402051)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department', 'france'),\n",
       " 0.06825002019931593)"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer2 =  [(('communes of the yonne department', 'france'), 0.06851956783485441), (('communes of the yonne department', 'departments of france'), 0.0684190413603516), (('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department', 'france'), 0.06838308170477443), (('communes of the yonne department', 'yonne'), 0.06830510902166947), (('communes of france', 'communes of the yonne department'), 0.06830108901309988), (('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department', 'departments of france'), 0.06829913140425876), (('illinois', 'list of cities in illinois'), 0.06825788908552215), (('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department', 'pyr\\\\u00e9n\\\\u00e9es-atlantiques'), 0.06824276476154595), (('commune in france', 'communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department'), 0.06819610556035147), (('communes of the allier department', 'france'), 0.06763361651363364)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('communes of the yonne department', 'france')"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer2[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('communes of the yonne department', 'france'), 0.06851956783485441),\n",
       " (('communes of the yonne department', 'departments of france'),\n",
       "  0.0684190413603516),\n",
       " (('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department', 'france'),\n",
       "  0.06838308170477443),\n",
       " (('communes of the yonne department', 'yonne'), 0.06830510902166947),\n",
       " (('communes of france', 'communes of the yonne department'),\n",
       "  0.06830108901309988),\n",
       " (('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department',\n",
       "   'departments of france'),\n",
       "  0.06829913140425876),\n",
       " (('illinois', 'list of cities in illinois'), 0.06825788908552215),\n",
       " (('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department',\n",
       "   'pyr\\\\u00e9n\\\\u00e9es-atlantiques'),\n",
       "  0.06824276476154595),\n",
       " (('commune in france',\n",
       "   'communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department'),\n",
       "  0.06819610556035147),\n",
       " (('communes of the allier department', 'france'), 0.06763361651363364)]"
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item1 in answer2 :\n",
    "    pair1 = item1[0]\n",
    "    for item2 in answer:\n",
    "        pair2=item2[0]\n",
    "        if pair2==pair1:\n",
    "            item2 = item1\n",
    "    if item1 not in answer:\n",
    "        answer.append(item1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_answer = sorted(answer, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grader = new_answer[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('communes of the yonne department', 'france'), 0.06851956783485441),\n",
       " (('communes of the yonne department', 'departments of france'),\n",
       "  0.0684190413603516),\n",
       " (('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department', 'france'),\n",
       "  0.06838308170477443),\n",
       " (('communes of the yonne department', 'yonne'), 0.06830510902166947),\n",
       " (('communes of france', 'communes of the yonne department'),\n",
       "  0.06830108901309988),\n",
       " (('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department',\n",
       "   'departments of france'),\n",
       "  0.06829913140425876),\n",
       " (('illinois', 'list of cities in illinois'), 0.06825788908552215),\n",
       " (('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department', 'france'),\n",
       "  0.06825002019931593),\n",
       " (('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department',\n",
       "   'pyr\\\\u00e9n\\\\u00e9es-atlantiques'),\n",
       "  0.06824276476154595),\n",
       " (('commune in france',\n",
       "   'communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department'),\n",
       "  0.06819610556035147),\n",
       " (('communes of the pyr\\\\u00e9n\\\\u00e9es-atlantiques department',\n",
       "   'departments of france'),\n",
       "  0.06816959025546497),\n",
       " (('communes of the allier department', 'france'), 0.06763361651363364),\n",
       " (('communes of the vend\\\\u00e9e department', 'france'), 0.06732691607213888),\n",
       " (('illinois', 'list of cities in illinois'), 0.06709503610594328),\n",
       " (('communes of the allier department', 'france'), 0.0669888479020342),\n",
       " (('communes of the allier department', 'departments of france'),\n",
       "  0.0667632804953407),\n",
       " (('communes of the yonne department', 'france'), 0.0665933570329292),\n",
       " (('communes of the yonne department', 'departments of france'),\n",
       "  0.06649728320993599),\n",
       " (('list of cities in illinois', 'united states'), 0.06642777388943956),\n",
       " (('city', 'list of cities in illinois'), 0.06486918837762555),\n",
       " (('communes of the mayenne department', 'france'), 0.06441469801963563),\n",
       " (('list of cities in indiana', 'united states'), 0.0642825118200217),\n",
       " (('dolj county', 'romania'), 0.061818313540956286),\n",
       " (('communes of the pas-de-calais department', 'france'), 0.06063394551773228),\n",
       " (('communes of the pas-de-calais department', 'departments of france'),\n",
       "  0.060534167840750566),\n",
       " (('iowa', 'list of cities in iowa'), 0.060249012058122484),\n",
       " (('communes of the aisne department', 'france'), 0.06018953539234461),\n",
       " (('communes of the aisne department', 'departments of france'),\n",
       "  0.06013498206908435),\n",
       " (('list of cities in iowa', 'united states'), 0.05996983823372155),\n",
       " (('city', 'list of cities in iowa'), 0.05989148958097098),\n",
       " (('communes of the gironde department', 'france'), 0.059672265545566656),\n",
       " (('communes of the gironde department', 'departments of france'),\n",
       "  0.05954958923801437),\n",
       " (('communes of the sarthe department', 'france'), 0.058845936015815774),\n",
       " (('communes of the sarthe department', 'departments of france'),\n",
       "  0.05873030518050841),\n",
       " (('list of cities in oklahoma', 'united states'), 0.05837812057467899),\n",
       " (('list of cities in oklahoma', 'oklahoma'), 0.05835728724134566),\n",
       " (('communes of the ard\\\\u00e8che department', 'france'), 0.05825406944175671),\n",
       " (('communes of the ain department', 'france'), 0.05800335494685684),\n",
       " (('communes of the ain department', 'departments of france'),\n",
       "  0.057969869439630416),\n",
       " (('ain', 'communes of the ain department'), 0.05786664097179347),\n",
       " (('communes of the vaucluse department', 'france'), 0.05769413809053141),\n",
       " (('communes of the calvados department', 'france'), 0.05763990496368279),\n",
       " (('communes of the calvados department', 'departments of france'),\n",
       "  0.057525982373471694),\n",
       " (('communes of the ard\\\\u00e8che department', 'departments of france'),\n",
       "  0.05622435402990622),\n",
       " (('communes of the ard\\\\u00e8che department', 'departments of france'),\n",
       "  0.056224354029906214),\n",
       " (('arkansas', 'list of cities in arkansas'), 0.05553318554754128),\n",
       " (('idaho', 'list of cities in idaho'), 0.05552855951510441),\n",
       " (('list of cities in idaho', 'united states'), 0.05539981894736573),\n",
       " (('communes of the alpes-maritimes department', 'france'),\n",
       "  0.05537579260779012),\n",
       " (('list of cities in arkansas', 'united states'), 0.05535927250406302),\n",
       " (('city', 'list of cities in arkansas'), 0.05466340875710438),\n",
       " (('communes of the alpes-maritimes department', 'departments of france'),\n",
       "  0.05464413681987893),\n",
       " (('city', 'list of cities in idaho'), 0.05444266001148339),\n",
       " (('essex', 'list of civil parishes in essex'), 0.053060339890832905),\n",
       " (('kentucky', 'list of cities in kentucky'), 0.05182799953150506),\n",
       " (('belgium', 'list of cities in belgium'), 0.051657947794183354),\n",
       " (('list of cities in kentucky', 'united states'), 0.05153528339808095),\n",
       " (('city', 'list of cities in kentucky'), 0.05153495768746181),\n",
       " (('arrondissement of largenti\\\\u00e8re', 'france'), 0.05073709613649795),\n",
       " (('communes of the bouches-du-rh\\\\u00f4ne department', 'france'),\n",
       "  0.050382819755594985),\n",
       " (('romania', 'suceava county'), 0.049781900828279074),\n",
       " (('arrondissement of bayeux', 'france'), 0.049682358935990927),\n",
       " (('arrondissement of largenti\\\\u00e8re', 'departments of france'),\n",
       "  0.04961734228005818),\n",
       " (('arrondissement of bayeux', 'departments of france'), 0.04922972859556402),\n",
       " (('belgium', 'flemish brabant'), 0.04915669612643677),\n",
       " (('arrondissement of privas', 'france'), 0.04894222203599536),\n",
       " (('communes of the bouches-du-rh\\\\u00f4ne department',\n",
       "   'departments of france'),\n",
       "  0.0488949842822584),\n",
       " (('arrondissement of belley', 'france'), 0.048547418874361),\n",
       " (('belgium', 'hainaut (province)'), 0.0484770605606388),\n",
       " (('arrondissement of belley', 'departments of france'), 0.04772628836265976),\n",
       " (('japan', 'list of japan international footballers'), 0.0467712707691298),\n",
       " (('arrondissement of privas', 'departments of france'), 0.04607602212495078),\n",
       " (('belgium', 'telephone numbers in belgium'), 0.045023654710355465),\n",
       " (('list of towns in virginia', 'united states'), 0.044572460127338676),\n",
       " (('japan', 'list of japanese footballers'), 0.044162435449358384),\n",
       " (('florida', 'list of settlements in florida'), 0.04412688848396136),\n",
       " (('belgium', 'west flanders'), 0.04333382305659213),\n",
       " (('belgium', 'li\\\\u00e8ge (province)'), 0.042843589879673336),\n",
       " (('france', 'list of french departments by population'),\n",
       "  0.042772662573012106),\n",
       " (('antwerp (province)', 'belgium'), 0.04252577877688411),\n",
       " (('departments of france', 'list of french departments by population'),\n",
       "  0.03893392592316101),\n",
       " (('belgium', 'east flanders'), 0.03860339181807095),\n",
       " (('belgium', 'luxembourg (belgium)'), 0.03852015151158596),\n",
       " (('belgium', 'namur (province)'), 0.03823283481967761),\n",
       " (('communes of the aube department', 'departments of france'),\n",
       "  0.03702625547753828),\n",
       " (('communes of the aube department', 'departments of france'),\n",
       "  0.037026255477538274),\n",
       " (('communes of the aube department', 'france'), 0.03692045600855769),\n",
       " (('list of settlements in florida', 'united states'), 0.03589471350268824),\n",
       " (('belgium', 'limburg (belgium)'), 0.03575355189930803),\n",
       " (('idaho', 'united states'), 0.0355952687447257),\n",
       " (('france', 'list of cities in france'), 0.03467150578983111),\n",
       " (('departments of france', 'france'), 0.034202524418152476),\n",
       " (('city', 'list of settlements in florida'), 0.03412649689115581),\n",
       " (('arrondissement of privas', 'communes of the ard\\\\u00e8che department'),\n",
       "  0.032148621579164934),\n",
       " (('arrondissement of largenti\\\\u00e8re',\n",
       "   'communes of the ard\\\\u00e8che department'),\n",
       "  0.03202904340084403),\n",
       " (('italy', 'list of cities in italy'), 0.03142299657472391),\n",
       " (('germany', 'list of cities in germany with more than 100,000 inhabitants'),\n",
       "  0.03024578804990069),\n",
       " (('1999', 'communes of the vaucluse department'), 0.030169316658287204),\n",
       " (('chicago', 'mayor of chicago'), 0.02985943456704292),\n",
       " (('1999', 'communes of the mayenne department'), 0.02954448927585469)]"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
