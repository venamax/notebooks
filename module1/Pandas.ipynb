{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is Python's answer to R.  It's a good tool for small(ish) data analysis -- i.e. when everything fits into memory.\n",
    "\n",
    "The basic new \"noun\" in pandas is the **data frame**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nouns (objects) in Pandas\n",
    "\n",
    "### Data Frames\n",
    "\n",
    "Like a table, with rows and columns (e.g. as in SQL).  Except:\n",
    "  - The rows can be indexed by something interesting (there is special support for labels like categorical and timeseries data).  This is especially useful when you have timeseries data with potentially missing data points.\n",
    "  - Cells can store Python objects. Like in SQL, columns are type homogeneous.\n",
    "  - Instead of \"NULL\", the name for a non-existent value is \"NA\".  Unlike R, Python's data frames only support NAs in columns of some data types (basically: floating point numbers and 'objects') -- but this is mostly a non-issue (because it will \"up-cast\" integers to float64, etc.)\n",
    "  \n",
    "### Data Series:\n",
    "These are named columns of a DataFrame (more correctly, a dataframe is a dictionary of Series).  The entries of the series have homogenous type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      cat\n",
      "1      dog\n",
      "2    mouse\n",
      "Name: animal, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# a data frame\n",
    "df1 = pd.DataFrame({\n",
    "    'number': [1, 2, 3],\n",
    "    'animal': ['cat', 'dog', 'mouse']\n",
    "})\n",
    "\n",
    "# the same data frame\n",
    "df2 = pd.DataFrame([\n",
    "    ('cat', 1),\n",
    "    ('dog', 2),\n",
    "    ('mouse', 3),\n",
    "], columns=['animal', 'number'])\n",
    "\n",
    "assert(np.all(df1 == df2))\n",
    "\n",
    "# a series\n",
    "print df1['animal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbs (operations) in Pandas\n",
    "  \n",
    "Pandas provides a \"batteries-included\" basic data analysis:\n",
    "  - **Loading data:** `read_csv`, `read_table`, `read_sql`, and `read_html`\n",
    "  - **Selection, filtering, and aggregation** (i.e. SQL-type operations): There's a special syntax for `SELECT`ing.  There's the `merge` method for `JOIN`ing.  There's also an easy syntax for what in SQL is a mouthful: Creating a new column whose value is computed from another column -- with the bonus that now the computations can use the full power of Python (though it might be faster if it didn't).\n",
    "  - **\"Pivot table\" style aggregation:** If you're an Excel cognoscenti, you may appreciate this.\n",
    "  - **NA handling:** Like R's data frames, there is good support for transforming NA values with default values / averaging tricks / etc.\n",
    "  - **Basic statistics:** e.g. `mean`, `median`, `max`, `min`, and the convenient `describe`.\n",
    "  - **Plugging into more advanced analytics:** Okay, this isn't batteries included.  But still, it plays reasonably with `sklearn`.\n",
    "  - **Visualization:** For instance `plot` and `hist`.\n",
    "  \n",
    "We'll go through a little on all of these in the context of an example.  To go through it, you must have the (output) data files from the HMDA \"Project structure\" example.  We're going to explore a dataset of mortgage insurance issued by the Federal Housing Authority (FHA).  The data is broken down by census tract and tells us how big of a player the FHA is in each tract (how many homes etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data (and basic statistics / visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'sort_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-df52b3b02ff4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GEOID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Census_Tract_Number'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m6\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'County_Code'\u001b[0m\u001b[1;33m]\u001b[0m     \u001b[1;33m+\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m9\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'State_Code'\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;31m## A computed field!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'State_Code'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# sorting data to make it easier to read\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2148\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m             raise AttributeError(\"'%s' object has no attribute '%s'\" %\n\u001b[1;32m-> 2150\u001b[1;33m                                  (type(self).__name__, name))\n\u001b[0m\u001b[0;32m   2151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'sort_values'"
     ]
    }
   ],
   "source": [
    "names =[\"State_Code\", \"County_Code\", \"Census_Tract_Number\", \"NUM_ALL\", \"NUM_FHA\", \"PCT_NUM_FHA\", \"AMT_ALL\", \"AMT_FHA\", \"PCT_AMT_FHA\"]\n",
    "df = pd.read_csv('small_data/fha_by_tract.csv', names=names)  ## Loading a CSV file, without a header (so we have to provide field names)\n",
    "\n",
    "df['GEOID'] = df['Census_Tract_Number']*100 + 10**6 * df['County_Code'] \\\n",
    "    + 10**9 * df['State_Code']   ## A computed field!\n",
    "    \n",
    "df2 = df.sort_values('State_Code')  # sorting data to make it easier to read\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## To drop a column\n",
    "\n",
    "df.drop('GEOID', axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note how sample() is used to speed up plotting/processing by subsampling the rows of the dataframe.\n",
    "\n",
    "print \"Basic statistics and a histogram of the percentage of mortages\"\n",
    "print \"in each census tract insured by FHA\"\n",
    "print df['PCT_AMT_FHA'].describe()\n",
    "df['PCT_AMT_FHA'].hist(bins=50, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The above distribution looks a little skewed, let's look at it's log\"\n",
    "print \"We can save off the data into a new column\"\n",
    "df['LOG_AMT_ALL'] = np.log1p(df['AMT_ALL'])\n",
    "print df['LOG_AMT_ALL'].describe()\n",
    "\n",
    "print \"We can use the apply function to transform data\"\n",
    "df['AMT_ALL'].apply(np.log1p).hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data\n",
    "\n",
    "There are 4 ways to select data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Selecting off a column\"\n",
    "print df['State_Code'].head()\n",
    "print\n",
    "print \"Selecting off multiple columns\"\n",
    "print df[['State_Code', 'County_Code']].head()\n",
    "print\n",
    "print \"programatically access column names\"\n",
    "print all([col for col in df] == df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_simple = df.dropna()\n",
    "\n",
    "# to select both row and column, use `.loc`\n",
    "print df_simple.loc[3, 'State_Code']\n",
    "\n",
    "# to index columns by their integer position (instead of the label), use `.iloc`\n",
    "print (df_simple.loc[:,'State_Code'] == df_simple.iloc[:,0]).all()\n",
    "\n",
    "# .ix can do both label and position\n",
    "print (df_simple.ix[:,'State_Code'] == df_simple.ix[:,0]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering data\n",
    "\n",
    "Now the `df[...]` notation is very flexible:\n",
    "  - It accepts column names (strings and lists of strings);\n",
    "  - It accepts column numbers (so long as there is no ambiguity with column names);\n",
    "  - It accepts _binary data series!_\n",
    "  \n",
    "This means that you can write\n",
    "\n",
    "        df[ df['column_name2']==MD & ( df['column_name1']==5 | df['column_name1']==6 ) ]\n",
    "   \n",
    "for what you would write in SQL as\n",
    "\n",
    ">         SELECT * FROM df WHERE\n",
    "            column_name2='MD\" AND (column_name1=5 OR column_name1=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Selection returns a boolean array ..\"\n",
    "print (df['State_Code'] == 1).head()\n",
    "print\n",
    "print \"... we can apply the usual boolean operators to it\"\n",
    "print ((df['State_Code'] == 1) & (df['Census_Tract_Number'] == 9613)).head()\n",
    "print\n",
    "print \"pandas indices take boolean lists of the appropriate length\"\n",
    "print df[df['State_Code'] == 5].head()\n",
    "print\n",
    "print \"p.s. - numpy indices also take boolean arguments: here are some odd numbers\"\n",
    "print np.arange(10)[np.arange(10) % 2 == 1]\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** selecting rows by binary data series only if they share the same data index!\n",
    "\n",
    "**Exercise:**\n",
    "1. Plot the histogram of percentages for different states in the same graph to compare them.\n",
    "2. Notice that there is a spike at 100%.  This means that the FHA has insured 100% of the houses in that census tract.  See what happens to the histogram when we restrict it to the case where the total number of loans is non-negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining data\n",
    "\n",
    "The analogue of a\n",
    "\n",
    ">             \n",
    "    SELECT * \n",
    "        FROM df1\n",
    "        INNER JOIN df2 \n",
    "        ON df1.field_name=df2.field_name;\n",
    "\n",
    "is\n",
    "\n",
    "    df_joined = df1.merge(df2, on='field_name')\n",
    "\n",
    "You can also do left / right / outer joins, mix-and-match column names, etc.  For that consult the Pandas documentation. (The example below will do a left join.)\n",
    "\n",
    "Of course, just looking at the distribution of insurance by census tract isn't interesting unless we know more about the census tract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading information about census tracts\n",
    "df_geo = pd.read_csv('small_data/2013_Gaz_tracts_national.tsv', sep='\\t')\n",
    "\n",
    "print \"joining two dataframes\"\n",
    "df_joined = df.merge(df_geo, on='GEOID', how='left')\n",
    "df_joined.sort_values('AMT_ALL', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating data\n",
    "\n",
    "The analog of SQL's `GROUP BY` is\n",
    "\n",
    "    grouped = df.groupby(['field_name1', ...])...\n",
    "\n",
    "The above is analogous to\n",
    ">             \n",
    "    SELECT mean(df.value1), std(df.value2) \n",
    "        FROM df\n",
    "        GROUP BY df.field_name1, ...\n",
    "\n",
    "Pandas is somewhat more flexible in how you can use grouping, not requiring you to specify an aggregation function up front.  A few examples are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This isn't a SQL-style 'GROUP BY'.\n",
    "df_joined.groupby('USPS').first().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the analog of\n",
    "# SELECT USPS, SUM(AMT_FHA), SUM(AMT_ALL), ... FROM df GROUP BY USPS;\n",
    "df_by_state = df_joined.groupby('USPS')['AMT_FHA', 'AMT_ALL', 'NUM_FHA', 'NUM_ALL'].sum()\n",
    "\n",
    "df_by_state['PCT_AMT_FHA'] = 100.0 * df_by_state['AMT_FHA']  / df_by_state['AMT_ALL']\n",
    "df_by_state['PCT_NUM_FHA'] = 100.0 * df_by_state['NUM_FHA']  / df_by_state['NUM_ALL']\n",
    "\n",
    "# This sure looks different than the census-tract level histogram!\n",
    "df_by_state['PCT_AMT_FHA'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting by indices and columns\n",
    "\n",
    "We can sort by the row (or coumn) index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_by_state.sort_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sort by the value in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_by_state.sort_values('AMT_FHA', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique values\n",
    "\n",
    "As in SQL, pandas can compute unique values, value counts, and test for membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"array of unique values\"\n",
    "print df['State_Code'].unique()[:10]\n",
    "print\n",
    "print \"Value counts (as series)\"\n",
    "print df['State_Code'].value_counts().head()\n",
    "print \n",
    "print \"Test if numbers are small\"\n",
    "print df['State_Code'].isin(df['State_Code'].head(3)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing and NA data\n",
    "\n",
    "\n",
    "When you read in a CSV file / SQL database there are often \"NA\" (or \"null\", \"None\", etc.) values.  The CSV reader has a special field for specifying how this is denoted, and SQL has the built-in notion of NULL.  Pandas provides some tools for working with these -- they are generally similar to (and a little bit worse than) `R`.\n",
    "\n",
    "Note that these methods are by default not in place -- that is, they create a new series and do not change the original one.\n",
    "\n",
    "For more details: http://pandas.pydata.org/pandas-docs/stable/missing_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df['GEOID'][:10]\n",
    "print df['GEOID'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# isnull() / notnull() test for null-ness and return a Boolean series\n",
    "\n",
    "df['GEOID'].isnull()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dropna() removes the rows with null data\n",
    "\n",
    "df['GEOID'].dropna().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fillna() replaces N/A values with something else\n",
    "# interpolate() replaces null values by (linear, or quadratic, or...) interpolation\n",
    "# There is support for indexing by times (not necessarily equally spaced), etc. in the documentation.\n",
    "\n",
    "df['fill_0'] = df['GEOID'].fillna(0)                        # Fills constant value, here 0\n",
    "df['fill_forward'] = df['GEOID'].fillna(method='ffill')           # Fill forwards\n",
    "df['fill_back'] = df['GEOID'].fillna(method='bfill', limit=5)  # Fill backwards, at most 5\n",
    "df['fill_mean'] = df['GEOID'].fillna(df['GEOID'].mean())       # Fills constant value, here the mean (imputation)\n",
    "df['fill_interp'] = df['GEOID'].interpolate()                    # Fills interpolated value\n",
    "df[['fill_0', 'fill_forward', 'fill_back', 'fill_mean', 'fill_interp']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "N/A values are (usually) smartly ignored when performing other calculations on dataframes. For example, when using string methods on series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_series = df['PCT_NUM_FHA'].replace(0, np.nan).apply(str)\n",
    "print text_series[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_series[:10].str.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = df_joined['USPS'].dropna()\n",
    "states[states.str.contains('A')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indices in Pandas\n",
    "\n",
    "Pandas indices is a powerful mechanism that allow us to handle data naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pandas will associate elements based on the index, not the ordering\n",
    "s1 = pd.Series([1,2,3], index=['a', 'b', 'c'])\n",
    "s2 = pd.Series([3,2,1], index=['c', 'b', 'a'])\n",
    "assert(np.all(s1 + s2 == 2 * s1))\n",
    "\n",
    "# pandas can also handle missing indices by inserting `NaN`\n",
    "s3 = pd.Series([3,2,1], index=['c', 'd', 'e'])\n",
    "print s1 + s3\n",
    "print\n",
    "\n",
    "# pandas also supports arithmetic operations assuming input fill values\n",
    "print s1.add(s3, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function application and mapping\n",
    "\n",
    "For element-wise function application, the most straightforward thing to do is to apply numpy functions to these objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.arange(24).reshape(4,6))\n",
    "np.sin(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, sometimes you want to compute things column-wise or row-wise.  In this case, you will need to use the `apply` method.  For example, the following takes the range of each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(np.arange(24).reshape(4,6))\n",
    "\n",
    "print \"Range of each column\"\n",
    "print df2.apply(lambda x: x.max() - x.min())\n",
    "print\n",
    "print \"Range of each row\"\n",
    "print df2.apply(lambda x: x.max() - x.min(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can apply more complex pure-python functions elementwise using the `applymap` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(np.arange(24).reshape(4,6))\n",
    "df3.applymap(lambda x: \"%.2f\" % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas HTML data import example\n",
    "\n",
    "Pandas takes a \"batteries included\" approach and throws in a whole lot of convenience functions.  For instance it has import functions for a variety of formats.  One of the pleasant surprises is a command `read_html` that's meant to automate the process of extracting tabular data from HTML.  In particular, it works pretty well with tables on Wikipedia.  \n",
    "\n",
    "Let's do an example: We'll try to extract the list of the world's tallest buildings from\n",
    "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world\n",
    "\n",
    "\n",
    "(This example will, likely, not get lecture time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs = pd.read_html('http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world', header=0, parse_dates=False)\n",
    "\n",
    "# There are several tables on the page.  By inspection we can figure out which one we want\n",
    "tallest = dfs[2]\n",
    "\n",
    "# The coordinates column needs to be fixed up.  This is a bit of string parsing:\n",
    "def clean_lat_long(s):\n",
    "    try:\n",
    "        parts = s.split(\"/\")\n",
    "    except AttributeError:\n",
    "        return (None, None)\n",
    "    if len(parts)<3:\n",
    "        return (None, None)\n",
    "    m=re.search(r\"(\\d+[.]\\d+);[^\\d]*(\\d+[.]\\d+)[^\\d]\", parts[2])\n",
    "    if not m:\n",
    "        return (None, None)\n",
    "    return (m.group(1), m.group(2))\n",
    "\n",
    "tallest['Clean_Coordinates'] = tallest['Coordinates'].apply(clean_lat_long)\n",
    "tallest['Latitude'] = tallest['Clean_Coordinates'].apply(lambda x:x[0])\n",
    "tallest['Longitude'] = tallest['Clean_Coordinates'].apply(lambda x:x[1])\n",
    "\n",
    "# Et voila\n",
    "tallest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tallest.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some remarks\n",
    "Unfortunately, that didn't work well on height (ft) and is so-so on year built.  \n",
    "\n",
    "In both cases, it is because it is assuming that the field is a _date_ rather than just a number.  We can hint these things to `read_html` through extra parameters.\n",
    "\n",
    "In this particular case we would probably not bother: It is pretty good on the year built, and height in meters is good enough.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "1. Parse the height correctly.\n",
    "1. Parse the table rankings of [UK universities available on Wikipedia](https://en.wikipedia.org/wiki/Rankings_of_universities_in_the_United_Kingdom):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Pivot table\" syntax, stacking and unstacking\n",
    "\n",
    "These are useful tools -- and if you're already familiar with Pivot Tables in Excel, for example, they'll be familiar. See http://pandas.pydata.org/pandas-docs/stable/reshaping.html for more information.\n",
    "\n",
    "In general pivot tables are useful when you want to look at aggregations but change what the dependent and independent variables are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a pivot table from a dataframe\n",
    "\n",
    "pt = pd.pivot_table(df, index='State_Code', values=['NUM_ALL', 'NUM_FHA'], aggfunc=np.sum)\n",
    "pt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stack() and unstack() methods are used to manage MultiIndexes, and can control whether you have a row MultiIndex, a column MultiIndex, or neither:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = df[['State_Code', 'County_Code', 'NUM_ALL', 'NUM_FHA']].groupby(['State_Code', 'County_Code']).mean()\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped.unstack().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unstacked = grouped.unstack(level=['County_Code', 'State_Code'])\n",
    "unstacked.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plugging into more advanced analytics\n",
    "\n",
    "Almost any \"advanced analytics\" tool in the Python ecosystem is going to take as input `np.array` type arrays.  You can access the underlying array of a data frame column as\n",
    "\n",
    "        df['column'].values\n",
    "        \n",
    "Many of them take `nd.array` whose underlying data can be accessed by \n",
    "\n",
    "        df.values\n",
    "        \n",
    "directly.  *Most* of the time, they will take `df['column']` and `df` without needing to look at values.\n",
    "\n",
    "This is particularly important if you want to use Pandas with the sklearn library. See this [blog post](http://www.markhneedham.com/blog/2013/11/09/python-making-scikit-learn-and-pandas-play-nice/) for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exit Tickets\n",
    "1. Describe several situations where you would prefer to use Pandas, and several where you'd prefer SQL.\n",
    "1. Write the syntax for slicing the rows of a dataframe according to a filtering criterion.\n",
    "1. Write the syntax for selecting, adding, or removing columns of a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
