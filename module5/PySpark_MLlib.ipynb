{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark MLlib\n",
    "*Official documentation [here](https://spark.apache.org/docs/latest/mllib-guide.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"temp\")\n",
    "print sc.version  # should be >= 1.5.1 for distributed matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed to convert RDDs into DataFrames\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib Data Types (based on RDDs)\n",
    "### Vector\n",
    "A mathematical vector. MLlib supports both dense vectors, where every entry is stored, and sparse vectors, where only the nonzero entries are stored to save space. Vectors can be constructed with the mllib.linalg.Vectors package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,[0,2],[1.0,3.0])\n",
      "(3,[0,2],[1.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors, SparseVector\n",
    "\n",
    "# Create a dense vector (1.0, 0.0, 3.0) from a NumPy array.\n",
    "dv1 = np.array([1.0, 0.0, 3.0])\n",
    "\n",
    "# Create a dense vector (1.0, 0.0, 3.0) from a Python list.\n",
    "dv2 = [1.0, 0.0, 3.0]\n",
    "\n",
    "# Create a SparseVector (1.0, 0.0, 3.0) by specifying its indices and values corresponding to nonzero entries.\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n",
    "\n",
    "# Create a sparse vector (1.0, 0.0, 3.0) by specifying its nonzero entries.\n",
    "sv2 = Vectors.sparse(3, [(0, 1.0), (2, 3.0)])\n",
    "\n",
    "print sv1\n",
    "print sv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabeledPoint\n",
    "A labeled data point for supervised learning algorithms such as classification and regression. Includes a feature vector and a label (which is a floating-point value). Located in the mllib.regression package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 -1.0\n",
      "========\n",
      "[1.0,0.0,3.0] (3,[0,2],[1.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "point1 = LabeledPoint(1.0, np.array([1.0, 0.0, 3.0]))\n",
    "\n",
    "point2 = LabeledPoint(-1.0, SparseVector(3, [0, 2], [1.0, 3.0]))\n",
    "\n",
    "print point1.label, point2.label\n",
    "print \"========\"\n",
    "print point1.features, point2.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local matrix\n",
    "*Integer*-typed row and column indices and double-typed values, stored on a single machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed matrix \n",
    "*Long*-typed row and column indices and double-typed values, stored distributively in one or more RDDs.  They come in three formats:\n",
    "\n",
    "- **RowMatrix:** Row-oriented distributed matrix without meaningful row indices, e.g. a collection of feature vectors. It is backed by an RDD of its rows, where each row is a local vector. We assume that the number of columns is not huge for a RowMatrix so that a single local vector can be reasonably communicated to the driver and can also be stored / operated on using a single node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 3\n",
      "cols: 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "# Create an RDD of vectors.\n",
    "rows = sc.parallelize([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n",
    "\n",
    "# Create a RowMatrix from an RDD of vectors.\n",
    "rowMatrix = RowMatrix(rows)\n",
    "\n",
    "# return size\n",
    "m = rowMatrix.numRows()\n",
    "n = rowMatrix.numCols() \n",
    "\n",
    "print \"rows: %d\\ncols: %d\" % (m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **IndexedRowMatrix:** Similar to a RowMatrix but with meaningful row indices. It is backed by an RDD of indexed rows, so that each row is represented by its index (long-typed) and a local vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 4\n",
      "cols: 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "\n",
    "indexedRows = sc.parallelize([IndexedRow(0, [1.0, 2.0]),\n",
    "                              IndexedRow(1, [2.0, 3.0]), \n",
    "                              IndexedRow(3, [4.0, 5.0])])\n",
    "\n",
    "indexedRowMatrix = IndexedRowMatrix(indexedRows)\n",
    "\n",
    "# return size\n",
    "m = indexedRowMatrix.numRows()\n",
    "n = indexedRowMatrix.numCols() \n",
    "\n",
    "print \"rows: %d\\ncols: %d\" % (m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **CoordinateMatrix:** (i.e. a distributed sparse matrix) is (essentially) a list of `(Long, Long, Double)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 3\n",
      "cols: 3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "\n",
    "matrixEntries = sc.parallelize([MatrixEntry(0, 0, 1.),\n",
    "                                MatrixEntry(1, 1, 1.),\n",
    "                                MatrixEntry(2, 2, 1.)])\n",
    "\n",
    "coordinateMatrix = CoordinateMatrix(matrixEntries)\n",
    "\n",
    "# return size\n",
    "m = coordinateMatrix.numRows()\n",
    "n = coordinateMatrix.numCols() \n",
    "\n",
    "print \"rows: %d\\ncols: %d\" % (m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **BlockMatrix:** A distributed matrix backed by an RDD of MatrixBlocks, where a MatrixBlock is a tuple of ((Int, Int), Matrix), where the (Int, Int) is the index of the block, and Matrix is the sub-matrix at the given index with size rowsPerBlock x colsPerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "\n",
    "BlockMatrix = coordinateMatrix.toBlockMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, because the matrix is stored in a distributed way, converting between matrix formats is expensive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning in MLlib\n",
    "\n",
    "Spark supports a number of machine-learning algorithms.\n",
    "\n",
    "- Classification and Regression\n",
    "    - SVM, linear regression\n",
    "    - SVR, logistic regression\n",
    "    - Naive Bayes\n",
    "    - Decision Trees\n",
    "    - Random Forests and Gradient-Boosted Trees\n",
    "- Clustering\n",
    "    - K-means (and streaming K-means)\n",
    "    - Gaussian Mixture Models\n",
    "    - Latent Dirichlet Allocation\n",
    "- Dimensionality Reduction\n",
    "    - SVD and PCA\n",
    "- It also has support for lower-level optimization primitives:\n",
    "    - Stochastic Gradient Descent\n",
    "    - Low-memory BFGS and L-BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelized SGD\n",
    "\n",
    "For linear models like SVM, Linear Regression, and Logistic Regression, the cost function we're trying to optimize is essentially an average over the individual error term from each data point. This is particularly great for parallelization.  For example, in linear regression, recall that the gradient is\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\log(L(\\beta))}{\\partial \\beta} &= \\frac{\\partial}{\\partial \\beta} \\frac{1}{2}\\sum_j \\|y_j - X_{j \\cdot} \\cdot \\beta\\| \\\\\n",
    "&= \\frac{1}{2}\\sum_j \\frac{\\partial}{\\partial \\beta} \\|y_j - X_{j \\cdot} \\cdot \\beta\\| \\\\\n",
    "& = \\sum_j y_j - X_{j \\cdot} \\cdot \\beta \\\\\n",
    "& \\approx \\sum_{sample \\mbox{ } j} y_j - X_{j \\cdot} \\cdot \\beta\n",
    "\\end{align}$$\n",
    "\n",
    "The key *mathematical properties* we have used are:\n",
    "\n",
    "1. the error functions are the sum of error contributions of different training instances\n",
    "1. linearity of the derivative\n",
    "1. associativity of addition\n",
    "1. downsampling giving an unbiased estimator\n",
    "\n",
    "Since the last sum is over the different training instances and these are stored on different nodes, we can parallelize the computation of the gradient in SGD across multiple nodes.  Of course, we still need to maintain the running weight $\\beta$ that has to be present on every node (through a broadcast variable that is updated).  Notice that SVM, Linear Regression, and Logistic Regression all have error functions that are just sums over training instances so SGD can be used for all these algorithms.\n",
    "\n",
    "Spark's [implementation](http://spark.apache.org/docs/latest/mllib-optimization.html#stochastic-gradient-descent-sgd) uses a tunable minibatch size parameter to sample a percentage of the features RDD. For each iteration, the updated weights are broadcast to the executors, and the update is calculated for each data point and sent back to be aggregated.\n",
    "\n",
    "Parallelization handles increasing number of sampled data points m quite well since there are no interaction terms and each calculation is independent. Controlling how the algorithm iterates to convergence is also important, and can be done with parameters for the total iterations and step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.251058287609\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD, LinearRegressionModel\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "import random\n",
    "\n",
    "# parameters\n",
    "TRAINING_ITERATIONS = 10\n",
    "TRAINING_FRACTION = 0.6\n",
    "\n",
    "# generate the data\n",
    "data = sc.parallelize(xrange(1,10001)) \\\n",
    "    .map(lambda x: LabeledPoint(random.random(), [random.random(), random.random(), random.random()]))\n",
    "\n",
    "# split the training and test sets\n",
    "splits = data.randomSplit([TRAINING_FRACTION, 1 - TRAINING_FRACTION], seed=42)\n",
    "training, test = (splits[0].cache(), splits[1])\n",
    "\n",
    "# train the model\n",
    "model = LinearRegressionWithSGD.train(training, TRAINING_ITERATIONS)\n",
    "\n",
    "# get r2 score\n",
    "predictions = test.map(lambda x: (float(model.predict(x.features)), x.label))\n",
    "print RegressionMetrics(predictions).r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML\n",
    "Spark ML implements the ideas of transformers, estimators, and pipelines by standardizing APIS across machine learning algorithms. This can streamline more complex workflows.\n",
    "\n",
    "The core functionality includes:\n",
    "* DataFrames - built off Spark SQL, can be created either directly or from RDDs as seen above\n",
    "* Transformers - algorithms that accept a DataFrame as input and return a DataFrame as output\n",
    "* Estimators - algorithms that accept a DataFrame as input and return a Transformer as output\n",
    "* Pipelines - chaining together Transformers and Estimators\n",
    "* Parameters - common API for specifying hyperparameters\n",
    "\n",
    "For example, a learning algorithm can be implemented as an Estimator which trains on a DataFrame of features and returns a Transformer which can output predictions based on a test DataFrame.\n",
    "\n",
    "Full documentation can be found [here](http://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames and Spark SQL\n",
    "\n",
    "Spark SQL is the current effort to provide support for writing SQL queries in Spark. Version 1.6.0 supports Hive, Parquet, and other data sources. [Docs](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "The key feature of Spark SQL is the use of DataFrames instead of RDDs. A DataFrame is a distributed collection of data organized into named columns, and operations on DataFrames are first parsed through an optimized execution engine which streamlines and may even reorder the request to optimize execution. The keyword to search here is Catalyst.\n",
    "\n",
    "Under the hood, operations on DataFrames are boiled down to operations on RDDs, but the RDDs are created by the execution engine, and not directly by the user. It is also possible to convert RDDs to DataFrames and vice versa.\n",
    "\n",
    "The Spark ML package, unlike MLlib, uses DataFrames as inputs and outputs.\n",
    "\n",
    "**Question:** What is an example of a \"bad\" sequence of operations which should be reordered for optimal performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(title=u'A decent guided tour of Spark and its major components.', label=0.0, prediction=1.0)\n",
      "Row(title=u'10/10 would buy again', label=1.0, prediction=1.0)\n",
      "Row(title=u'it is simple to follow. well organized. straight ...', label=1.0, prediction=1.0)\n",
      "Row(title=u'Just what you need to get started in Apache Spark.', label=1.0, prediction=1.0)\n",
      "Row(title=u'Very good book for learning Spark', label=1.0, prediction=1.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "reviews = [(\"Prose is well-written, but style is an impediment to learning. Should be called 'Reviewing Spark,' not 'Learning Spark'\", 0.0),\n",
    "            (\"Nice Headstart to Spark\", 1.0),\n",
    "            (\"Start here: Excellent reference for Spark\", 1.0),\n",
    "            (\"Insightful and so Spark-tastic!\", 1.0),\n",
    "            (\"Good intro but wordy and lacking details in areas\", 0.0),\n",
    "            (\"Best of the Books Currently Available\", 1.0),\n",
    "            (\"A good resource for people interested in learning Spark\", 1.0),\n",
    "            (\"Great Overview\", 1.0)]\n",
    "\n",
    "test_reviews = [(\"A decent guided tour of Spark and its major components.\", 0.0),\n",
    "                (\"10/10 would buy again\", 1.0),\n",
    "                (\"it is simple to follow. well organized. straight ...\", 1.0),\n",
    "                (\"Just what you need to get started in Apache Spark.\", 1.0),\n",
    "                (\"Very good book for learning Spark\", 1.0)]\n",
    "\n",
    "training = sqlContext.createDataFrame(reviews, [\"title\", \"label\"])\n",
    "test = sqlContext.createDataFrame(test_reviews, [\"title\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and logistic regression.\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "logreg = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, logreg])\n",
    "\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Make predictions on test documents\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"title\", \"label\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               title|label|\n",
      "+--------------------+-----+\n",
      "|Prose is well-wri...|  0.0|\n",
      "|Nice Headstart to...|  1.0|\n",
      "|Start here: Excel...|  1.0|\n",
      "|Insightful and so...|  1.0|\n",
      "|Good intro but wo...|  0.0|\n",
      "|Best of the Books...|  1.0|\n",
      "|A good resource f...|  1.0|\n",
      "|      Great Overview|  1.0|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|               title|label|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|A decent guided t...|  0.0|[a, decent, guide...|(262144,[97,3543,...|[-4.7714032977492...|[0.00839737493866...|       1.0|\n",
      "|10/10 would buy a...|  1.0|[10/10, would, bu...|(262144,[67599,75...|[-3.1142959220057...|[0.04252139879146...|       1.0|\n",
      "|it is simple to f...|  1.0|[it, is, simple, ...|(262144,[3370,337...|[-3.0736180319128...|[0.04420869786925...|       1.0|\n",
      "|Just what you nee...|  1.0|[just, what, you,...|(262144,[3365,370...|[-2.9512720957303...|[0.04967642318936...|       1.0|\n",
      "|Very good book fo...|  1.0|[very, good, book...|(262144,[29470,32...|[-4.5818316581120...|[0.01013241332411...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [title#7,label#8], MapPartitionsRDD[95] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:-2\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "title: string, label: double\n",
      "LogicalRDD [title#7,label#8], MapPartitionsRDD[95] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:-2\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [title#7,label#8], MapPartitionsRDD[95] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:-2\n",
      "\n",
      "== Physical Plan ==\n",
      "Scan PhysicalRDD[title#7,label#8]\n",
      "\n",
      "Code Generation: true\n"
     ]
    }
   ],
   "source": [
    "training.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|Prose is well-wri...|\n",
      "|Good intro but wo...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.filter(training[\"label\"] == 0).select(\"title\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               title|total|\n",
      "+--------------------+-----+\n",
      "|Insightful and so...|    1|\n",
      "|Good intro but wo...|    1|\n",
      "|A good resource f...|    1|\n",
      "|Prose is well-wri...|    1|\n",
      "|Start here: Excel...|    1|\n",
      "|Best of the Books...|    1|\n",
      "|Nice Headstart to...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Requires Hive to permanently store tables\n",
    "training.select(\"title\").registerTempTable('text')  # This is NOT the same as a temp table in SQL proper\n",
    "df = sqlContext.sql(\"select title, count(*) as total from text group by title having length(title) > 15\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('length('title) > 15)\n",
      " 'Aggregate ['title], [unresolvedalias('title),unresolvedalias(count(1) AS total#21L)]\n",
      "  'UnresolvedRelation [text], None\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "title: string, total: bigint\n",
      "Filter (length(title#7) > 15)\n",
      " Aggregate [title#7], [title#7,count(1) AS total#21L]\n",
      "  Subquery text\n",
      "   Project [title#7]\n",
      "    LogicalRDD [title#7,label#8], MapPartitionsRDD[95] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:-2\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (length(title#7) > 15)\n",
      " Aggregate [title#7], [title#7,count(1) AS total#21L]\n",
      "  Project [title#7]\n",
      "   LogicalRDD [title#7,label#8], MapPartitionsRDD[95] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:-2\n",
      "\n",
      "== Physical Plan ==\n",
      "Filter (length(title#7) > 15)\n",
      " TungstenAggregate(key=[title#7], functions=[(count(1),mode=Final,isDistinct=false)], output=[title#7,total#21L])\n",
      "  TungstenExchange hashpartitioning(title#7)\n",
      "   TungstenAggregate(key=[title#7], functions=[(count(1),mode=Partial,isDistinct=false)], output=[title#7,currentCount#29L])\n",
      "    TungstenProject [title#7]\n",
      "     Scan PhysicalRDD[title#7,label#8]\n",
      "\n",
      "Code Generation: true\n"
     ]
    }
   ],
   "source": [
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tungsten\n",
    "\n",
    "* Memory management and GC (better than the JVM)\n",
    "* Cache-aware computation\n",
    "* Codegen (compile queries into Java bytecode)\n",
    "\n",
    "Cache-aware computation example:\n",
    "Case 1: pointer -> key, value\n",
    "Case 2: ke, pointer -> key, value\n",
    "Where to find keys for sort purposes?\n",
    "\n",
    "[More](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)\n",
    "\n",
    "## DataFrame performance and tuning\n",
    "\n",
    "See [here](http://spark.apache.org/docs/latest/sql-programming-guide.html#performance-tuning) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example algorithm: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def localpath(path):\n",
    "    return 'file://' + str(os.path.abspath(os.path.curdir)) + '/' + path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# text = sc.parallelize(reviews + test_reviews).map(lambda (line, score): (line.split(\" \"), score)).toDF(['text', 'score'])\n",
    "gutenberg = sc.textFile(localpath('small_data/gutenberg/')).map(lambda line: (line.split(\" \"), 1)).toDF(['text', 'score'])\n",
    "w2v = Word2Vec(inputCol=\"text\", outputCol=\"vectors\")\n",
    "model = w2v.fit(gutenberg)\n",
    "result = model.transform(gutenberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors = model.getVectors().rdd.map(lambda x: (x.word, x.vector))\n",
    "#print model.findSynonyms('woman', 10).rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "king_vec = vectors.lookup('king')[0]\n",
    "queen_vec = vectors.lookup('queen')[0]\n",
    "man_vec = vectors.lookup('man')[0]\n",
    "woman_vec = vectors.lookup('woman')[0]\n",
    "fun_vec = vectors.lookup('fun')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00365848164074,-0.0065196454525,-0.0804284065962,0.152335777879,0.0326004549861,-0.019918911159,-0.0436401516199,0.0555587783456,-0.0210002548993,-0.0436907336116,0.170062333345,0.0474225617945,0.0547049194574,0.0117957973853,-0.0102078747004,0.0845418348908,0.00444008409977,-0.00810476671904,-0.0137997018173,-0.00498426239938,0.090994335711,-0.0399200767279,-0.0091584995389,-0.0305552836508,0.0126363532618,0.0905004069209,-0.000924758322071,0.067603982985,0.061579503119,-0.120177254081,-0.0262913368642,-0.00785322673619,0.0950697362423,-0.0419631488621,-0.0350100472569,0.076530598104,-0.0146682085469,0.00591638032347,0.192819431424,0.0727502256632,0.0341582782567,-0.0685464963317,-0.0473059006035,0.0641125813127,-0.0584225617349,0.0779399499297,-0.0152564421296,0.0353797115386,0.0428476147354,-0.0386568158865,-0.0168507024646,0.0676571428776,0.00822687707841,0.0100878244266,-0.0634497925639,0.0724031105638,-0.0357816703618,0.0267206374556,0.0137565480545,0.0707818940282,0.000108062871732,0.0136248767376,-0.0184704139829,-0.0116509962827,0.0397662520409,0.0538813397288,0.0453904867172,-0.131426542997,-0.0586736649275,0.0861864835024,-0.00987862795591,-0.00962557736784,0.00581086194143,0.0119223799556,0.0930415987968,0.0438240990043,-0.0414282679558,-0.0390815809369,0.0271556284279,0.0908843651414,0.0283787380904,-0.0209336411208,-0.0447009280324,0.00650458456948,0.0201279576868,0.0645348876715,-0.0983925536275,-0.0178143810481,-0.118114970624,0.0247344393283,0.0788134559989,0.0516433082521,0.0603202730417,0.100357286632,-0.0405418612063,-0.0231550484896,-0.192146271467,0.100561395288,-0.0663677677512,0.0906515344977]\n"
     ]
    }
   ],
   "source": [
    "print fun_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0937515497208,0.0183246023953,0.000950152869336,0.0185837522149,0.0713655352592,-0.0063332375139,-0.0454377606511,0.0473997183144,-0.0285515654832,-0.055686134845,0.127984464169,-0.0252038557082,0.0617561638355,0.000601839739829,-0.0069263051264,0.118331976235,0.0913769304752,0.0498140752316,0.0473664104939,0.0187189653516,0.17080681026,-0.0446416139603,-0.0121530350298,0.0164760164917,0.0329034179449,0.0415324233472,0.0440274849534,0.10448115319,0.115042738616,-0.111048586667,-0.0205034073442,0.0550046488643,0.00128966965713,-0.091121353209,-0.035124886781,0.111250318587,-0.0404968075454,-0.0558430813253,0.0932889133692,-0.0199452191591,-0.0024047892075,-0.0284786876291,-0.0481160841882,0.0962753891945,-0.0534815043211,0.106240496039,0.0274849459529,-0.0146097261459,-0.0200682729483,-0.0649460628629,-0.0845576226711,-0.0514568053186,0.0300664678216,0.0407979972661,-0.104042924941,0.0585550256073,0.0601285584271,-0.0118421986699,-0.0593617074192,0.109998866916,0.11498131603,0.0689779147506,0.0346549674869,0.0676555037498,0.0486168824136,0.120160661638,0.0175206754357,0.000410869542975,0.0449951775372,-0.0311304032803,-0.0168559737504,-0.0168079268187,0.0394051969051,-0.0113252401352,-0.00633141910657,0.00622987840325,0.0286322683096,-0.00805493816733,0.0491929166019,0.0647815987468,0.022898459807,-0.00882508140057,-0.0779932662845,0.0295406710356,0.109998703003,0.093480437994,-0.0338569693267,0.0708979666233,-0.0270495116711,0.00629139551893,0.0769621878862,-0.0102854967117,-0.0178204476833,0.0490526482463,-0.00224206596613,-0.027467507869,-0.133942052722,0.00591441849247,-0.0370589531958,-0.0148874027655]\n"
     ]
    }
   ],
   "source": [
    "print king_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.229802003101\n",
      "1.60639476246\n",
      "2.2375540988\n",
      "2.65991495515\n",
      "2.11733516905\n",
      "1.63887484289\n",
      "1.0481737307\n"
     ]
    }
   ],
   "source": [
    "print queen_vec.squared_distance(king_vec)\n",
    "print queen_vec.squared_distance(woman_vec)\n",
    "print queen_vec.squared_distance(man_vec)\n",
    "print queen_vec.squared_distance(king_vec + man_vec - woman_vec)\n",
    "print queen_vec.squared_distance(king_vec - man_vec + woman_vec)\n",
    "print fun_vec.squared_distance(man_vec + king_vec)\n",
    "print fun_vec.squared_distance(woman_vec )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Use SVM to predict colon cancer from gene expressions\n",
    "You can start getting a feel for the MLlib operations by following the [Spark docs example](https://spark.apache.org/docs/1.3.0/mllib-linear-methods.html#linear-support-vector-machines-svms) on this dataset.\n",
    "\n",
    "#### About the data format: LibSVM\n",
    "MLlib conveniently provides a data loading method, `MLUtils.loadLibSVMFile()`, for the LibSVM format for which many other languages (R, Matlab, etc.) also have loading methods.  \n",
    "A dataset of *n* features will have one row per datum, with the label and values of each feature organized as follows:\n",
    ">{label} 1:{value} 2:{value} ... n:{value}\n",
    "\n",
    "Take these two datapoints with six features and labels of -1 and 1 respectively as an example:\n",
    ">-1.000000  1:2.080750 2:1.099070 3:0.927763 4:1.029080 5:-0.130763 6:1.265460  \n",
    "1.000000  1:1.109460 2:0.786453 3:0.445560 4:-0.146323 5:-0.996316 6:0.555759 \n",
    "\n",
    "#### About the colon-cancer dataset\n",
    "This dataset was introduced in the 1999 paper \"Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays.\" (Available on PNAS)\n",
    "\n",
    "Here's the abstract of the paper:  \n",
    "> *Oligonucleotide arrays can provide a broad picture of the state of the cell, by monitoring the expression level of thousands of genes at the same time. It is of interest to develop techniques for extracting useful information from the resulting data sets. Here we report the application of a two-way clustering method for analyzing a data set consisting of the expression patterns of different cell types. Gene expression in 40 tumor and 22 normal colon tissue samples was analyzed with an Affymetrix oligonucleotide array complementary to more than 6,500 human genes. An efficient two-way clustering algorithm was applied to both the genes and the tissues, revealing broad coherent patterns that suggest a high degree of organization underlying gene expression in these tissues. Coregulated families of genes clustered together, as demonstrated for the ribosomal proteins. Clustering also separated cancerous from noncancerous tissue and cell lines from in vivo tissues on the basis of subtle distributed patterns of genes even when expression of individual genes varied only slightly between the tissues. Two-way clustering thus may be of use both in classifying genes into functional groups and in classifying tissues based on gene expression.*\n",
    "\n",
    "There are 2000 features, 62 data points (40 tumor (label=0), 22 normal (label=1)), and 2 classes (labels) for the colon cancer dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exit Tickets\n",
    "1. When would you use `org.apache.spark.mllib.linalg.Vector` versus `breeze.linalg.DenseVector`?\n",
    "1. Why can SVM, Linear Regression, and Logistic Regression be parallelized?  How would you parallelize KMeans?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
