{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Technologies \n",
    "Streaming is useful for realtime analytics, online machine learning, continuous computation, and more.\n",
    "\n",
    "## Apache Kafka\n",
    "*A distributed, partitioned, replicated commit log service.*     \n",
    "\n",
    "Data streams are partitioned and spread over a cluster of machines to allow data streams larger than the capability of any single machine and to allow clusters of coordinated consumers\n",
    "![kafka](images/kafka_producer_consumer.png)\n",
    "\n",
    "- Kafka maintains feeds of messages in categories called topics.\n",
    "- Processes that publish messages to a Kafka topic: *producers.*\n",
    "- Processes that subscribe to topics and process the feed of published messages: *consumers*\n",
    "- Kafka is run as a cluster comprised of one or more servers, each of which is called a *broker.*\n",
    "- Java client for communication between clients and servers is provided natively; other languages are available.\n",
    "\n",
    "#### Useful for:\n",
    "- Website activity tracking\n",
    "- Metrics in operational monitoring, i.e. aggregating stats from distributed applications\n",
    "- Log aggregation, collecting the physical log files off servers and putting them in a central place (e.g. HDFS) for processing\n",
    "- Stream processing\n",
    "\n",
    "## Apache Storm\n",
    "*\"A free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing.\"*\n",
    "\n",
    "- Fast: can process 1 million+ tuples per second per node!\n",
    "- Scalable and fault-tolerant\n",
    "- Easy to set up and operate\n",
    "\n",
    "#### Abstractions \n",
    "- **Core abstraction:** The *stream*, an unbounded sequence of tuples. Storm provides the primitives for transforming a stream into a new stream in a distributed and reliable way. \n",
    "- **Spout:** a source of streams, e.g. a connection to the Twitter API\n",
    "- **Bolt:** consumes any number of input streams and runs processing logic; possibly emits new streams. \n",
    "    - E.g. run functions, filter, aggregate streams, join streams, talk to databases, etc.\n",
    "- Networks of spouts and bolts are packaged into a **topology**, which is the top level abstraction that you submit to Storm clusters for execution. \n",
    "![storm](images/storm_topology.png)\n",
    "\n",
    "- Each node in the topology executes in parallel (you can specify the level of parallelism for each node) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming\n",
    "\n",
    "### Core abstraction:\n",
    "The `DStream`, aka discretized stream - a sequence of data arriving over time.\n",
    "- Internally a `DStream[T]` is represented as a sequence of `RDD[T]`s representing the elements `T` that arrive at each time step\n",
    "- A `DStream` can be created from various input sources, e.g. Kafka, HDFS\n",
    "- A `DStream` supports two types of operations: \n",
    "    - *Transformations* yield a new DStream. Many of the same operations available on RDDS, in addition to operations related to time e.g. sliding windows\n",
    "    - *Output operations* write data to an external system \n",
    "    \n",
    "Many functions abstract over the `RDD` layer.  The function\n",
    "\n",
    "- `flatMap`\n",
    "- `map`\n",
    "- `filter`\n",
    "- `reduce`\n",
    "- `union`\n",
    "\n",
    "all operate directly on elements `T`.  Other functions, most notably \n",
    "\n",
    "- `foreachRDD`\n",
    "- `transform`\n",
    "\n",
    "depend explicitly on the `RDD` level.  Remember that while the elements of the `DStream` are separted by time (some happen earlier than others), the elements of individual `RDD`s are separated by \"distance\" (they are potentially on separate nodes).  Finally, there are operations that aggregate in time, notably\n",
    "\n",
    "- `slice`: (selects a \"small\" subset of the `DStream` by time)\n",
    "- `window`: creates a new `DStream` generated by sliding windows of the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Spark Streaming application\n",
    "#### Imports\n",
    "The most basic spark applications need to import this code:\n",
    "```scala\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "```\n",
    "\n",
    "#### Creating a spark context\n",
    "Inside our spark app, the first step is to setup a new `StreamingContext`.  This functions much like a usual `SparkContext` but takes an additional `duration` argument which sets how often batches are generated.  In the example below, we are aiming for a second.  We also reduce the warning level to be less loquacious so we can actually see the output.\n",
    "\n",
    "```scala\n",
    "val sparkConf = new SparkConf().setAppName(\"QueueStream\")\n",
    "val ssc = new StreamingContext(sparkConf, Seconds(1))\n",
    "\n",
    "// Set warning level to not get as many qarnings\n",
    "Logger.getRootLogger.setLevel(Level.WARN)\n",
    "```\n",
    "\n",
    "#### Extra build dependencies: `build.sbt`:\n",
    "```scala\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-streaming\" % \"1.3.0\" % \"provided\"\n",
    "```\n",
    "\n",
    "#### Building\n",
    "Build the project using this bash command from the command prompt\n",
    "\n",
    "```bash\n",
    "sbt assembly  # or `sbt clean assembly`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example\n",
    "\n",
    "and run it using\n",
    "\n",
    "```bash\n",
    "spark-submit target/scala-*/streaming-example-assembly-*.jar -p Netcount --host localhost --port 9999\n",
    "```\n",
    "\n",
    "to start this spark app listening on port 9999.  Then in another terminal, start\n",
    "\n",
    "```bash\n",
    "nc -lk 9999\n",
    "```\n",
    "\n",
    "and type in lines to have them broadcasted on port 9999.  You should see the word counts show up in the output of our spark program.\n",
    "\n",
    "### How it works\n",
    "\n",
    "Inside your spark app, the first step is to setup a new `StreamingContext`.  This functions much like a usual `SparkContext` but takes an additional `duration` argument which sets how often batches are generated.  In the example below, we are aiming for a second.  We also reduce the warning level to be less loquacious so we can actually see the output.\n",
    "\n",
    "```scala\n",
    "    val sparkConf = new SparkConf().setAppName(\"QueueStream\")\n",
    "    val ssc = new StreamingContext(sparkConf, Seconds(1))\n",
    "    \n",
    "    // Set warning level to not get as many qarnings\n",
    "    Logger.getRootLogger.setLevel(Level.WARN)\n",
    "```\n",
    "\n",
    "Next, we create a socket text `DStream` (a `DStream` whose data comes from the url and port numbers specified below).  We run a standard wordcount on them and print the results, line-by-line.\n",
    "\n",
    "```scala\n",
    "// Create the Socket Text Stream and use it do some processing\n",
    "val wordCounts = ssc.socketTextStream(args(0), args(1).toInt)\n",
    "                      .flatMap(_.split(\" \"))\n",
    "                      .map(x => (x, 1))\n",
    "                      .reduceByKey(_ + _)\n",
    "\n",
    "wordCounts.print()\n",
    "```\n",
    "\n",
    "All the operations above were declarative.  We need to start the computation and wait until the execution stops or an exception is called.\n",
    "\n",
    "```scala\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "```\n",
    "\n",
    "You can find access to the code at [projects/streaming-example/src/main/scala/streamingexample/Netcount.scala](projects/streaming-example/src/main/scala/streamingexample/Netcount.scala)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping track of state\n",
    "\n",
    "We do a more complex example which keeps track of state in between operations.  In this project, we analyze the word frequency of Hamlet's famous soliloquy.  In the main function, the setup requires a checkpoint directory where our intermediate states are persisted for safe keeping:\n",
    "\n",
    "```scala\n",
    "ssc.checkpoint(\"/tmp/\")   // set checkpoint\n",
    "```\n",
    "\n",
    "Next, we create a queue populated with lines of shakespeare to be streamed:\n",
    "```scala\n",
    "val rddQueue = Queue[RDD[String]](shakespeare\n",
    "  .map(line => ssc.sparkContext.makeRDD(Seq(line))): _*\n",
    ")\n",
    "\n",
    "val wordStream = ssc.queueStream(rddQueue, oneAtATime=true)\n",
    "                      .flatMap(_.split(\" \"))\n",
    "                      .map( x => (x, 1))\n",
    "                      .reduceByKey(_ + _)\n",
    "```\n",
    "\n",
    "Finally, we need update our state (by key).  Firstly, let's specify the update function:\n",
    "\n",
    "```scala\n",
    "def updateFunc(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = {\n",
    "Some(runningCount.getOrElse(0) + newValues.sum)\n",
    "}\n",
    "```\n",
    "\n",
    "and then build it within the main function:\n",
    "\n",
    "```scala\n",
    "val stateDstream = wordStream.updateStateByKey[Int](updateFunc _)\n",
    "```\n",
    "\n",
    "(For each key) `updateFunc` takes the `runningCount` and updates it wit hthe sum of counts coming in.  Notice that what `updateStateByKey` does across time, `reduceByKey` does across an `RDD`: propagating a summary state.\n",
    "\n",
    "You can find a copy of the source code at \n",
    "[projects/streaming-example/src/main/scala/streamingexample/Shakespeare.scala](projects/streaming-example/src/main/scala/streamingexample/Shakespeare.scala), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window state\n",
    "\n",
    "Finally, we can increase the \"duration\" of the \"window\" over which `DStream`'s `RDD`s data read from.  Remember that each `DStream` is actually a stream of `RDD`s.  In our examples thus far, the windows arrive secondly and represent the data in the last second.  Both of those can be retweaked.\n",
    "\n",
    "![image](images/streaming-dstream-window.png)\n",
    "\n",
    "For example, in the above diagram, we have created a new `DStream` based on the previous `DStream` but where each window represents the last 3 (units of time) and this occurs every 2 (units of time).  That is, we have created a window where\n",
    "- *window length* -- duration or the window (3 in this example)\n",
    "- *sliding interval* -- interval at which the window is outputted (2 in this example).\n",
    "\n",
    "### An example\n",
    "In our example, we first create an original stream from a queue.  Each element of the stream contains an `RDD` holding a single integer.\n",
    "\n",
    "```scala\n",
    "val rddQueue = Queue[RDD[Int]]((1 to 100)\n",
    "  .map( i => ssc.sparkContext.makeRDD(Seq(i))): _*\n",
    ")\n",
    "\n",
    "val original = ssc.queueStream(rddQueue, oneAtATime=true).map(_.toDouble)\n",
    "```\n",
    "\n",
    "We create two streams from this original stream, the first one just reflects the current `RDD`, the second one reflects the second one reflects the sum of the last two RDDs.\n",
    "```scala\n",
    "val value = original.map(x => (\"value\" -> x))\n",
    "\n",
    "val lastTwoSum = original.window(Seconds(2))\n",
    "  .transform( rdd => ssc.sparkContext.makeRDD(Seq(rdd.sum)) )\n",
    "  .map(x => (\"lastTwoSum\" -> x))\n",
    "```\n",
    "\n",
    "Finally, we join them using a union and print those values to the screne.\n",
    "```scala\n",
    "val output = ssc.union(Seq(value, lastTwoSum)).print()\n",
    "```\n",
    "\n",
    "You'll notice that in the new stream, we see the values for boht \"value\" and \"lastTwoSum\" and they are as promised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming tweets demo\n",
    "\n",
    "The project lives here: `datacourse/projects/streaming_twitter_cl`\n",
    "\n",
    "#### Collecting tweets with built-in Twitter utilities\n",
    "```scala\n",
    " val tweetStream = TwitterUtils.createStream(ssc, Utils.getAuth)\n",
    "  .map(gson.toJson(_))\n",
    "\n",
    "tweetStream.foreachRDD((rdd, time) => {\n",
    "  val count = rdd.count()\n",
    "  if (count > 0) {\n",
    "    val outputRDD = rdd.repartition(partitionsEachInterval)\n",
    "    outputRDD.saveAsTextFile(\n",
    "      outputDirectory + \"/tweets_\" + time.milliseconds.toString)\n",
    "    numTweetsCollected += count\n",
    "    if (numTweetsCollected > numTweetsToCollect) {\n",
    "      System.exit(0)\n",
    "    }\n",
    "  }\n",
    "})\n",
    "```\n",
    "\n",
    "#### Examining the Tweets and Training a K-means Clustering Model\n",
    "\n",
    "The example uses SparkSQL to examine the data based on the tweets. SparkSQL can load JSON files and infer the schema based on the data. The commands you pass into SparkSQL to bring back to stdout will follow pretty standard SQL syntax.\n",
    "\n",
    "```scala\n",
    "sqlContext.sql(<command>).collect().foreach(println)\n",
    "```\n",
    "\n",
    "This clustering aims to identify clusters of tweets written in the same language. We do so by vectorizing hashed bigrams of characters within each tweet to recognize common sequences of characters in languages. Here `tf` is a `HashingTF` from `mllib.feature.HashingTF`\n",
    "\n",
    "```scala\n",
    "  def featurize(s: String): Vector = {\n",
    "    tf.transform(s.sliding(2).toSeq)\n",
    "  }\n",
    "```\n",
    "\n",
    "And here we train a KMeans model from MLlib:\n",
    "```scala\n",
    "val vectors = texts.map(Utils.featurize).cache()\n",
    "val model = KMeans.train(vectors, numClusters, numIterations)\n",
    "sc.makeRDD(model.clusterCenters, numClusters).\n",
    "    saveAsObjectFile(outputModelDir)\n",
    "```\n",
    "\n",
    "#### Realtime Classification\n",
    "\n",
    "Finally, let's apply the model we've created in realtime! We'll:\n",
    "1. load a Spark Streaming Context\n",
    "1. create a Twitter DStream and grab just their `text` field\n",
    "1. load the trained KMeans model\n",
    "1. Choose the id of a language cluster we're interested in, and apply the model to all tweets, filtering only on that specificed cluster to see if the printed output is what we expect. \n",
    "\n",
    "```scala\n",
    "println(\"Initializing Streaming Spark Context...\")\n",
    "val conf = new SparkConf().setAppName(this.getClass.getSimpleName)\n",
    "val ssc = new StreamingContext(conf, Seconds(5))\n",
    "\n",
    "println(\"Initializing Twitter stream...\")\n",
    "val tweets = TwitterUtils.createStream(ssc, Utils.getAuth)\n",
    "val statuses = tweets.map(_.getText)\n",
    "\n",
    "println(\"Initializing the the KMeans model...\")\n",
    "val model = new KMeansModel(ssc.sparkContext.objectFile[Vector](\n",
    "    modelFile.toString).collect())\n",
    "\n",
    "val filteredTweets = statuses\n",
    "  .filter(t => model.predict(Utils.featurize(t)) == clusterNumber)\n",
    "filteredTweets.print()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An exercise for the reader\n",
    "\n",
    "The first streaming model to make it into MLlib is a streaming k-means. This means that the cluster estimations can be dynamically updated as new data arrive. The algorithm uses a generalization of the mini-batch k-means update rule. For each batch of data, we assign all points to their nearest cluster, compute new cluster centers, then update each cluster. \n",
    "\n",
    "In this example we trained our model \"in batch\" on a static store of data. Try writing another class in the `streaming_twitter_cl` project that instead initializes a new `StreamingKMeans` model with random centers and the number of languages you want to classify as your number of clusters, and dynamically updates on fresh stream of Twitter ata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.3 (Scala 2.10)",
   "language": "scala",
   "name": "spark-1.3-scala-2.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
