{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib\n",
    "*Official documentation [here](https://spark.apache.org/docs/latest/mllib-guide.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "\n",
    "classOf[SparkContext].getPackage.getImplementationVersion\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(spark)\n",
    "\n",
    "/*\n",
    "val hiveContext = new org.apache.spark.sql.hive.HiveContext(spark)\n",
    "\n",
    "hiveContext.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\")\n",
    "hiveContext.sql(\"LOAD DATA LOCAL INPATH 'small_data/employer/hashmap.csv' INTO TABLE src\")\n",
    "\n",
    "// Queries are expressed in HiveQL\n",
    "sqlContext.sql(\"FROM src SELECT key, value\").collect().foreach(println)\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def time[R](block: => R): R = {\n",
    "    val start: Long = System.nanoTime()\n",
    "    val result = block\n",
    "    val end: Long = System.nanoTime()\n",
    "    val duration: Double = (end - start) / 1000000000.0  // what happens if you leave off the decimal point?\n",
    "    println(\"Elapsed time: \" + duration + \"s\")\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib Data Types\n",
    "### Vector\n",
    "A mathematical vector. MLlib supports both dense vectors, where every entry is stored, and sparse vectors, where only the nonzero entries are stored to save space. Vectors can be constructed with the mllib.linalg.Vectors class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "\n",
    "// Create a dense vector (1.0, 0.0, 3.0).\n",
    "val dv: Vector = Vectors.dense(1.0, 0.0, 3.0)\n",
    "\n",
    "// Create a sparse vector (1.0, 0.0, 3.0) by specifying its indices and values corresponding to nonzero entries.\n",
    "val sv1: Vector = Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))\n",
    "\n",
    "// Create a sparse vector (1.0, 0.0, 3.0) by specifying its nonzero entries.\n",
    "val sv2: Vector = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0)))\n",
    "\n",
    "assert(sv1 == sv2)\n",
    "assert(dv == sv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** you cannot easily add these vectors!  These are really meant to be used just for machine-learning algorithms.  Convert these to [Breeze](https://github.com/scalanlp/breeze) vetors for your basic linear algebra manipulations.  Like [numpy](http://www.numpy.org/), Breeze uses low-level libraries like [BLAS](http://www.netlib.org/blas/) and [LAPACK](http://www.netlib.org/lapack/) under the hood.  It is not as full ledged as numpy and type-safety can be a little annoying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// NOTE: you cannot easily add these vectors!\n",
    "\n",
    "val x = Vectors.dense((1 to 10).map(_.toDouble).toArray)\n",
    "val y = Vectors.dense((1 to 10).map(_.toDouble).toArray)\n",
    "// x + y (this will fail!)\n",
    "\n",
    "import breeze.linalg.DenseVector\n",
    "\n",
    "val bx = new DenseVector(x.toArray)\n",
    "val by = new DenseVector(y.toArray)\n",
    "val sum = Vectors.dense((bx + by).toArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabeledPoint\n",
    "A labeled data point for supervised learning algorithms such as classification and regression. Includes a feature vector and a label (which is a floating-point value). Located in the mllib.regression package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "\n",
    "val labelPoint1 = LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0))\n",
    "\n",
    "val labelPoint2 = LabeledPoint(1.0, Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0)))\n",
    "\n",
    "println(labelPoint1.label)\n",
    "println(labelPoint1.features)\n",
    "assert(labelPoint1 == labelPoint2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local matrix\n",
    "*Integer*-typed row and column indices and double-typed values, stored on a single machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.{Matrix, Matrices}\n",
    "\n",
    "val dm: Matrix = Matrices.dense(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed matrix \n",
    "*Long*-typed row and column indices and double-typed values, stored distributively in one or more RDDs.  They come in three formats:\n",
    "\n",
    "- **RowMatrix:** Row-oriented distributed matrix without meaningful row indices, e.g. a collection of feature vectors. It is backed by an RDD of its rows, where each row is a local vector. We assume that the number of columns is not huge for a RowMatrix so that a single local vector can be reasonably communicated to the driver and can also be stored / operated on using a single node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "\n",
    "val rows = spark.parallelize(List(\n",
    "    Vectors.dense(1.0, 2.0),\n",
    "    Vectors.dense(2.0, 3.0),\n",
    "    Vectors.dense(3.0, 4.0)\n",
    "))\n",
    "val rowMatrix: RowMatrix = new RowMatrix(rows)\n",
    "\n",
    "// return size\n",
    "val m = rowMatrix.numRows()\n",
    "val n = rowMatrix.numCols()\n",
    "\n",
    "println(s\"rows: $m, cols: $n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **IndexedRowMatrix:** Similar to a RowMatrix but with meaningful row indices. It is backed by an RDD of indexed rows, so that each row is represented by its index (long-typed) and a local vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.{IndexedRowMatrix, IndexedRow}\n",
    "\n",
    "val indexedRows = spark.parallelize(List(\n",
    "    IndexedRow(0L, Vectors.dense(1.0, 2.0)),\n",
    "    IndexedRow(1L, Vectors.dense(2.0, 3.0)),\n",
    "    IndexedRow(3L, Vectors.dense(4.0, 5.0))\n",
    "))\n",
    "\n",
    "val indexedRowMatrix: IndexedRowMatrix = new IndexedRowMatrix(indexedRows)\n",
    "\n",
    "// return size\n",
    "val m = indexedRowMatrix.numRows()\n",
    "val n = indexedRowMatrix.numCols()\n",
    "\n",
    "println(s\"rows: $m, cols: $n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **CoordinateMatrix:** (i.e. a distributed sparse matrix) is (essentially) a list of `(Long, Long, Double)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}\n",
    "\n",
    "val matrixEntires = spark.parallelize(List(\n",
    "    MatrixEntry(0, 0, 1.),\n",
    "    MatrixEntry(1, 1, 1.),\n",
    "    MatrixEntry(2, 2, 1.)\n",
    "))\n",
    "\n",
    "val coordinateMatrix: CoordinateMatrix = new CoordinateMatrix(matrixEntires)\n",
    "\n",
    "val m = coordinateMatrix.numRows()\n",
    "val n = coordinateMatrix.numCols()\n",
    "\n",
    "println(s\"rows: $m, cols: $n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **BlockMatrix:** A distributed matrix backed by an RDD of MatrixBlocks, where a MatrixBlock is a tuple of ((Int, Int), Matrix), where the (Int, Int) is the index of the block, and Matrix is the sub-matrix at the given index with size rowsPerBlock x colsPerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.BlockMatrix\n",
    "\n",
    "val blockMatrix: BlockMatrix = coordinateMatrix.toBlockMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, because the matrix is stored in a distributed way, converting between matrix formats is expensive!\n",
    "\n",
    "**Rating**  \n",
    "A rating of a product by a user, used in the `mllib.recommendation` package for product recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning in MLlib\n",
    "\n",
    "Spark supports a number of machine-learning algorithms.\n",
    "\n",
    "- Classification and Regression\n",
    "    - SVM, linear regression\n",
    "    - SVR, logistic regression\n",
    "    - Naive Bayes\n",
    "    - Decision Trees\n",
    "    - Random Forests and Gradient-Boosted Trees\n",
    "- Clustering\n",
    "    - K-means (and streaming K-means)\n",
    "    - Gaussian Mixture Models\n",
    "    - Latent Dirichlet Allocation\n",
    "- Dimensionality Reduction\n",
    "    - SVD and PCA\n",
    "- It also has support for lower-level optimization primitives:\n",
    "    - Stochastic Gradient Descent\n",
    "    - Low-memory BFGS and L-BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelized SGD\n",
    "\n",
    "For linear models like SVM, Linear Regression, and Logistic Regression, the cost function we're trying to optimize is essentially an average over the individual error term from each data point. This is particularly great for parallelization.  For example, in linear regression, recall that the gradient is\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\log(L(\\beta))}{\\partial \\beta} &= \\frac{\\partial}{\\partial \\beta} \\frac{1}{2}\\sum_j \\|y_j - X_{j \\cdot} \\cdot \\beta\\| \\\\\n",
    "&= \\frac{1}{2}\\sum_j \\frac{\\partial}{\\partial \\beta} \\|y_j - X_{j \\cdot} \\cdot \\beta\\| \\\\\n",
    "& = \\sum_j y_j - X_{j \\cdot} \\cdot \\beta \\\\\n",
    "& \\approx \\sum_{sample \\mbox{ } j} y_j - X_{j \\cdot} \\cdot \\beta\n",
    "\\end{align}$$\n",
    "\n",
    "The key *mathematical properties* we have used are:\n",
    "\n",
    "1. the error functions are the sum of error contributions of different training instances\n",
    "1. linearity of the derivative\n",
    "1. associativity of addition\n",
    "1. downsampling giving an unbiased estimator\n",
    "\n",
    "Since the last sum is over the different training instances and these are stored on different nodes, we can parallelize the computation of the gradient in SGD across multiple nodes.  Of course, we still need to maintain the running weight $\\beta$ that has to be present on every node (through a broadcast variable that is updated).  Notice that SVM, Linear Regression, and Logistic Regression all have error functions that are just sums over training instances so SGD can be used for all these algorithms.\n",
    "\n",
    "Spark's [implementation](http://spark.apache.org/docs/latest/mllib-optimization.html#stochastic-gradient-descent-sgd) uses a tunable minibatch size parameter to sample a percentage of the features RDD. For each iteration, the updated weights are broadcast to the executors, and the update is calculated for each data point and sent back to be aggregated.\n",
    "\n",
    "Parallelization handles increasing number of sampled data points m quite well since there are no interaction terms and each calculation is independent. Controlling how the algorithm iterates to convergence is also important, and can be done with parameters for the total iterations and step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.regression.LinearRegressionWithSGD \n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "// parameters\n",
    "val trainingIterations = 10\n",
    "val trainingFraction = .6\n",
    "\n",
    "// generate data\n",
    "val data = spark.parallelize(1 to 10000).map( _ =>\n",
    "    LabeledPoint(\n",
    "        math.random,\n",
    "        Vectors.dense(math.random, math.random, math.random)\n",
    "    )\n",
    ")\n",
    "\n",
    "// split the training and test sets\n",
    "val (training, test) = {\n",
    "    val splits = data.randomSplit(\n",
    "        Array(trainingFraction, 1. - trainingFraction),\n",
    "        seed = 42L\n",
    "    )\n",
    "    (splits(0).cache(), splits(1))\n",
    "}\n",
    "\n",
    "// train model\n",
    "val model = LinearRegressionWithSGD.train(training, trainingIterations)\n",
    "\n",
    "// get r2 score\n",
    "val r2 = {\n",
    "    val predictionAndLabels = test.map{\n",
    "        case LabeledPoint(label, features) => \n",
    "            (model.predict(features), label)\n",
    "    }\n",
    "    new RegressionMetrics(predictionAndLabels).r2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spark ML\n",
    "Spark ML implements the ideas of transformers, estimators, and pipelines by standardizing APIS across machine learning algorithms. This can streamline more complex workflows.\n",
    "\n",
    "The core functionality includes:\n",
    "* DataFrames - built off Spark SQL, can be created either directly or from RDDs as seen above\n",
    "* Transformers - algorithms that accept a DataFrame as input and return a DataFrame as output\n",
    "* Estimators - algorithms that accept a DataFrame as input and return a Transformer as output\n",
    "* Pipelines - chaining together Transformers and Estimators\n",
    "* Parameters - common API for specifying hyperparameters\n",
    "\n",
    "For example, a learning algorithm can be implemented as an Estimator which trains on a DataFrame of features and returns a Transformer which can output predictions based on a test DataFrame.\n",
    "\n",
    "Full documentation can be found [here](http://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* See the PySpark_MLlib [notebook](PySpark_MLlib.ipynb) for examples (requiring Spark 1.5.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A side note on DataFrame performance and tuning\n",
    "\n",
    "See [here](http://spark.apache.org/docs/latest/sql-programming-guide.html#performance-tuning) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A digression: Plotting (in-memory) with Spark and Scala\n",
    "\n",
    "### Adding  external dependencies\n",
    "TL;DR use the `--packages` flag for external dependencies with Maven coordinates and `--jars` flag for local dependency jars for anything *not* in your `build.sbt`.\n",
    "\n",
    "- **spark-shell**: 3rd party (Maven) dependencies in spark-shell with Maven coordinates: \n",
    "```bash\n",
    "$ spark-shell --master {host} --packages \"{group}:{artifact}:{version}\" --jars oneJar.jar,anotherJar.jar\"\n",
    "```\n",
    "\n",
    "- **spark-submit**: Include 3rd party library jars to spark-submit. Note that packages with Maven coordinates should properly be in your `build.sbt` if you are using `spark-submit`:\n",
    "```bash\n",
    "$ spark-submit --master {host} --jars {someJar}.jar,{anotherJar}.jar --class MyApp path/to/MyApp.jar\n",
    "```\n",
    "\n",
    "\n",
    "### Example: plot values in spark-shell\n",
    "\n",
    "**N.B.** To get plots to appear from DigitalOcean on your local machine, you have to turn on X11 forwarding by ssh'ing into your box with `ssh -X`\n",
    "\n",
    "- To add dependencies in spark-shell with Maven coordinates: \n",
    "```bash\n",
    "$ spark-shell --master local[4] --packages \"{group}:{artifact}:{version}\"\n",
    "```\n",
    "\n",
    "\n",
    "1. Fire up the spark-shell:\n",
    "```bash\n",
    "$ spark-shell --master local[*] --packages \"org.scalanlp:breeze-viz_2.10:0.11.2\"\n",
    "```\n",
    "\n",
    "1. Create a plot that appears in a separate window\n",
    "\n",
    "```scala\n",
    "scala> val a = Array(0.3, 0.9, 1.4, 34.21, -3.4)\n",
    "a: Array[Double] = Array(0.3, 0.9, 1.4, 34.21, -3.4)\n",
    "\n",
    "scala> import breeze.plot._\n",
    "import breeze.plot._\n",
    "\n",
    "scala> val f = Figure()\n",
    "f: breeze.plot.Figure = breeze.plot.Figure@73543048\n",
    "\n",
    "scala> val p = f.subplot(0)\n",
    "p: breeze.plot.Plot = breeze.plot.Plot@2e549515\n",
    "\n",
    "scala> p += plot(a,a,'.')\n",
    "res4: breeze.plot.Plot = breeze.plot.Plot@2e549515\n",
    "\n",
    "scala> val g = breeze.stats.distributions.Gaussian(0,1)\n",
    "...\n",
    "scala> val p2 = f.subplot(1,1)\n",
    "...\n",
    "scala> p2 += hist(g.sample(100000),100)\n",
    "```\n",
    "\n",
    "### Example: plot a distribution\n",
    "\n",
    "Check out Cloudera's [`KernelDensity.estimate` implementation](https://github.com/sryza/aas/tree/master/ch09-risk/src/main/scala/com/cloudera/datascience/risk)\n",
    "\n",
    "```scala\n",
    "import breeze.plot._\n",
    "import com.cloudera.datascience.risk._\n",
    "\n",
    "object AnalysisClass {\n",
    "    ...\n",
    "  def plotDistribution(samples: Array[Double]): Figure = {\n",
    "    val min = samples.min\n",
    "    val max = samples.max\n",
    "    // Using toList before toArray avoids a Scala bug\n",
    "    val domain = Range.Double(min, max, (max - min) / 100).toList.toArray\n",
    "    val densities = KernelDensity.estimate(samples, domain)\n",
    "    val f = Figure()\n",
    "    val p = f.subplot(0)\n",
    "    p += plot(domain, densities)\n",
    "    p.ylabel = \"Density\"\n",
    "    f\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "In order to run this, include the *breeze* dependency in your `build.sbt`, and include the path to the `com.cloudera.datascience` package in the `--jars` flag of spark-submit.\n",
    "\n",
    "```scala\n",
    "libraryDependencies  ++= Seq(\n",
    "  \"org.scalanlp\" %% \"breeze\" % \"0.11.2\",\n",
    "  \"org.scalanlp\" %% \"breeze-natives\" % \"0.11.2\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Use SVM to predict colon cancer from gene expressions\n",
    "You can start getting a feel for the MLlib operations by following the [Spark docs example](https://spark.apache.org/docs/1.3.0/mllib-linear-methods.html#linear-support-vector-machines-svms) on this dataset.\n",
    "\n",
    "#### About the data format: LibSVM\n",
    "MLlib conveniently provides a data loading method, `MLUtils.loadLibSVMFile()`, for the LibSVM format for which many other languages (R, Matlab, etc.) also have loading methods.  \n",
    "A dataset of *n* features will have one row per datum, with the label and values of each feature organized as follows:\n",
    ">{label} 1:{value} 2:{value} ... n:{value}\n",
    "\n",
    "Take these two datapoints with six features and labels of -1 and 1 respectively as an example:\n",
    ">-1.000000  1:2.080750 2:1.099070 3:0.927763 4:1.029080 5:-0.130763 6:1.265460  \n",
    "1.000000  1:1.109460 2:0.786453 3:0.445560 4:-0.146323 5:-0.996316 6:0.555759 \n",
    "\n",
    "#### About the colon-cancer dataset\n",
    "This dataset was introduced in the 1999 paper [Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays.](http://www.pnas.org/content/96/12/6745.short)  \n",
    "\n",
    "Here's the abstract of the paper:  \n",
    "> *Oligonucleotide arrays can provide a broad picture of the state of the cell, by monitoring the expression level of thousands of genes at the same time. It is of interest to develop techniques for extracting useful information from the resulting data sets. Here we report the application of a two-way clustering method for analyzing a data set consisting of the expression patterns of different cell types. Gene expression in 40 tumor and 22 normal colon tissue samples was analyzed with an Affymetrix oligonucleotide array complementary to more than 6,500 human genes. An efficient two-way clustering algorithm was applied to both the genes and the tissues, revealing broad coherent patterns that suggest a high degree of organization underlying gene expression in these tissues. Coregulated families of genes clustered together, as demonstrated for the ribosomal proteins. Clustering also separated cancerous from noncancerous tissue and cell lines from in vivo tissues on the basis of subtle distributed patterns of genes even when expression of individual genes varied only slightly between the tissues. Two-way clustering thus may be of use both in classifying genes into functional groups and in classifying tissues based on gene expression.*\n",
    "\n",
    "There are 2000 features, 62 data points (40 tumor (label=0), 22 normal (label=1)), and 2 classes (labels) for the colon cancer dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exit Tickets\n",
    "1. When would you use `org.apache.spark.mllib.linalg.Vector` versus `breeze.linalg.DenseVector`?\n",
    "1. Why can SVM, Linear Regression, and Logistic Regression be parallelized?  How would you parallelize KMeans?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.3 (Scala 2.10)",
   "language": "scala",
   "name": "spark-1.3-scala-2.10"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
